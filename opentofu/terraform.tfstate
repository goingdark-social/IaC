{"version":4,"terraform_version":"1.10.7","serial":294,"lineage":"44d23655-0774-124a-b227-5445ec8ff6bc","outputs":{"stalwart_mail_dns_records":{"value":{"mail_a_record":"mail.peekoff.com","reverse_dns_ipv4":"mail.peekoff.com","reverse_dns_ipv6":"mail.peekoff.com"},"type":["object",{"mail_a_record":"string","reverse_dns_ipv4":"string","reverse_dns_ipv6":"string"}]},"stalwart_mail_ipv4":{"value":"77.42.14.20","type":"string"},"stalwart_mail_ipv6":{"value":"2a01:4f9:c01e:58e::1","type":"string"},"worker_public_ipv4_list":{"value":["46.62.203.120","135.181.92.222"],"type":["tuple",["string","string"]]},"worker_public_ipv6_list":{"value":["",""],"type":["tuple",["string","string"]]}},"resources":[{"mode":"data","type":"cloudflare_zone","name":"peekoff","provider":"provider[\"registry.opentofu.org/cloudflare/cloudflare\"]","instances":[{"schema_version":0,"attributes":{"account":{"id":"a694d529ab7d7176bcac8585f8bafdf4","name":"Benjamin.sanden2@gmail.com's Account"},"activated_on":"2025-12-18T20:39:11Z","cname_suffix":null,"created_on":"2025-12-18T20:39:06Z","development_mode":0,"filter":{"account":null,"direction":null,"match":null,"name":"peekoff.com","order":null,"status":null},"id":"e8146b7b5485151dc758a57e6830da35","meta":{"cdn_only":null,"custom_certificate_quota":0,"dns_only":null,"foundation_dns":null,"page_rule_quota":3,"phishing_detected":false,"step":4},"modified_on":"2025-12-20T20:47:42Z","name":"peekoff.com","name_servers":["aida.ns.cloudflare.com","julio.ns.cloudflare.com"],"original_dnshost":null,"original_name_servers":null,"original_registrar":null,"owner":{"id":null,"name":null,"type":"user"},"paused":false,"permissions":["#dns_records:edit","#dns_records:read","#zone:read"],"plan":{"can_subscribe":false,"currency":"USD","externally_managed":false,"frequency":"","id":"0feeeeeeeeeeeeeeeeeeeeeeeeeeeeee","is_subscribed":false,"legacy_discount":false,"legacy_id":"free","name":"Free Website","price":0},"status":"active","tenant":{"id":null,"name":null},"tenant_unit":{"id":null},"type":"full","vanity_name_servers":[],"verification_key":null,"zone_id":"e8146b7b5485151dc758a57e6830da35"},"sensitive_attributes":[]}]},{"mode":"data","type":"hcloud_load_balancer","name":"worker_lb","provider":"provider[\"registry.opentofu.org/hashicorp/hcloud\"]","instances":[{"schema_version":0,"attributes":{"algorithm":[{"type":"round_robin"}],"delete_protection":false,"id":5452744,"ipv4":"77.42.14.20","ipv6":"2a01:4f9:c01e:58e::1","labels":{"hcloud-ccm/service-uid":"ac839b99-af21-4a42-a839-8227b9db1448"},"load_balancer_type":"lb11","location":"hel1","name":"aac839b99af214a42a8398227b9db144","network_id":11368893,"network_ip":"10.0.95.130","network_zone":"eu-central","service":null,"target":[],"with_selector":null},"sensitive_attributes":[]}]},{"mode":"managed","type":"hcloud_rdns","name":"stalwart_mail_ipv4","provider":"provider[\"registry.opentofu.org/hashicorp/hcloud\"]","instances":[{"schema_version":0,"attributes":{"dns_ptr":"mail.peekoff.com","floating_ip_id":null,"id":"l-5452744-77.42.14.20","ip_address":"77.42.14.20","load_balancer_id":5452744,"primary_ip_id":null,"server_id":null},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["data.hcloud_load_balancer.worker_lb"]}]},{"mode":"managed","type":"hcloud_rdns","name":"stalwart_mail_ipv6","provider":"provider[\"registry.opentofu.org/hashicorp/hcloud\"]","instances":[{"schema_version":0,"attributes":{"dns_ptr":"mail.peekoff.com","floating_ip_id":null,"id":"l-5452744-2a01:4f9:c01e:58e::1","ip_address":"2a01:4f9:c01e:58e::1","load_balancer_id":5452744,"primary_ip_id":null,"server_id":null},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["data.hcloud_load_balancer.worker_lb"]}]},{"module":"module.kubernetes","mode":"data","type":"hcloud_certificates","name":"state","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":0,"attributes":{"certificates":[{"certificate":"-----BEGIN CERTIFICATE-----\nMIIC2jCCAcKgAwIBAgIRAPnXgpn+6biaAieROzh+VDYwDQYJKoZIhvcNAQELBQAw\nFDESMBAGA1UEAxMJZ29pbmdkYXJrMCAXDTI1MDgyMzA5MTI0N1oYDzIxMjUwODI0\nMDkxMjQ3WjAUMRIwEAYDVQQDEwlnb2luZ2RhcmswggEiMA0GCSqGSIb3DQEBAQUA\nA4IBDwAwggEKAoIBAQDRgrI1Ty5TlbY65wrgR0ErWAdUlu8R7+qk22e8hKg7mViK\neCLyhCFmpZp7s0LNTf6hbHDIL67Goz/UbYegIHWNGj7In4ZP5hjgRaOTTxoV7ZCm\ndIV1mAp5B4naU3fjyPG1Tc3sVUc0hIjRdp9H2OKTlQGp4WmrV+Z5Dve29tObTMld\nqMIkDwPG4jeAwUndAtpBdX1TAVKix8+Pqg1L/I/NeSOa2oVKlo1EtlDANIOKjhCl\nQk19ISuGqzIzLS0bu22L7tSUj7KOhPJx7Qm0jc/YQERiZr5nrSNwcNwIxNC5jSZZ\nDgj3WgMyZQVScXtmalNad79FvdV9ujIwRPzgbgdnAgMBAAGjJTAjMBMGA1UdJQQM\nMAoGCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwDQYJKoZIhvcNAQELBQADggEBAJNk\n7E8QpmF7KRvRO6rorysv82xT7Q+KSuFBdaxgu0V0LmHCpn3ZXZqJwAB1GLiKv1Th\n7cvhr+jsr5KdYHFbIEenOH0BsQnqODj5GJtRRXzvQLcL2awzb+NAN8n1wnwdC6cZ\nbwWFO3jg0DThGF80vEjNME3EGC3nfz8RIhH5zxSUjt3OX9s2xi2kttlgcw/HTP3C\np+mqLI6ZHlwsDM1+n+fBjJhPCWR/n+/OyDeyLbWkadgLjIU41hI7e9vH698ZHQNw\nToP/ckQD7X2FSpdBp/z+NQA63WzQZE30YEVxGXnonUu9Z1X55E2U6DkH2SFv44TL\n2qcGS3CP9LYspObP994=\n-----END CERTIFICATE-----\n","created":"2025-08-23T09:16:39Z","domain_names":["goingdark"],"fingerprint":"9E:FB:96:BA:3E:6C:F2:08:D1:82:52:B8:10:4E:F4:49:2B:9B:1E:C8:E9:58:4E:7A:30:E3:83:B2:0B:FD:42:A7","id":1487604,"labels":{"cluster":"goingdark","state":"initialized"},"name":"goingdark-state","not_valid_after":"2125-08-24T09:12:47Z","not_valid_before":"2025-08-23T09:12:47Z","type":"uploaded"}],"id":"ce4bee37c3aa6aaa34a99c25473558bb48279242","with_selector":"cluster=goingdark,state=initialized"},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"hcloud_image","name":"amd64","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"architecture":"x86","created":"2025-09-17T18:57:06Z","deprecated":null,"description":"Talos Linux AMD64 for goingdark","id":317978117,"include_deprecated":false,"labels":{"cluster":"goingdark","os":"talos","talos_schematic_id":"ce4c980550dd2ab1b17bbf2b08801c7e","talos_version":"v1.11.1"},"most_recent":true,"name":"","os_flavor":"debian","os_version":"12","rapid_deploy":false,"selector":null,"type":"snapshot","with_architecture":"x86","with_selector":"os=talos,cluster=goingdark,talos_version=v1.11.1,talos_schematic_id=ce4c980550dd2ab1b17bbf2b08801c7e","with_status":null},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"hcloud_images","name":"amd64","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"id":"b740c2af7cbd9fe06be182c3307774b7f4982972","images":[{"architecture":"x86","created":"2025-09-17T18:57:06Z","deprecated":"","description":"Talos Linux AMD64 for goingdark","id":317978117,"labels":{"cluster":"goingdark","os":"talos","talos_schematic_id":"ce4c980550dd2ab1b17bbf2b08801c7e","talos_version":"v1.11.1"},"name":"","os_flavor":"debian","os_version":"12","rapid_deploy":false,"selector":"","type":"snapshot"}],"include_deprecated":false,"most_recent":true,"with_architecture":["x86"],"with_selector":"os=talos,cluster=goingdark,talos_version=v1.11.1,talos_schematic_id=ce4c980550dd2ab1b17bbf2b08801c7e","with_status":null},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"helm_template","name":"cilium","provider":"module.kubernetes.provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"schema_version":0,"attributes":{"api_versions":null,"atomic":false,"chart":"cilium","crds":[],"create_namespace":false,"dependency_update":false,"description":"","devel":false,"disable_openapi_validation":false,"disable_webhooks":false,"id":"cilium","include_crds":false,"is_upgrade":false,"keyring":"","kube_version":"v1.33.4","manifest":"---\n# Source: cilium/templates/cilium-secrets-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\n  annotations:\n---\n# Source: cilium/templates/cilium-agent/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"cilium\"\n  namespace: kube-system\n---\n# Source: cilium/templates/cilium-agent/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cilium-config-agent\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  verbs:\n  - get\n  - list\n  - watch\n---\n# Source: cilium/templates/cilium-agent/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cilium-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\n---\n# Source: cilium/templates/cilium-operator/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cilium-operator-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - secrets\n  verbs:\n  - create\n  - delete\n  - update\n  - patch\n---\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cilium-config-agent\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cilium-config-agent\nsubjects:\n  - kind: ServiceAccount\n    name: \"cilium\"\n    namespace: kube-system\n---\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cilium-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cilium-tlsinterception-secrets\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium\"\n  namespace: kube-system\n---\n# Source: cilium/templates/cilium-operator/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cilium-operator-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cilium-operator-tlsinterception-secrets\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium-operator\"\n  namespace: kube-system\n---\n# Source: cilium/templates/cilium-envoy/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: cilium-envoy\n  namespace: kube-system\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9964\"\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/name: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    io.cilium/app: proxy\nspec:\n  clusterIP: None\n  type: ClusterIP\n  selector:\n    k8s-app: cilium-envoy\n  ports:\n  - name: envoy-metrics\n    port: 9964\n    protocol: TCP\n    targetPort: envoy-metrics\n---\n# Source: cilium/templates/cilium-agent/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9962\"\n        prometheus.io/scrape: \"true\"\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: \"brief\"\n              value: \"true\"\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: \"brief\"\n              value: \"true\"\n            - name: \"require-k8s-connectivity\"\n              value: \"false\"\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: \"brief\"\n              value: \"true\"\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: \"1\"\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: \"120\"\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - \"bash\"\n              - \"-c\"\n              - |\n                    set -o errexit\n                    set -o pipefail\n                    set -o nounset\n                    \n                    # When running in AWS ENI mode, it's likely that 'aws-node' has\n                    # had a chance to install SNAT iptables rules. These can result\n                    # in dropped traffic, so we should attempt to remove them.\n                    # We do it using a 'postStart' hook since this may need to run\n                    # for nodes which might have already been init'ed but may still\n                    # have dangling rules. This is safe because there are no\n                    # dependencies on anything that is part of the startup script\n                    # itself, and can be safely run multiple times per node (e.g. in\n                    # case of a restart).\n                    if [[ \"$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\" != \"0\" ]];\n                    then\n                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'\n                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore\n                    fi\n                    echo 'Done!'\n                    \n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        ports:\n        - name: peer-service\n          containerPort: 4244\n          hostPort: 4244\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9962\n          hostPort: 9962\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - CHOWN\n              - KILL\n              - NET_ADMIN\n              - NET_RAW\n              - IPC_LOCK\n              - SYS_ADMIN\n              - SYS_RESOURCE\n              - DAC_OVERRIDE\n              - FOWNER\n              - SETGID\n              - SETUID\n            drop:\n              - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        # Unprivileged containers need to mount /proc/sys/net from the host\n        # to have write access\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        # Unprivileged containers need to mount /proc/sys/kernel from the host\n        # to have write access\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          # Unprivileged containers can't set mount propagation to bidirectional\n          # in this case we will mount the bpf fs from an init container that\n          # is privileged and set the mount propagation from host to container\n          # in Cilium.\n          mountPropagation: HostToContainer\n        # Check for duplicate mounts before mounting\n        - name: cilium-cgroup\n          mountPath: /sys/fs/cgroup\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n          # Needed to be able to load kernel modules\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: tmp\n          mountPath: /tmp\n        \n      initContainers:\n      - name: config\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n      - name: apply-sysctl-overwrites\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        # The statically linked Go program binary is invoked to avoid any\n        # dependency on utilities like sh that can be missing on certain\n        # distros installed on the underlying host. Copy the binary to the\n        # same directory where we install cilium cni plugin so that exec permissions\n        # are available.\n        - |\n          cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n          rm /hostbin/cilium-sysctlfix\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - SYS_ADMIN\n              - SYS_CHROOT\n              - SYS_PTRACE\n            drop:\n              - ALL\n      # Mount the bpf fs if it is not mounted. We will perform this task\n      # from a privileged container because the mount propagation bidirectional\n      # only works from privileged containers.\n      - name: mount-bpf-fs\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        args:\n        - 'mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf'\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - NET_ADMIN\n              - SYS_ADMIN\n              - SYS_RESOURCE\n            drop:\n              - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          # Required to mount cgroup filesystem from the host to cilium agent pod\n        - name: cilium-cgroup\n          mountPath: /sys/fs/cgroup\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium # wait-for-kube-proxy\n      # Install the CNI binaries in an InitContainer so we don't have a writable host mount in the agent\n      - name: install-cni-binaries\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n          - \"/install-plugin.sh\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n              - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n          - name: cni-path\n            mountPath: /host/opt/cni/bin # .Values.cni.install\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: \"cilium\"\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - operator: Exists\n      volumes:\n        # For sharing configuration between the \"config\" initContainer and the agent\n      - name: tmp\n        emptyDir: {}\n        # To keep state between restarts / upgrades\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        # To exec into pod network namespaces\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n        # To keep state between restarts / upgrades for bpf maps\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      # To mount cgroup2 filesystem on the host or apply sysctlfix\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      # To keep state between restarts / upgrades for cgroup2 filesystem\n      - name: cilium-cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n          type: DirectoryOrCreate\n      # To install cilium cni plugin in the host\n      - name: cni-path\n        hostPath:\n          path:  /opt/cni/bin\n          type: DirectoryOrCreate\n        # To install cilium cni configuration in the host\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        # To be able to load kernel modules\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n        # To access iptables concurrently with other processes (e.g. kube-proxy)\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      # Sharing socket with Cilium Envoy on the same node by using a host path\n      - name: envoy-sockets\n        hostPath:\n          path: \"/var/run/cilium/envoy/sockets\"\n          type: DirectoryOrCreate\n        # To read the clustermesh configuration\n      - name: clustermesh-secrets\n        projected:\n          # note: the leading zero means this number is in octal representation: do not remove it\n          defaultMode: 0400\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n              # note: items are not explicitly listed here, since the entries of this secret\n              # depend on the peers configured, and that would cause a restart of all agents\n              # at every addition/removal. Leaving the field empty makes each secret entry\n              # to be automatically projected into the volume as a file whose name is the key.\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          # note: we configure the volume for the kvstoremesh-specific certificate\n          # regardless of whether KVStoreMesh is enabled or not, so that it can be\n          # automatically mounted in case KVStoreMesh gets subsequently enabled,\n          # without requiring an agent restart.\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n---\n# Source: cilium/templates/cilium-envoy/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: kube-system\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: \"quay.io/cilium/cilium-envoy:v1.34.4-1754895458-68cffdfa568b6b226d70a7ef81fc65dda3b890bf@sha256:247e908700012f7ef56f75908f8c965215c26a27762f296068645eb55450bda2\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - '--'\n        - '-c /var/run/cilium/envoy/bootstrap-config.json'\n        - '--base-id 0'\n        - '--log-level info'\n        startupProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - NET_ADMIN\n              - SYS_ADMIN\n            drop:\n              - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: \"cilium-envoy\"\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - \"true\"\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: \"/var/run/cilium/envoy/sockets\"\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: \"/var/run/cilium/envoy/artifacts\"\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: \"cilium-envoy-config\"\n          # note: the leading zero means this number is in octal representation: do not remove it\n          defaultMode: 0400\n          items:\n            - key: bootstrap-config.json\n              path: bootstrap-config.json\n        # To keep state between restarts / upgrades\n        # To keep state between restarts / upgrades for bpf maps\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n---\n# Source: cilium/templates/cilium-operator/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: kube-system\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  # See docs on ServerCapabilities.LeasesResourceLock in file pkg/k8s/version/version.go\n  # for more details.\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  # ensure operator update on single node k8s clusters, by using rolling update with maxUnavailable=100% in case\n  # of one replica and no user configured Recreate strategy.\n  # otherwise an update might get stuck due to the default maxUnavailable=50% in combination with the\n  # podAntiAffinity which prevents deployments of multiple operator replicas on the same node.\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 100%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9963\"\n        prometheus.io/scrape: \"true\"\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: \"quay.io/cilium/operator-generic:v1.18.1@sha256:97f4553afa443465bdfbc1cc4927c93f16ac5d78e4dd2706736e7395382201bc\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: \"cilium-operator\"\n      automountServiceAccountToken: true\n      # In HA mode, cilium-operator pods must not be scheduled on the same\n      # node as they will clash with each other.\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n        node-role.kubernetes.io/control-plane: \"\"\n      tolerations:\n        - operator: Exists\n        - key: node.cilium.io/agent-not-ready\n          operator: Exists\n      \n      volumes:\n        # To read the configuration from the config map\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n---\n# Source: cilium/templates/cilium-envoy/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"cilium-envoy\"\n  namespace: kube-system\n---\n# Source: cilium/templates/cilium-operator/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"cilium-operator\"\n  namespace: kube-system\n---\n# Source: cilium/templates/cilium-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n\n  # Identity allocation mode selects how identities are shared between cilium\n  # nodes by setting how they are stored. The options are \"crd\", \"kvstore\" or\n  # \"doublewrite-readkvstore\" / \"doublewrite-readcrd\".\n  # - \"crd\" stores identities in kubernetes as CRDs (custom resource definition).\n  #   These can be queried with:\n  #     kubectl get ciliumid\n  # - \"kvstore\" stores identities in an etcd kvstore, that is\n  #   configured below. Cilium versions before 1.6 supported only the kvstore\n  #   backend. Upgrades from these older cilium versions should continue using\n  #   the kvstore by commenting out the identity-allocation-mode below, or\n  #   setting it to \"kvstore\".\n  # - \"doublewrite\" modes store identities in both the kvstore and CRDs. This is useful\n  #   for seamless migrations from the kvstore mode to the crd mode. Consult the\n  #   documentation for more information on how to perform the migration.\n  identity-allocation-mode: crd\n\n  identity-heartbeat-timeout: \"30m0s\"\n  identity-gc-interval: \"15m0s\"\n  cilium-endpoint-gc-interval: \"5m0s\"\n  nodes-gc-interval: \"5m0s\"\n\n  # If you want to run cilium in debug mode change this value to true\n  debug: \"false\"\n  debug-verbose: \"\"\n  metrics-sampling-interval: \"5m\"\n  # The agent can be put into the following three policy enforcement modes\n  # default, always and never.\n  # https://docs.cilium.io/en/latest/security/policy/intro/#policy-enforcement-modes\n  enable-policy: \"default\"\n  policy-cidr-match-mode: \"\"\n  # If you want metrics enabled in all of your Cilium agents, set the port for\n  # which the Cilium agents will have their metrics exposed.\n  # This option deprecates the \"prometheus-serve-addr\" in the\n  # \"cilium-metrics-config\" ConfigMap\n  # NOTE that this will open the port on ALL nodes where Cilium pods are\n  # scheduled.\n  prometheus-serve-addr: \":9962\"\n  # A space-separated list of controller groups for which to enable metrics.\n  # The special values of \"all\" and \"none\" are supported.\n  controller-group-metrics:\n    write-cni-file\n    sync-host-ips\n    sync-lb-maps-with-k8s-services\n  # If you want metrics enabled in cilium-operator, set the port for\n  # which the Cilium Operator will have their metrics exposed.\n  # NOTE that this will open the port on the nodes where Cilium operator pod\n  # is scheduled.\n  operator-prometheus-serve-addr: \":9963\"\n  enable-metrics: \"true\"\n  enable-policy-secrets-sync: \"true\"\n  policy-secrets-only-from-secrets-namespace: \"true\"\n  policy-secrets-namespace: \"cilium-secrets\"\n\n  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4\n  # address.\n  enable-ipv4: \"true\"\n\n  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6\n  # address.\n  enable-ipv6: \"false\"\n  # Users who wish to specify their own custom CNI configuration file must set\n  # custom-cni-conf to \"true\", otherwise Cilium may overwrite the configuration.\n  custom-cni-conf: \"false\"\n  enable-bpf-clock-probe: \"false\"\n  # If you want cilium monitor to aggregate tracing for packets, set this level\n  # to \"low\", \"medium\", or \"maximum\". The higher the level, the less packets\n  # that will be seen in monitor output.\n  monitor-aggregation: medium\n\n  # The monitor aggregation interval governs the typical time between monitor\n  # notification events for each allowed connection.\n  #\n  # Only effective when monitor aggregation is set to \"medium\" or higher.\n  monitor-aggregation-interval: \"5s\"\n\n  # The monitor aggregation flags determine which TCP flags which, upon the\n  # first observation, cause monitor notifications to be generated.\n  #\n  # Only effective when monitor aggregation is set to \"medium\" or higher.\n  monitor-aggregation-flags: all\n  # Specifies the ratio (0.0-1.0] of total system memory to use for dynamic\n  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.\n  bpf-map-dynamic-size-ratio: \"0.0025\"\n  enable-host-legacy-routing: \"false\"\n  # bpf-policy-map-max specifies the maximum number of entries in endpoint\n  # policy map (per endpoint)\n  bpf-policy-map-max: \"16384\"\n  # bpf-policy-stats-map-max specifies the maximum number of entries in global\n  # policy stats map\n  bpf-policy-stats-map-max: \"65536\"\n  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,\n  # backend and affinity maps.\n  bpf-lb-map-max: \"65536\"\n  bpf-lb-external-clusterip: \"false\"\n  bpf-lb-source-range-all-types: \"false\"\n  bpf-lb-algorithm-annotation: \"false\"\n  bpf-lb-mode-annotation: \"false\"\n\n  bpf-distributed-lru: \"false\"\n  bpf-events-drop-enabled: \"true\"\n  bpf-events-policy-verdict-enabled: \"true\"\n  bpf-events-trace-enabled: \"true\"\n\n  # Pre-allocation of map entries allows per-packet latency to be reduced, at\n  # the expense of up-front memory allocation for the entries in the maps. The\n  # default value below will minimize memory usage in the default installation;\n  # users who are sensitive to latency may consider setting this to \"true\".\n  #\n  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore\n  # this option and behave as though it is set to \"true\".\n  #\n  # If this value is modified, then during the next Cilium startup the restore\n  # of existing endpoints and tracking of ongoing connections may be disrupted.\n  # As a result, reply packets may be dropped and the load-balancing decisions\n  # for established connections may change.\n  #\n  # If this option is set to \"false\" during an upgrade from 1.3 or earlier to\n  # 1.4 or later, then it may cause one-time disruptions during the upgrade.\n  preallocate-bpf-maps: \"false\"\n\n  # Name of the cluster. Only relevant when building a mesh of clusters.\n  cluster-name: \"default\"\n  # Unique ID of the cluster. Must be unique across all conneted clusters and\n  # in the range of 1 and 255. Only relevant when building a mesh of clusters.\n  cluster-id: \"0\"\n\n  # Encapsulation mode for communication between nodes\n  # Possible values:\n  #   - disabled\n  #   - vxlan (default)\n  #   - geneve\n\n  routing-mode: \"native\"\n  tunnel-protocol: \"vxlan\"\n  tunnel-source-port-range: \"0-0\"\n  service-no-backend-response: \"reject\"\n\n\n  # Enables L7 proxy for L7 policy enforcement and visibility\n  enable-l7-proxy: \"true\"\n  enable-ipv4-masquerade: \"true\"\n  enable-ipv4-big-tcp: \"false\"\n  enable-ipv6-big-tcp: \"false\"\n  enable-ipv6-masquerade: \"true\"\n  enable-tcx: \"true\"\n  datapath-mode: \"veth\"\n  enable-bpf-masquerade: \"true\"\n  enable-masquerade-to-route-source: \"false\"\n  enable-wireguard: \"true\"\n  wireguard-persistent-keepalive: \"0s\"\n\n  enable-xt-socket-fallback: \"true\"\n  install-no-conntrack-iptables-rules: \"true\"\n  iptables-random-fully: \"false\"\n\n  auto-direct-node-routes: \"false\"\n  direct-routing-skip-unreachable: \"false\"\n\n\n  ipv4-native-routing-cidr: 10.0.0.0/16\n\n  kube-proxy-replacement: \"true\"\n  kube-proxy-replacement-healthz-bind-address: \"0.0.0.0:10256\"\n  bpf-lb-sock: \"false\"\n  nodeport-addresses: \"\"\n  enable-health-check-nodeport: \"true\"\n  enable-health-check-loadbalancer-ip: \"false\"\n  node-port-bind-protection: \"true\"\n  enable-auto-protect-node-port-range: \"true\"\n  bpf-lb-acceleration: \"native\"\n  enable-svc-source-range-check: \"true\"\n  enable-l2-neigh-discovery: \"false\"\n  k8s-require-ipv4-pod-cidr: \"true\"\n  k8s-require-ipv6-pod-cidr: \"false\"\n  enable-k8s-networkpolicy: \"true\"\n  enable-endpoint-lockdown-on-policy-overflow: \"false\"\n  # Tell the agent to generate and write a CNI configuration file\n  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist\n  cni-exclusive: \"true\"\n  cni-log-file: \"/var/run/cilium/cilium-cni.log\"\n  enable-endpoint-health-checking: \"true\"\n  enable-health-checking: \"true\"\n  health-check-icmp-failure-threshold: \"3\"\n  enable-well-known-identities: \"false\"\n  enable-node-selector-labels: \"false\"\n  synchronize-k8s-nodes: \"true\"\n  operator-api-serve-addr: \"127.0.0.1:9234\"\n\n  enable-hubble: \"false\"\n  ipam: \"kubernetes\"\n  ipam-cilium-node-update-rate: \"15s\"\n\n  default-lb-service-ipam: \"lbipam\"\n  egress-gateway-reconciliation-trigger-interval: \"1s\"\n  enable-vtep: \"false\"\n  vtep-endpoint: \"\"\n  vtep-cidr: \"\"\n  vtep-mask: \"\"\n  vtep-mac: \"\"\n  procfs: \"/host/proc\"\n  bpf-root: \"/sys/fs/bpf\"\n  cgroup-root: \"/sys/fs/cgroup\"\n\n  identity-management-mode: \"agent\"\n  enable-sctp: \"false\"\n  remove-cilium-node-taints: \"true\"\n  set-cilium-node-taints: \"true\"\n  set-cilium-is-up-condition: \"true\"\n  unmanaged-pod-watcher-interval: \"15\"\n  # explicit setting gets precedence\n  dnsproxy-enable-transparent-mode: \"true\"\n  dnsproxy-socket-linger-timeout: \"10\"\n  tofqdns-dns-reject-response-code: \"refused\"\n  tofqdns-enable-dns-compression: \"true\"\n  tofqdns-endpoint-max-ip-per-hostname: \"1000\"\n  tofqdns-idle-connection-grace-period: \"0s\"\n  tofqdns-max-deferred-connection-deletes: \"10000\"\n  tofqdns-proxy-response-max-delay: \"100ms\"\n  tofqdns-preallocate-identities:  \"true\"\n  agent-not-ready-taint-key: \"node.cilium.io/agent-not-ready\"\n\n  mesh-auth-enabled: \"true\"\n  mesh-auth-queue-size: \"1024\"\n  mesh-auth-rotated-identities-queue-size: \"1024\"\n  mesh-auth-gc-interval: \"5m0s\"\n\n  proxy-xff-num-trusted-hops-ingress: \"0\"\n  proxy-xff-num-trusted-hops-egress: \"0\"\n  proxy-connect-timeout: \"2\"\n  proxy-initial-fetch-timeout: \"30\"\n  proxy-max-requests-per-connection: \"0\"\n  proxy-max-connection-duration-seconds: \"0\"\n  proxy-idle-timeout-seconds: \"60\"\n  proxy-max-concurrent-retries: \"128\"\n  http-retry-count: \"3\"\n\n  external-envoy-proxy: \"true\"\n  envoy-base-id: \"0\"\n  envoy-access-log-buffer-size: \"4096\"\n  envoy-keep-cap-netbindservice: \"false\"\n  max-connected-clusters: \"255\"\n  clustermesh-enable-endpoint-sync: \"false\"\n  clustermesh-enable-mcs-api: \"false\"\n  policy-default-local-cluster: \"false\"\n\n  nat-map-stats-entries: \"32\"\n  nat-map-stats-interval: \"30s\"\n  enable-internal-traffic-policy: \"true\"\n  enable-lb-ipam: \"true\"\n  enable-non-default-deny-policies: \"true\"\n  enable-source-ip-verification: \"true\"\n\n# Extra config allows adding arbitrary properties to the cilium config.\n# By putting it at the end of the ConfigMap, it's also possible to override existing properties.\n---\n# Source: cilium/templates/cilium-envoy/configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-envoy-config\n  namespace: kube-system\ndata:\n  # Keep the key name as bootstrap-config.json to avoid breaking changes\n  bootstrap-config.json: |\n    {\"admin\":{\"address\":{\"pipe\":{\"path\":\"/var/run/cilium/envoy/sockets/admin.sock\"}}},\"applicationLogConfig\":{\"logFormat\":{\"textFormat\":\"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v\"}},\"bootstrapExtensions\":[{\"name\":\"envoy.bootstrap.internal_listener\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener\"}}],\"dynamicResources\":{\"cdsConfig\":{\"apiConfigSource\":{\"apiType\":\"GRPC\",\"grpcServices\":[{\"envoyGrpc\":{\"clusterName\":\"xds-grpc-cilium\"}}],\"setNodeOnFirstMessageOnly\":true,\"transportApiVersion\":\"V3\"},\"initialFetchTimeout\":\"30s\",\"resourceApiVersion\":\"V3\"},\"ldsConfig\":{\"apiConfigSource\":{\"apiType\":\"GRPC\",\"grpcServices\":[{\"envoyGrpc\":{\"clusterName\":\"xds-grpc-cilium\"}}],\"setNodeOnFirstMessageOnly\":true,\"transportApiVersion\":\"V3\"},\"initialFetchTimeout\":\"30s\",\"resourceApiVersion\":\"V3\"}},\"node\":{\"cluster\":\"ingress-cluster\",\"id\":\"host~127.0.0.1~no-id~localdomain\"},\"overloadManager\":{\"resourceMonitors\":[{\"name\":\"envoy.resource_monitors.global_downstream_max_connections\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig\",\"max_active_downstream_connections\":\"50000\"}}]},\"staticResources\":{\"clusters\":[{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"ingress-cluster\",\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"useDownstreamProtocolConfig\":{}}}},{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"egress-cluster-tls\",\"transportSocket\":{\"name\":\"cilium.tls_wrapper\",\"typedConfig\":{\"@type\":\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\"}},\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"upstreamHttpProtocolOptions\":{},\"useDownstreamProtocolConfig\":{}}}},{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"egress-cluster\",\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"useDownstreamProtocolConfig\":{}}}},{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"ingress-cluster-tls\",\"transportSocket\":{\"name\":\"cilium.tls_wrapper\",\"typedConfig\":{\"@type\":\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\"}},\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"upstreamHttpProtocolOptions\":{},\"useDownstreamProtocolConfig\":{}}}},{\"connectTimeout\":\"2s\",\"loadAssignment\":{\"clusterName\":\"xds-grpc-cilium\",\"endpoints\":[{\"lbEndpoints\":[{\"endpoint\":{\"address\":{\"pipe\":{\"path\":\"/var/run/cilium/envoy/sockets/xds.sock\"}}}}]}]},\"name\":\"xds-grpc-cilium\",\"type\":\"STATIC\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"explicitHttpConfig\":{\"http2ProtocolOptions\":{}}}}},{\"connectTimeout\":\"2s\",\"loadAssignment\":{\"clusterName\":\"/envoy-admin\",\"endpoints\":[{\"lbEndpoints\":[{\"endpoint\":{\"address\":{\"pipe\":{\"path\":\"/var/run/cilium/envoy/sockets/admin.sock\"}}}}]}]},\"name\":\"/envoy-admin\",\"type\":\"STATIC\"}],\"listeners\":[{\"address\":{\"socketAddress\":{\"address\":\"0.0.0.0\",\"portValue\":9964}},\"filterChains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"httpFilters\":[{\"name\":\"envoy.filters.http.router\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"internalAddressConfig\":{\"cidrRanges\":[{\"addressPrefix\":\"10.0.0.0\",\"prefixLen\":8},{\"addressPrefix\":\"172.16.0.0\",\"prefixLen\":12},{\"addressPrefix\":\"192.168.0.0\",\"prefixLen\":16},{\"addressPrefix\":\"127.0.0.1\",\"prefixLen\":32}]},\"routeConfig\":{\"virtualHosts\":[{\"domains\":[\"*\"],\"name\":\"prometheus_metrics_route\",\"routes\":[{\"match\":{\"prefix\":\"/metrics\"},\"name\":\"prometheus_metrics_route\",\"route\":{\"cluster\":\"/envoy-admin\",\"prefixRewrite\":\"/stats/prometheus\"}}]}]},\"statPrefix\":\"envoy-prometheus-metrics-listener\",\"streamIdleTimeout\":\"300s\"}}]}],\"name\":\"envoy-prometheus-metrics-listener\"},{\"address\":{\"socketAddress\":{\"address\":\"127.0.0.1\",\"portValue\":9878}},\"filterChains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"httpFilters\":[{\"name\":\"envoy.filters.http.router\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"internalAddressConfig\":{\"cidrRanges\":[{\"addressPrefix\":\"10.0.0.0\",\"prefixLen\":8},{\"addressPrefix\":\"172.16.0.0\",\"prefixLen\":12},{\"addressPrefix\":\"192.168.0.0\",\"prefixLen\":16},{\"addressPrefix\":\"127.0.0.1\",\"prefixLen\":32}]},\"routeConfig\":{\"virtual_hosts\":[{\"domains\":[\"*\"],\"name\":\"health\",\"routes\":[{\"match\":{\"prefix\":\"/healthz\"},\"name\":\"health\",\"route\":{\"cluster\":\"/envoy-admin\",\"prefixRewrite\":\"/ready\"}}]}]},\"statPrefix\":\"envoy-health-listener\",\"streamIdleTimeout\":\"300s\"}}]}],\"name\":\"envoy-health-listener\"}]}}\n---\n# Source: cilium/templates/cilium-agent/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cilium\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  - services\n  - pods\n  - endpoints\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions\n  verbs:\n  - list\n  - watch\n  # This is used when validating policies in preflight. This will need to stay\n  # until we figure out how to avoid \"get\" inside the preflight, and then\n  # should be removed ideally.\n  - get\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumloadbalancerippools\n  - ciliumbgppeeringpolicies\n  - ciliumbgpnodeconfigs\n  - ciliumbgpadvertisements\n  - ciliumbgppeerconfigs\n  - ciliumclusterwideenvoyconfigs\n  - ciliumclusterwidenetworkpolicies\n  - ciliumegressgatewaypolicies\n  - ciliumendpoints\n  - ciliumendpointslices\n  - ciliumenvoyconfigs\n  - ciliumidentities\n  - ciliumlocalredirectpolicies\n  - ciliumnetworkpolicies\n  - ciliumnodes\n  - ciliumnodeconfigs\n  - ciliumcidrgroups\n  - ciliuml2announcementpolicies\n  - ciliumpodippools\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumidentities\n  - ciliumendpoints\n  - ciliumnodes\n  verbs:\n  - create\n- apiGroups:\n  - cilium.io\n  # To synchronize garbage collection of such resources\n  resources:\n  - ciliumidentities\n  verbs:\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpoints\n  verbs:\n  - delete\n  - get\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnodes\n  - ciliumnodes/status\n  verbs:\n  - get\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpoints/status\n  - ciliumendpoints\n  - ciliuml2announcementpolicies/status\n  - ciliumbgpnodeconfigs/status\n  verbs:\n  - patch\n---\n# Source: cilium/templates/cilium-operator/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cilium-operator\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n  # to automatically delete [core|kube]dns pods so that are starting to being\n  # managed by Cilium\n  - delete\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  resourceNames:\n  - cilium-config\n  verbs:\n   # allow patching of the configmap to set annotations\n  - patch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  # To remove node taints\n  - nodes\n  # To set NetworkUnavailable false on startup\n  - nodes/status\n  verbs:\n  - patch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  # to perform LB IP allocation for BGP\n  - services/status\n  verbs:\n  - update\n  - patch\n- apiGroups:\n  - \"\"\n  resources:\n  # to check apiserver connectivity\n  - namespaces\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  # to perform the translation of a CNP that contains `ToGroup` to its endpoints\n  - services\n  - endpoints\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnetworkpolicies\n  - ciliumclusterwidenetworkpolicies\n  verbs:\n  # Create auto-generated CNPs and CCNPs from Policies that have 'toGroups'\n  - create\n  - update\n  - deletecollection\n  # To update the status of the CNPs and CCNPs\n  - patch\n  - get\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnetworkpolicies/status\n  - ciliumclusterwidenetworkpolicies/status\n  verbs:\n  # Update the auto-generated CNPs and CCNPs status.\n  - patch\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpoints\n  - ciliumidentities\n  verbs:\n  # To perform garbage collection of such resources\n  - delete\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumidentities\n  verbs:\n  # To synchronize garbage collection of such resources\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnodes\n  verbs:\n  - create\n  - update\n  - get\n  - list\n  - watch\n    # To perform CiliumNode garbage collector\n  - delete\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnodes/status\n  verbs:\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpointslices\n  - ciliumenvoyconfigs\n  - ciliumbgppeerconfigs\n  - ciliumbgpadvertisements\n  - ciliumbgpnodeconfigs\n  verbs:\n  - create\n  - update\n  - get\n  - list\n  - watch\n  - delete\n  - patch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumbgpclusterconfigs/status\n  - ciliumbgppeerconfigs/status\n  verbs:\n  - update\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions\n  verbs:\n  - create\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions\n  verbs:\n  - update\n  resourceNames:\n  - ciliumloadbalancerippools.cilium.io\n  - ciliumbgppeeringpolicies.cilium.io\n  - ciliumbgpclusterconfigs.cilium.io\n  - ciliumbgppeerconfigs.cilium.io\n  - ciliumbgpadvertisements.cilium.io\n  - ciliumbgpnodeconfigs.cilium.io\n  - ciliumbgpnodeconfigoverrides.cilium.io\n  - ciliumclusterwideenvoyconfigs.cilium.io\n  - ciliumclusterwidenetworkpolicies.cilium.io\n  - ciliumegressgatewaypolicies.cilium.io\n  - ciliumendpoints.cilium.io\n  - ciliumendpointslices.cilium.io\n  - ciliumenvoyconfigs.cilium.io\n  - ciliumidentities.cilium.io\n  - ciliumlocalredirectpolicies.cilium.io\n  - ciliumnetworkpolicies.cilium.io\n  - ciliumnodes.cilium.io\n  - ciliumnodeconfigs.cilium.io\n  - ciliumcidrgroups.cilium.io\n  - ciliuml2announcementpolicies.cilium.io\n  - ciliumpodippools.cilium.io\n  - ciliumgatewayclassconfigs.cilium.io\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumloadbalancerippools\n  - ciliumpodippools\n  - ciliumbgppeeringpolicies\n  - ciliumbgpclusterconfigs\n  - ciliumbgpnodeconfigoverrides\n  - ciliumbgppeerconfigs\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n    - cilium.io\n  resources:\n    - ciliumpodippools\n  verbs:\n    - create\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumloadbalancerippools/status\n  verbs:\n  - patch\n# For cilium-operator running in HA mode.\n#\n# Cilium operator running in HA mode requires the use of ResourceLock for Leader Election\n# between multiple running instances.\n# The preferred way of doing this is to use LeasesResourceLock as edits to Leases are less\n# common and fewer objects in the cluster watch \"all Leases\".\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - create\n  - get\n  - update\n---\n# Source: cilium/templates/cilium-agent/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cilium\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cilium\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium\"\n  namespace: kube-system\n---\n# Source: cilium/templates/cilium-operator/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cilium-operator\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cilium-operator\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium-operator\"\n  namespace: kube-system\n","manifests":{"templates/cilium-agent/clusterrole.yaml":"---\n# Source: cilium/templates/cilium-agent/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cilium\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  - services\n  - pods\n  - endpoints\n  - nodes\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions\n  verbs:\n  - list\n  - watch\n  # This is used when validating policies in preflight. This will need to stay\n  # until we figure out how to avoid \"get\" inside the preflight, and then\n  # should be removed ideally.\n  - get\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumloadbalancerippools\n  - ciliumbgppeeringpolicies\n  - ciliumbgpnodeconfigs\n  - ciliumbgpadvertisements\n  - ciliumbgppeerconfigs\n  - ciliumclusterwideenvoyconfigs\n  - ciliumclusterwidenetworkpolicies\n  - ciliumegressgatewaypolicies\n  - ciliumendpoints\n  - ciliumendpointslices\n  - ciliumenvoyconfigs\n  - ciliumidentities\n  - ciliumlocalredirectpolicies\n  - ciliumnetworkpolicies\n  - ciliumnodes\n  - ciliumnodeconfigs\n  - ciliumcidrgroups\n  - ciliuml2announcementpolicies\n  - ciliumpodippools\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumidentities\n  - ciliumendpoints\n  - ciliumnodes\n  verbs:\n  - create\n- apiGroups:\n  - cilium.io\n  # To synchronize garbage collection of such resources\n  resources:\n  - ciliumidentities\n  verbs:\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpoints\n  verbs:\n  - delete\n  - get\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnodes\n  - ciliumnodes/status\n  verbs:\n  - get\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpoints/status\n  - ciliumendpoints\n  - ciliuml2announcementpolicies/status\n  - ciliumbgpnodeconfigs/status\n  verbs:\n  - patch\n","templates/cilium-agent/clusterrolebinding.yaml":"---\n# Source: cilium/templates/cilium-agent/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cilium\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cilium\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium\"\n  namespace: kube-system\n","templates/cilium-agent/daemonset.yaml":"---\n# Source: cilium/templates/cilium-agent/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9962\"\n        prometheus.io/scrape: \"true\"\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: \"brief\"\n              value: \"true\"\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: \"brief\"\n              value: \"true\"\n            - name: \"require-k8s-connectivity\"\n              value: \"false\"\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: \"brief\"\n              value: \"true\"\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: \"1\"\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: \"120\"\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - \"bash\"\n              - \"-c\"\n              - |\n                    set -o errexit\n                    set -o pipefail\n                    set -o nounset\n                    \n                    # When running in AWS ENI mode, it's likely that 'aws-node' has\n                    # had a chance to install SNAT iptables rules. These can result\n                    # in dropped traffic, so we should attempt to remove them.\n                    # We do it using a 'postStart' hook since this may need to run\n                    # for nodes which might have already been init'ed but may still\n                    # have dangling rules. This is safe because there are no\n                    # dependencies on anything that is part of the startup script\n                    # itself, and can be safely run multiple times per node (e.g. in\n                    # case of a restart).\n                    if [[ \"$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\" != \"0\" ]];\n                    then\n                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'\n                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore\n                    fi\n                    echo 'Done!'\n                    \n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        ports:\n        - name: peer-service\n          containerPort: 4244\n          hostPort: 4244\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9962\n          hostPort: 9962\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - CHOWN\n              - KILL\n              - NET_ADMIN\n              - NET_RAW\n              - IPC_LOCK\n              - SYS_ADMIN\n              - SYS_RESOURCE\n              - DAC_OVERRIDE\n              - FOWNER\n              - SETGID\n              - SETUID\n            drop:\n              - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        # Unprivileged containers need to mount /proc/sys/net from the host\n        # to have write access\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        # Unprivileged containers need to mount /proc/sys/kernel from the host\n        # to have write access\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          # Unprivileged containers can't set mount propagation to bidirectional\n          # in this case we will mount the bpf fs from an init container that\n          # is privileged and set the mount propagation from host to container\n          # in Cilium.\n          mountPropagation: HostToContainer\n        # Check for duplicate mounts before mounting\n        - name: cilium-cgroup\n          mountPath: /sys/fs/cgroup\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n          # Needed to be able to load kernel modules\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: tmp\n          mountPath: /tmp\n        \n      initContainers:\n      - name: config\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n      - name: apply-sysctl-overwrites\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        # The statically linked Go program binary is invoked to avoid any\n        # dependency on utilities like sh that can be missing on certain\n        # distros installed on the underlying host. Copy the binary to the\n        # same directory where we install cilium cni plugin so that exec permissions\n        # are available.\n        - |\n          cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n          rm /hostbin/cilium-sysctlfix\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - SYS_ADMIN\n              - SYS_CHROOT\n              - SYS_PTRACE\n            drop:\n              - ALL\n      # Mount the bpf fs if it is not mounted. We will perform this task\n      # from a privileged container because the mount propagation bidirectional\n      # only works from privileged containers.\n      - name: mount-bpf-fs\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        args:\n        - 'mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf'\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - NET_ADMIN\n              - SYS_ADMIN\n              - SYS_RESOURCE\n            drop:\n              - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          # Required to mount cgroup filesystem from the host to cilium agent pod\n        - name: cilium-cgroup\n          mountPath: /sys/fs/cgroup\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium # wait-for-kube-proxy\n      # Install the CNI binaries in an InitContainer so we don't have a writable host mount in the agent\n      - name: install-cni-binaries\n        image: \"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\"\n        imagePullPolicy: IfNotPresent\n        command:\n          - \"/install-plugin.sh\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n              - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n          - name: cni-path\n            mountPath: /host/opt/cni/bin # .Values.cni.install\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: \"cilium\"\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - operator: Exists\n      volumes:\n        # For sharing configuration between the \"config\" initContainer and the agent\n      - name: tmp\n        emptyDir: {}\n        # To keep state between restarts / upgrades\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        # To exec into pod network namespaces\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n        # To keep state between restarts / upgrades for bpf maps\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      # To mount cgroup2 filesystem on the host or apply sysctlfix\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      # To keep state between restarts / upgrades for cgroup2 filesystem\n      - name: cilium-cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n          type: DirectoryOrCreate\n      # To install cilium cni plugin in the host\n      - name: cni-path\n        hostPath:\n          path:  /opt/cni/bin\n          type: DirectoryOrCreate\n        # To install cilium cni configuration in the host\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        # To be able to load kernel modules\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n        # To access iptables concurrently with other processes (e.g. kube-proxy)\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      # Sharing socket with Cilium Envoy on the same node by using a host path\n      - name: envoy-sockets\n        hostPath:\n          path: \"/var/run/cilium/envoy/sockets\"\n          type: DirectoryOrCreate\n        # To read the clustermesh configuration\n      - name: clustermesh-secrets\n        projected:\n          # note: the leading zero means this number is in octal representation: do not remove it\n          defaultMode: 0400\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n              # note: items are not explicitly listed here, since the entries of this secret\n              # depend on the peers configured, and that would cause a restart of all agents\n              # at every addition/removal. Leaving the field empty makes each secret entry\n              # to be automatically projected into the volume as a file whose name is the key.\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          # note: we configure the volume for the kvstoremesh-specific certificate\n          # regardless of whether KVStoreMesh is enabled or not, so that it can be\n          # automatically mounted in case KVStoreMesh gets subsequently enabled,\n          # without requiring an agent restart.\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n","templates/cilium-agent/role.yaml":"---\n# Source: cilium/templates/cilium-agent/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cilium-config-agent\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  verbs:\n  - get\n  - list\n  - watch\n---\n# Source: cilium/templates/cilium-agent/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cilium-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\n","templates/cilium-agent/rolebinding.yaml":"---\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cilium-config-agent\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cilium-config-agent\nsubjects:\n  - kind: ServiceAccount\n    name: \"cilium\"\n    namespace: kube-system\n---\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cilium-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cilium-tlsinterception-secrets\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium\"\n  namespace: kube-system\n","templates/cilium-agent/serviceaccount.yaml":"---\n# Source: cilium/templates/cilium-agent/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"cilium\"\n  namespace: kube-system\n","templates/cilium-configmap.yaml":"---\n# Source: cilium/templates/cilium-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n\n  # Identity allocation mode selects how identities are shared between cilium\n  # nodes by setting how they are stored. The options are \"crd\", \"kvstore\" or\n  # \"doublewrite-readkvstore\" / \"doublewrite-readcrd\".\n  # - \"crd\" stores identities in kubernetes as CRDs (custom resource definition).\n  #   These can be queried with:\n  #     kubectl get ciliumid\n  # - \"kvstore\" stores identities in an etcd kvstore, that is\n  #   configured below. Cilium versions before 1.6 supported only the kvstore\n  #   backend. Upgrades from these older cilium versions should continue using\n  #   the kvstore by commenting out the identity-allocation-mode below, or\n  #   setting it to \"kvstore\".\n  # - \"doublewrite\" modes store identities in both the kvstore and CRDs. This is useful\n  #   for seamless migrations from the kvstore mode to the crd mode. Consult the\n  #   documentation for more information on how to perform the migration.\n  identity-allocation-mode: crd\n\n  identity-heartbeat-timeout: \"30m0s\"\n  identity-gc-interval: \"15m0s\"\n  cilium-endpoint-gc-interval: \"5m0s\"\n  nodes-gc-interval: \"5m0s\"\n\n  # If you want to run cilium in debug mode change this value to true\n  debug: \"false\"\n  debug-verbose: \"\"\n  metrics-sampling-interval: \"5m\"\n  # The agent can be put into the following three policy enforcement modes\n  # default, always and never.\n  # https://docs.cilium.io/en/latest/security/policy/intro/#policy-enforcement-modes\n  enable-policy: \"default\"\n  policy-cidr-match-mode: \"\"\n  # If you want metrics enabled in all of your Cilium agents, set the port for\n  # which the Cilium agents will have their metrics exposed.\n  # This option deprecates the \"prometheus-serve-addr\" in the\n  # \"cilium-metrics-config\" ConfigMap\n  # NOTE that this will open the port on ALL nodes where Cilium pods are\n  # scheduled.\n  prometheus-serve-addr: \":9962\"\n  # A space-separated list of controller groups for which to enable metrics.\n  # The special values of \"all\" and \"none\" are supported.\n  controller-group-metrics:\n    write-cni-file\n    sync-host-ips\n    sync-lb-maps-with-k8s-services\n  # If you want metrics enabled in cilium-operator, set the port for\n  # which the Cilium Operator will have their metrics exposed.\n  # NOTE that this will open the port on the nodes where Cilium operator pod\n  # is scheduled.\n  operator-prometheus-serve-addr: \":9963\"\n  enable-metrics: \"true\"\n  enable-policy-secrets-sync: \"true\"\n  policy-secrets-only-from-secrets-namespace: \"true\"\n  policy-secrets-namespace: \"cilium-secrets\"\n\n  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4\n  # address.\n  enable-ipv4: \"true\"\n\n  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6\n  # address.\n  enable-ipv6: \"false\"\n  # Users who wish to specify their own custom CNI configuration file must set\n  # custom-cni-conf to \"true\", otherwise Cilium may overwrite the configuration.\n  custom-cni-conf: \"false\"\n  enable-bpf-clock-probe: \"false\"\n  # If you want cilium monitor to aggregate tracing for packets, set this level\n  # to \"low\", \"medium\", or \"maximum\". The higher the level, the less packets\n  # that will be seen in monitor output.\n  monitor-aggregation: medium\n\n  # The monitor aggregation interval governs the typical time between monitor\n  # notification events for each allowed connection.\n  #\n  # Only effective when monitor aggregation is set to \"medium\" or higher.\n  monitor-aggregation-interval: \"5s\"\n\n  # The monitor aggregation flags determine which TCP flags which, upon the\n  # first observation, cause monitor notifications to be generated.\n  #\n  # Only effective when monitor aggregation is set to \"medium\" or higher.\n  monitor-aggregation-flags: all\n  # Specifies the ratio (0.0-1.0] of total system memory to use for dynamic\n  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.\n  bpf-map-dynamic-size-ratio: \"0.0025\"\n  enable-host-legacy-routing: \"false\"\n  # bpf-policy-map-max specifies the maximum number of entries in endpoint\n  # policy map (per endpoint)\n  bpf-policy-map-max: \"16384\"\n  # bpf-policy-stats-map-max specifies the maximum number of entries in global\n  # policy stats map\n  bpf-policy-stats-map-max: \"65536\"\n  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,\n  # backend and affinity maps.\n  bpf-lb-map-max: \"65536\"\n  bpf-lb-external-clusterip: \"false\"\n  bpf-lb-source-range-all-types: \"false\"\n  bpf-lb-algorithm-annotation: \"false\"\n  bpf-lb-mode-annotation: \"false\"\n\n  bpf-distributed-lru: \"false\"\n  bpf-events-drop-enabled: \"true\"\n  bpf-events-policy-verdict-enabled: \"true\"\n  bpf-events-trace-enabled: \"true\"\n\n  # Pre-allocation of map entries allows per-packet latency to be reduced, at\n  # the expense of up-front memory allocation for the entries in the maps. The\n  # default value below will minimize memory usage in the default installation;\n  # users who are sensitive to latency may consider setting this to \"true\".\n  #\n  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore\n  # this option and behave as though it is set to \"true\".\n  #\n  # If this value is modified, then during the next Cilium startup the restore\n  # of existing endpoints and tracking of ongoing connections may be disrupted.\n  # As a result, reply packets may be dropped and the load-balancing decisions\n  # for established connections may change.\n  #\n  # If this option is set to \"false\" during an upgrade from 1.3 or earlier to\n  # 1.4 or later, then it may cause one-time disruptions during the upgrade.\n  preallocate-bpf-maps: \"false\"\n\n  # Name of the cluster. Only relevant when building a mesh of clusters.\n  cluster-name: \"default\"\n  # Unique ID of the cluster. Must be unique across all conneted clusters and\n  # in the range of 1 and 255. Only relevant when building a mesh of clusters.\n  cluster-id: \"0\"\n\n  # Encapsulation mode for communication between nodes\n  # Possible values:\n  #   - disabled\n  #   - vxlan (default)\n  #   - geneve\n\n  routing-mode: \"native\"\n  tunnel-protocol: \"vxlan\"\n  tunnel-source-port-range: \"0-0\"\n  service-no-backend-response: \"reject\"\n\n\n  # Enables L7 proxy for L7 policy enforcement and visibility\n  enable-l7-proxy: \"true\"\n  enable-ipv4-masquerade: \"true\"\n  enable-ipv4-big-tcp: \"false\"\n  enable-ipv6-big-tcp: \"false\"\n  enable-ipv6-masquerade: \"true\"\n  enable-tcx: \"true\"\n  datapath-mode: \"veth\"\n  enable-bpf-masquerade: \"true\"\n  enable-masquerade-to-route-source: \"false\"\n  enable-wireguard: \"true\"\n  wireguard-persistent-keepalive: \"0s\"\n\n  enable-xt-socket-fallback: \"true\"\n  install-no-conntrack-iptables-rules: \"true\"\n  iptables-random-fully: \"false\"\n\n  auto-direct-node-routes: \"false\"\n  direct-routing-skip-unreachable: \"false\"\n\n\n  ipv4-native-routing-cidr: 10.0.0.0/16\n\n  kube-proxy-replacement: \"true\"\n  kube-proxy-replacement-healthz-bind-address: \"0.0.0.0:10256\"\n  bpf-lb-sock: \"false\"\n  nodeport-addresses: \"\"\n  enable-health-check-nodeport: \"true\"\n  enable-health-check-loadbalancer-ip: \"false\"\n  node-port-bind-protection: \"true\"\n  enable-auto-protect-node-port-range: \"true\"\n  bpf-lb-acceleration: \"native\"\n  enable-svc-source-range-check: \"true\"\n  enable-l2-neigh-discovery: \"false\"\n  k8s-require-ipv4-pod-cidr: \"true\"\n  k8s-require-ipv6-pod-cidr: \"false\"\n  enable-k8s-networkpolicy: \"true\"\n  enable-endpoint-lockdown-on-policy-overflow: \"false\"\n  # Tell the agent to generate and write a CNI configuration file\n  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist\n  cni-exclusive: \"true\"\n  cni-log-file: \"/var/run/cilium/cilium-cni.log\"\n  enable-endpoint-health-checking: \"true\"\n  enable-health-checking: \"true\"\n  health-check-icmp-failure-threshold: \"3\"\n  enable-well-known-identities: \"false\"\n  enable-node-selector-labels: \"false\"\n  synchronize-k8s-nodes: \"true\"\n  operator-api-serve-addr: \"127.0.0.1:9234\"\n\n  enable-hubble: \"false\"\n  ipam: \"kubernetes\"\n  ipam-cilium-node-update-rate: \"15s\"\n\n  default-lb-service-ipam: \"lbipam\"\n  egress-gateway-reconciliation-trigger-interval: \"1s\"\n  enable-vtep: \"false\"\n  vtep-endpoint: \"\"\n  vtep-cidr: \"\"\n  vtep-mask: \"\"\n  vtep-mac: \"\"\n  procfs: \"/host/proc\"\n  bpf-root: \"/sys/fs/bpf\"\n  cgroup-root: \"/sys/fs/cgroup\"\n\n  identity-management-mode: \"agent\"\n  enable-sctp: \"false\"\n  remove-cilium-node-taints: \"true\"\n  set-cilium-node-taints: \"true\"\n  set-cilium-is-up-condition: \"true\"\n  unmanaged-pod-watcher-interval: \"15\"\n  # explicit setting gets precedence\n  dnsproxy-enable-transparent-mode: \"true\"\n  dnsproxy-socket-linger-timeout: \"10\"\n  tofqdns-dns-reject-response-code: \"refused\"\n  tofqdns-enable-dns-compression: \"true\"\n  tofqdns-endpoint-max-ip-per-hostname: \"1000\"\n  tofqdns-idle-connection-grace-period: \"0s\"\n  tofqdns-max-deferred-connection-deletes: \"10000\"\n  tofqdns-proxy-response-max-delay: \"100ms\"\n  tofqdns-preallocate-identities:  \"true\"\n  agent-not-ready-taint-key: \"node.cilium.io/agent-not-ready\"\n\n  mesh-auth-enabled: \"true\"\n  mesh-auth-queue-size: \"1024\"\n  mesh-auth-rotated-identities-queue-size: \"1024\"\n  mesh-auth-gc-interval: \"5m0s\"\n\n  proxy-xff-num-trusted-hops-ingress: \"0\"\n  proxy-xff-num-trusted-hops-egress: \"0\"\n  proxy-connect-timeout: \"2\"\n  proxy-initial-fetch-timeout: \"30\"\n  proxy-max-requests-per-connection: \"0\"\n  proxy-max-connection-duration-seconds: \"0\"\n  proxy-idle-timeout-seconds: \"60\"\n  proxy-max-concurrent-retries: \"128\"\n  http-retry-count: \"3\"\n\n  external-envoy-proxy: \"true\"\n  envoy-base-id: \"0\"\n  envoy-access-log-buffer-size: \"4096\"\n  envoy-keep-cap-netbindservice: \"false\"\n  max-connected-clusters: \"255\"\n  clustermesh-enable-endpoint-sync: \"false\"\n  clustermesh-enable-mcs-api: \"false\"\n  policy-default-local-cluster: \"false\"\n\n  nat-map-stats-entries: \"32\"\n  nat-map-stats-interval: \"30s\"\n  enable-internal-traffic-policy: \"true\"\n  enable-lb-ipam: \"true\"\n  enable-non-default-deny-policies: \"true\"\n  enable-source-ip-verification: \"true\"\n\n# Extra config allows adding arbitrary properties to the cilium config.\n# By putting it at the end of the ConfigMap, it's also possible to override existing properties.\n","templates/cilium-envoy/configmap.yaml":"---\n# Source: cilium/templates/cilium-envoy/configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-envoy-config\n  namespace: kube-system\ndata:\n  # Keep the key name as bootstrap-config.json to avoid breaking changes\n  bootstrap-config.json: |\n    {\"admin\":{\"address\":{\"pipe\":{\"path\":\"/var/run/cilium/envoy/sockets/admin.sock\"}}},\"applicationLogConfig\":{\"logFormat\":{\"textFormat\":\"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v\"}},\"bootstrapExtensions\":[{\"name\":\"envoy.bootstrap.internal_listener\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener\"}}],\"dynamicResources\":{\"cdsConfig\":{\"apiConfigSource\":{\"apiType\":\"GRPC\",\"grpcServices\":[{\"envoyGrpc\":{\"clusterName\":\"xds-grpc-cilium\"}}],\"setNodeOnFirstMessageOnly\":true,\"transportApiVersion\":\"V3\"},\"initialFetchTimeout\":\"30s\",\"resourceApiVersion\":\"V3\"},\"ldsConfig\":{\"apiConfigSource\":{\"apiType\":\"GRPC\",\"grpcServices\":[{\"envoyGrpc\":{\"clusterName\":\"xds-grpc-cilium\"}}],\"setNodeOnFirstMessageOnly\":true,\"transportApiVersion\":\"V3\"},\"initialFetchTimeout\":\"30s\",\"resourceApiVersion\":\"V3\"}},\"node\":{\"cluster\":\"ingress-cluster\",\"id\":\"host~127.0.0.1~no-id~localdomain\"},\"overloadManager\":{\"resourceMonitors\":[{\"name\":\"envoy.resource_monitors.global_downstream_max_connections\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig\",\"max_active_downstream_connections\":\"50000\"}}]},\"staticResources\":{\"clusters\":[{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"ingress-cluster\",\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"useDownstreamProtocolConfig\":{}}}},{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"egress-cluster-tls\",\"transportSocket\":{\"name\":\"cilium.tls_wrapper\",\"typedConfig\":{\"@type\":\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\"}},\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"upstreamHttpProtocolOptions\":{},\"useDownstreamProtocolConfig\":{}}}},{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"egress-cluster\",\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"useDownstreamProtocolConfig\":{}}}},{\"circuitBreakers\":{\"thresholds\":[{\"maxRetries\":128}]},\"cleanupInterval\":\"2.500s\",\"connectTimeout\":\"2s\",\"lbPolicy\":\"CLUSTER_PROVIDED\",\"name\":\"ingress-cluster-tls\",\"transportSocket\":{\"name\":\"cilium.tls_wrapper\",\"typedConfig\":{\"@type\":\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\"}},\"type\":\"ORIGINAL_DST\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"commonHttpProtocolOptions\":{\"idleTimeout\":\"60s\",\"maxConnectionDuration\":\"0s\",\"maxRequestsPerConnection\":0},\"upstreamHttpProtocolOptions\":{},\"useDownstreamProtocolConfig\":{}}}},{\"connectTimeout\":\"2s\",\"loadAssignment\":{\"clusterName\":\"xds-grpc-cilium\",\"endpoints\":[{\"lbEndpoints\":[{\"endpoint\":{\"address\":{\"pipe\":{\"path\":\"/var/run/cilium/envoy/sockets/xds.sock\"}}}}]}]},\"name\":\"xds-grpc-cilium\",\"type\":\"STATIC\",\"typedExtensionProtocolOptions\":{\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\":{\"@type\":\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\",\"explicitHttpConfig\":{\"http2ProtocolOptions\":{}}}}},{\"connectTimeout\":\"2s\",\"loadAssignment\":{\"clusterName\":\"/envoy-admin\",\"endpoints\":[{\"lbEndpoints\":[{\"endpoint\":{\"address\":{\"pipe\":{\"path\":\"/var/run/cilium/envoy/sockets/admin.sock\"}}}}]}]},\"name\":\"/envoy-admin\",\"type\":\"STATIC\"}],\"listeners\":[{\"address\":{\"socketAddress\":{\"address\":\"0.0.0.0\",\"portValue\":9964}},\"filterChains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"httpFilters\":[{\"name\":\"envoy.filters.http.router\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"internalAddressConfig\":{\"cidrRanges\":[{\"addressPrefix\":\"10.0.0.0\",\"prefixLen\":8},{\"addressPrefix\":\"172.16.0.0\",\"prefixLen\":12},{\"addressPrefix\":\"192.168.0.0\",\"prefixLen\":16},{\"addressPrefix\":\"127.0.0.1\",\"prefixLen\":32}]},\"routeConfig\":{\"virtualHosts\":[{\"domains\":[\"*\"],\"name\":\"prometheus_metrics_route\",\"routes\":[{\"match\":{\"prefix\":\"/metrics\"},\"name\":\"prometheus_metrics_route\",\"route\":{\"cluster\":\"/envoy-admin\",\"prefixRewrite\":\"/stats/prometheus\"}}]}]},\"statPrefix\":\"envoy-prometheus-metrics-listener\",\"streamIdleTimeout\":\"300s\"}}]}],\"name\":\"envoy-prometheus-metrics-listener\"},{\"address\":{\"socketAddress\":{\"address\":\"127.0.0.1\",\"portValue\":9878}},\"filterChains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"httpFilters\":[{\"name\":\"envoy.filters.http.router\",\"typedConfig\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"internalAddressConfig\":{\"cidrRanges\":[{\"addressPrefix\":\"10.0.0.0\",\"prefixLen\":8},{\"addressPrefix\":\"172.16.0.0\",\"prefixLen\":12},{\"addressPrefix\":\"192.168.0.0\",\"prefixLen\":16},{\"addressPrefix\":\"127.0.0.1\",\"prefixLen\":32}]},\"routeConfig\":{\"virtual_hosts\":[{\"domains\":[\"*\"],\"name\":\"health\",\"routes\":[{\"match\":{\"prefix\":\"/healthz\"},\"name\":\"health\",\"route\":{\"cluster\":\"/envoy-admin\",\"prefixRewrite\":\"/ready\"}}]}]},\"statPrefix\":\"envoy-health-listener\",\"streamIdleTimeout\":\"300s\"}}]}],\"name\":\"envoy-health-listener\"}]}}\n","templates/cilium-envoy/daemonset.yaml":"---\n# Source: cilium/templates/cilium-envoy/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: kube-system\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: \"quay.io/cilium/cilium-envoy:v1.34.4-1754895458-68cffdfa568b6b226d70a7ef81fc65dda3b890bf@sha256:247e908700012f7ef56f75908f8c965215c26a27762f296068645eb55450bda2\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - '--'\n        - '-c /var/run/cilium/envoy/bootstrap-config.json'\n        - '--base-id 0'\n        - '--log-level info'\n        startupProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n              - NET_ADMIN\n              - SYS_ADMIN\n            drop:\n              - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: \"cilium-envoy\"\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - \"true\"\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: \"/var/run/cilium/envoy/sockets\"\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: \"/var/run/cilium/envoy/artifacts\"\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: \"cilium-envoy-config\"\n          # note: the leading zero means this number is in octal representation: do not remove it\n          defaultMode: 0400\n          items:\n            - key: bootstrap-config.json\n              path: bootstrap-config.json\n        # To keep state between restarts / upgrades\n        # To keep state between restarts / upgrades for bpf maps\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n","templates/cilium-envoy/service.yaml":"---\n# Source: cilium/templates/cilium-envoy/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: cilium-envoy\n  namespace: kube-system\n  annotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"9964\"\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/name: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    io.cilium/app: proxy\nspec:\n  clusterIP: None\n  type: ClusterIP\n  selector:\n    k8s-app: cilium-envoy\n  ports:\n  - name: envoy-metrics\n    port: 9964\n    protocol: TCP\n    targetPort: envoy-metrics\n","templates/cilium-envoy/serviceaccount.yaml":"---\n# Source: cilium/templates/cilium-envoy/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"cilium-envoy\"\n  namespace: kube-system\n","templates/cilium-operator/clusterrole.yaml":"---\n# Source: cilium/templates/cilium-operator/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cilium-operator\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n  # to automatically delete [core|kube]dns pods so that are starting to being\n  # managed by Cilium\n  - delete\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  resourceNames:\n  - cilium-config\n  verbs:\n   # allow patching of the configmap to set annotations\n  - patch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  # To remove node taints\n  - nodes\n  # To set NetworkUnavailable false on startup\n  - nodes/status\n  verbs:\n  - patch\n- apiGroups:\n  - discovery.k8s.io\n  resources:\n  - endpointslices\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  # to perform LB IP allocation for BGP\n  - services/status\n  verbs:\n  - update\n  - patch\n- apiGroups:\n  - \"\"\n  resources:\n  # to check apiserver connectivity\n  - namespaces\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  # to perform the translation of a CNP that contains `ToGroup` to its endpoints\n  - services\n  - endpoints\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnetworkpolicies\n  - ciliumclusterwidenetworkpolicies\n  verbs:\n  # Create auto-generated CNPs and CCNPs from Policies that have 'toGroups'\n  - create\n  - update\n  - deletecollection\n  # To update the status of the CNPs and CCNPs\n  - patch\n  - get\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnetworkpolicies/status\n  - ciliumclusterwidenetworkpolicies/status\n  verbs:\n  # Update the auto-generated CNPs and CCNPs status.\n  - patch\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpoints\n  - ciliumidentities\n  verbs:\n  # To perform garbage collection of such resources\n  - delete\n  - list\n  - watch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumidentities\n  verbs:\n  # To synchronize garbage collection of such resources\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnodes\n  verbs:\n  - create\n  - update\n  - get\n  - list\n  - watch\n    # To perform CiliumNode garbage collector\n  - delete\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumnodes/status\n  verbs:\n  - update\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumendpointslices\n  - ciliumenvoyconfigs\n  - ciliumbgppeerconfigs\n  - ciliumbgpadvertisements\n  - ciliumbgpnodeconfigs\n  verbs:\n  - create\n  - update\n  - get\n  - list\n  - watch\n  - delete\n  - patch\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumbgpclusterconfigs/status\n  - ciliumbgppeerconfigs/status\n  verbs:\n  - update\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions\n  verbs:\n  - create\n  - get\n  - list\n  - watch\n- apiGroups:\n  - apiextensions.k8s.io\n  resources:\n  - customresourcedefinitions\n  verbs:\n  - update\n  resourceNames:\n  - ciliumloadbalancerippools.cilium.io\n  - ciliumbgppeeringpolicies.cilium.io\n  - ciliumbgpclusterconfigs.cilium.io\n  - ciliumbgppeerconfigs.cilium.io\n  - ciliumbgpadvertisements.cilium.io\n  - ciliumbgpnodeconfigs.cilium.io\n  - ciliumbgpnodeconfigoverrides.cilium.io\n  - ciliumclusterwideenvoyconfigs.cilium.io\n  - ciliumclusterwidenetworkpolicies.cilium.io\n  - ciliumegressgatewaypolicies.cilium.io\n  - ciliumendpoints.cilium.io\n  - ciliumendpointslices.cilium.io\n  - ciliumenvoyconfigs.cilium.io\n  - ciliumidentities.cilium.io\n  - ciliumlocalredirectpolicies.cilium.io\n  - ciliumnetworkpolicies.cilium.io\n  - ciliumnodes.cilium.io\n  - ciliumnodeconfigs.cilium.io\n  - ciliumcidrgroups.cilium.io\n  - ciliuml2announcementpolicies.cilium.io\n  - ciliumpodippools.cilium.io\n  - ciliumgatewayclassconfigs.cilium.io\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumloadbalancerippools\n  - ciliumpodippools\n  - ciliumbgppeeringpolicies\n  - ciliumbgpclusterconfigs\n  - ciliumbgpnodeconfigoverrides\n  - ciliumbgppeerconfigs\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n    - cilium.io\n  resources:\n    - ciliumpodippools\n  verbs:\n    - create\n- apiGroups:\n  - cilium.io\n  resources:\n  - ciliumloadbalancerippools/status\n  verbs:\n  - patch\n# For cilium-operator running in HA mode.\n#\n# Cilium operator running in HA mode requires the use of ResourceLock for Leader Election\n# between multiple running instances.\n# The preferred way of doing this is to use LeasesResourceLock as edits to Leases are less\n# common and fewer objects in the cluster watch \"all Leases\".\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - create\n  - get\n  - update\n","templates/cilium-operator/clusterrolebinding.yaml":"---\n# Source: cilium/templates/cilium-operator/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cilium-operator\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cilium-operator\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium-operator\"\n  namespace: kube-system\n","templates/cilium-operator/deployment.yaml":"---\n# Source: cilium/templates/cilium-operator/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: kube-system\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  # See docs on ServerCapabilities.LeasesResourceLock in file pkg/k8s/version/version.go\n  # for more details.\n  replicas: 1\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  # ensure operator update on single node k8s clusters, by using rolling update with maxUnavailable=100% in case\n  # of one replica and no user configured Recreate strategy.\n  # otherwise an update might get stuck due to the default maxUnavailable=50% in combination with the\n  # podAntiAffinity which prevents deployments of multiple operator replicas on the same node.\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 100%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"9963\"\n        prometheus.io/scrape: \"true\"\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: \"quay.io/cilium/operator-generic:v1.18.1@sha256:97f4553afa443465bdfbc1cc4927c93f16ac5d78e4dd2706736e7395382201bc\"\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        - name: KUBERNETES_SERVICE_HOST\n          value: \"127.0.0.1\"\n        - name: KUBERNETES_SERVICE_PORT\n          value: \"7445\"\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: \"127.0.0.1\"\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: \"cilium-operator\"\n      automountServiceAccountToken: true\n      # In HA mode, cilium-operator pods must not be scheduled on the same\n      # node as they will clash with each other.\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n        node-role.kubernetes.io/control-plane: \"\"\n      tolerations:\n        - operator: Exists\n        - key: node.cilium.io/agent-not-ready\n          operator: Exists\n      \n      volumes:\n        # To read the configuration from the config map\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n","templates/cilium-operator/role.yaml":"---\n# Source: cilium/templates/cilium-operator/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cilium-operator-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - secrets\n  verbs:\n  - create\n  - delete\n  - update\n  - patch\n","templates/cilium-operator/rolebinding.yaml":"---\n# Source: cilium/templates/cilium-operator/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cilium-operator-tlsinterception-secrets\n  namespace: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cilium-operator-tlsinterception-secrets\nsubjects:\n- kind: ServiceAccount\n  name: \"cilium-operator\"\n  namespace: kube-system\n","templates/cilium-operator/serviceaccount.yaml":"---\n# Source: cilium/templates/cilium-operator/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: \"cilium-operator\"\n  namespace: kube-system\n","templates/cilium-secrets-namespace.yaml":"---\n# Source: cilium/templates/cilium-secrets-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: \"cilium-secrets\"\n  labels:\n    app.kubernetes.io/part-of: cilium\n  annotations:\n"},"name":"cilium","namespace":"kube-system","notes":"\n    You have successfully installed Cilium.\n\nYour release version is 1.18.1.\n\nFor any further help, visit https://docs.cilium.io/en/v1.18/gettinghelp\n","pass_credentials":null,"postrender":null,"render_subchart_notes":false,"replace":false,"repository":"https://helm.cilium.io","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":null,"set_list":null,"set_sensitive":null,"show_only":null,"skip_crds":false,"skip_tests":false,"timeout":300,"validate":false,"values":["\"bpf\":\n  \"datapathMode\": \"veth\"\n  \"hostLegacyRouting\": false\n  \"masquerade\": true\n\"cgroup\":\n  \"autoMount\":\n    \"enabled\": false\n  \"hostRoot\": \"/sys/fs/cgroup\"\n\"dnsProxy\":\n  \"enableTransparentMode\": true\n\"egressGateway\":\n  \"enabled\": false\n\"encryption\":\n  \"enabled\": true\n  \"type\": \"wireguard\"\n\"hubble\":\n  \"enabled\": false\n  \"peerService\":\n    \"clusterDomain\": \"cluster.local\"\n  \"relay\":\n    \"enabled\": false\n  \"ui\":\n    \"enabled\": false\n\"installNoConntrackIptablesRules\": true\n\"ipam\":\n  \"mode\": \"kubernetes\"\n\"ipv4NativeRoutingCIDR\": \"10.0.0.0/16\"\n\"k8s\":\n  \"requireIPv4PodCIDR\": true\n\"k8sServiceHost\": \"127.0.0.1\"\n\"k8sServicePort\": 7445\n\"kubeProxyReplacement\": true\n\"kubeProxyReplacementHealthzBindAddr\": \"0.0.0.0:10256\"\n\"loadBalancer\":\n  \"acceleration\": \"native\"\n\"operator\":\n  \"nodeSelector\":\n    \"node-role.kubernetes.io/control-plane\": \"\"\n  \"prometheus\":\n    \"enabled\": true\n    \"serviceMonitor\":\n      \"enabled\": false\n      \"interval\": \"15s\"\n  \"replicas\": 1\n  \"tolerations\":\n  - \"operator\": \"Exists\"\n\"prometheus\":\n  \"enabled\": true\n  \"serviceMonitor\":\n    \"enabled\": false\n    \"interval\": \"15s\"\n    \"trustCRDsExist\": false\n\"routingMode\": \"native\"\n\"securityContext\":\n  \"capabilities\":\n    \"ciliumAgent\":\n    - \"CHOWN\"\n    - \"KILL\"\n    - \"NET_ADMIN\"\n    - \"NET_RAW\"\n    - \"IPC_LOCK\"\n    - \"SYS_ADMIN\"\n    - \"SYS_RESOURCE\"\n    - \"DAC_OVERRIDE\"\n    - \"FOWNER\"\n    - \"SETGID\"\n    - \"SETUID\"\n    \"cleanCiliumState\":\n    - \"NET_ADMIN\"\n    - \"SYS_ADMIN\"\n    - \"SYS_RESOURCE\"\n","{}\n"],"verify":false,"version":"1.18.1","wait":false},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"helm_template","name":"cluster_autoscaler","provider":"module.kubernetes.provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"api_versions":null,"atomic":false,"chart":"cluster-autoscaler","crds":[],"create_namespace":false,"dependency_update":false,"description":"","devel":false,"disable_openapi_validation":false,"disable_webhooks":false,"id":"cluster-autoscaler","include_crds":false,"is_upgrade":false,"keyring":"","kube_version":"v1.33.4","manifest":"---\n# Source: cluster-autoscaler/templates/pdb.yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: \"cluster-autoscaler\"\n      app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n---\n# Source: cluster-autoscaler/templates/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nautomountServiceAccountToken: true\n---\n# Source: cluster-autoscaler/templates/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - events\n      - endpoints\n    verbs:\n      - create\n      - patch\n  - apiGroups:\n    - \"\"\n    resources:\n    - pods/eviction\n    verbs:\n    - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods/status\n    verbs:\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - endpoints\n    resourceNames:\n      - cluster-autoscaler\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n    - watch\n    - list\n    - create\n    - delete\n    - get\n    - update\n  - apiGroups:\n    - \"\"\n    resources:\n      - namespaces\n      - pods\n      - services\n      - replicationcontrollers\n      - persistentvolumeclaims\n      - persistentvolumes\n    verbs:\n      - watch\n      - list\n      - get\n  - apiGroups:\n    - batch\n    resources:\n      - jobs\n      - cronjobs\n    verbs:\n      - watch\n      - list\n      - get\n  - apiGroups:\n    - batch\n    - extensions\n    resources:\n    - jobs\n    verbs:\n    - get\n    - list\n    - patch\n    - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - replicasets\n      - daemonsets\n    verbs:\n      - watch\n      - list\n      - get\n  - apiGroups:\n      - policy\n    resources:\n      - poddisruptionbudgets\n    verbs:\n      - watch\n      - list\n  - apiGroups:\n    - apps\n    resources:\n    - daemonsets\n    - replicasets\n    - statefulsets\n    verbs:\n    - watch\n    - list\n    - get\n  - apiGroups:\n    - storage.k8s.io\n    resources:\n    - storageclasses\n    - csinodes\n    - csidrivers\n    - csistoragecapacities\n    - volumeattachments\n    verbs:\n    - watch\n    - list\n    - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - list\n      - watch\n      - get\n  - apiGroups:\n    - coordination.k8s.io\n    resources:\n    - leases\n    verbs:\n    - create\n  - apiGroups:\n    - coordination.k8s.io\n    resourceNames:\n    - cluster-autoscaler\n    resources:\n    - leases\n    verbs:\n    - get\n    - update\n---\n# Source: cluster-autoscaler/templates/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler-hetzner-cluster-autoscaler\n    namespace: kube-system\n---\n# Source: cluster-autoscaler/templates/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    resourceNames:\n      - cluster-autoscaler-status\n    verbs:\n      - delete\n      - get\n      - update\n---\n# Source: cluster-autoscaler/templates/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler-hetzner-cluster-autoscaler\n    namespace: kube-system\n---\n# Source: cluster-autoscaler/templates/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nspec:\n  ports:\n    - port: 8085\n      protocol: TCP\n      targetPort: 8085\n      name: http\n  selector:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n  type: \"ClusterIP\"\n---\n# Source: cluster-autoscaler/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    {}\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: \"cluster-autoscaler\"\n      app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: \"cluster-autoscaler\"\n        app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    spec:\n      priorityClassName: \"system-cluster-critical\"\n      dnsPolicy: \"ClusterFirst\"\n      containers:\n        - name: hetzner-cluster-autoscaler\n          image: \"registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          command:\n            - ./cluster-autoscaler\n            - --cloud-provider=hetzner\n            - --namespace=kube-system\n            - --nodes=0:2:cx43:hel1:goingdark-autoscaler\n            - --balance-similar-node-groups=true\n            - --expander=least-waste\n            - --logtostderr=true\n            - --scale-down-delay-after-add=10m\n            - --scale-down-delay-after-delete=10m\n            - --scale-down-unneeded-time=8m\n            - --scale-down-utilization-threshold=0.75\n            - --stderrthreshold=info\n            - --v=4\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: SERVICE_ACCOUNT\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.serviceAccountName\n            - name: HCLOUD_CLUSTER_CONFIG_FILE\n              value: \"/config/cluster-config\"\n            - name: HCLOUD_FIREWALL\n              value: \"2379867\"\n            - name: HCLOUD_NETWORK\n              value: \"11368893\"\n            - name: HCLOUD_PUBLIC_IPV4\n              value: \"true\"\n            - name: HCLOUD_PUBLIC_IPV6\n              value: \"false\"\n            - name: HCLOUD_SERVER_CREATION_TIMEOUT\n              value: \"10\"\n            - name: HCLOUD_SSH_KEY\n              value: \"101120850\"\n            - name: HCLOUD_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hcloud\n                  key: token\n          livenessProbe:\n            httpGet:\n              path: /health-check\n              port: 8085\n          ports:\n            - containerPort: 8085\n          resources:\n            {}\n          volumeMounts:\n            - name: cluster-autoscaler-hetzner-config\n              mountPath: /config\n              readOnly: true\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      serviceAccountName: cluster-autoscaler-hetzner-cluster-autoscaler\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/control-plane\n          operator: Exists\n      topologySpreadConstraints:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/instance: cluster-autoscaler\n              app.kubernetes.io/name: hetzner-cluster-autoscaler\n          maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: ScheduleAnyway\n      volumes:\n        - name: cluster-autoscaler-hetzner-config\n          secret:\n            secretName: cluster-autoscaler-hetzner-config\n","manifests":{"templates/clusterrole.yaml":"---\n# Source: cluster-autoscaler/templates/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - events\n      - endpoints\n    verbs:\n      - create\n      - patch\n  - apiGroups:\n    - \"\"\n    resources:\n    - pods/eviction\n    verbs:\n    - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods/status\n    verbs:\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - endpoints\n    resourceNames:\n      - cluster-autoscaler\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n    - watch\n    - list\n    - create\n    - delete\n    - get\n    - update\n  - apiGroups:\n    - \"\"\n    resources:\n      - namespaces\n      - pods\n      - services\n      - replicationcontrollers\n      - persistentvolumeclaims\n      - persistentvolumes\n    verbs:\n      - watch\n      - list\n      - get\n  - apiGroups:\n    - batch\n    resources:\n      - jobs\n      - cronjobs\n    verbs:\n      - watch\n      - list\n      - get\n  - apiGroups:\n    - batch\n    - extensions\n    resources:\n    - jobs\n    verbs:\n    - get\n    - list\n    - patch\n    - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - replicasets\n      - daemonsets\n    verbs:\n      - watch\n      - list\n      - get\n  - apiGroups:\n      - policy\n    resources:\n      - poddisruptionbudgets\n    verbs:\n      - watch\n      - list\n  - apiGroups:\n    - apps\n    resources:\n    - daemonsets\n    - replicasets\n    - statefulsets\n    verbs:\n    - watch\n    - list\n    - get\n  - apiGroups:\n    - storage.k8s.io\n    resources:\n    - storageclasses\n    - csinodes\n    - csidrivers\n    - csistoragecapacities\n    - volumeattachments\n    verbs:\n    - watch\n    - list\n    - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - list\n      - watch\n      - get\n  - apiGroups:\n    - coordination.k8s.io\n    resources:\n    - leases\n    verbs:\n    - create\n  - apiGroups:\n    - coordination.k8s.io\n    resourceNames:\n    - cluster-autoscaler\n    resources:\n    - leases\n    verbs:\n    - get\n    - update\n","templates/clusterrolebinding.yaml":"---\n# Source: cluster-autoscaler/templates/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler-hetzner-cluster-autoscaler\n    namespace: kube-system\n","templates/deployment.yaml":"---\n# Source: cluster-autoscaler/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    {}\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: \"cluster-autoscaler\"\n      app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: \"cluster-autoscaler\"\n        app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    spec:\n      priorityClassName: \"system-cluster-critical\"\n      dnsPolicy: \"ClusterFirst\"\n      containers:\n        - name: hetzner-cluster-autoscaler\n          image: \"registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0\"\n          imagePullPolicy: \"IfNotPresent\"\n          command:\n            - ./cluster-autoscaler\n            - --cloud-provider=hetzner\n            - --namespace=kube-system\n            - --nodes=0:2:cx43:hel1:goingdark-autoscaler\n            - --balance-similar-node-groups=true\n            - --expander=least-waste\n            - --logtostderr=true\n            - --scale-down-delay-after-add=10m\n            - --scale-down-delay-after-delete=10m\n            - --scale-down-unneeded-time=8m\n            - --scale-down-utilization-threshold=0.75\n            - --stderrthreshold=info\n            - --v=4\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: SERVICE_ACCOUNT\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.serviceAccountName\n            - name: HCLOUD_CLUSTER_CONFIG_FILE\n              value: \"/config/cluster-config\"\n            - name: HCLOUD_FIREWALL\n              value: \"2379867\"\n            - name: HCLOUD_NETWORK\n              value: \"11368893\"\n            - name: HCLOUD_PUBLIC_IPV4\n              value: \"true\"\n            - name: HCLOUD_PUBLIC_IPV6\n              value: \"false\"\n            - name: HCLOUD_SERVER_CREATION_TIMEOUT\n              value: \"10\"\n            - name: HCLOUD_SSH_KEY\n              value: \"101120850\"\n            - name: HCLOUD_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hcloud\n                  key: token\n          livenessProbe:\n            httpGet:\n              path: /health-check\n              port: 8085\n          ports:\n            - containerPort: 8085\n          resources:\n            {}\n          volumeMounts:\n            - name: cluster-autoscaler-hetzner-config\n              mountPath: /config\n              readOnly: true\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      serviceAccountName: cluster-autoscaler-hetzner-cluster-autoscaler\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/control-plane\n          operator: Exists\n      topologySpreadConstraints:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/instance: cluster-autoscaler\n              app.kubernetes.io/name: hetzner-cluster-autoscaler\n          maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: ScheduleAnyway\n      volumes:\n        - name: cluster-autoscaler-hetzner-config\n          secret:\n            secretName: cluster-autoscaler-hetzner-config\n","templates/pdb.yaml":"---\n# Source: cluster-autoscaler/templates/pdb.yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: \"cluster-autoscaler\"\n      app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n","templates/role.yaml":"---\n# Source: cluster-autoscaler/templates/role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    resourceNames:\n      - cluster-autoscaler-status\n    verbs:\n      - delete\n      - get\n      - update\n","templates/rolebinding.yaml":"---\n# Source: cluster-autoscaler/templates/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: cluster-autoscaler-hetzner-cluster-autoscaler\n    namespace: kube-system\n","templates/service.yaml":"---\n# Source: cluster-autoscaler/templates/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nspec:\n  ports:\n    - port: 8085\n      protocol: TCP\n      targetPort: 8085\n      name: http\n  selector:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n  type: \"ClusterIP\"\n","templates/serviceaccount.yaml":"---\n# Source: cluster-autoscaler/templates/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/instance: \"cluster-autoscaler\"\n    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n    app.kubernetes.io/managed-by: \"Helm\"\n    helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n  name: cluster-autoscaler-hetzner-cluster-autoscaler\n  namespace: kube-system\nautomountServiceAccountToken: true\n"},"name":"cluster-autoscaler","namespace":"kube-system","notes":"\n\nTo verify that cluster-autoscaler has started, run:\n\n  kubectl --namespace=kube-system get pods -l \"app.kubernetes.io/name=hetzner-cluster-autoscaler,app.kubernetes.io/instance=cluster-autoscaler\"","pass_credentials":null,"postrender":null,"render_subchart_notes":false,"replace":false,"repository":"https://kubernetes.github.io/autoscaler","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":null,"set_list":null,"set_sensitive":null,"show_only":null,"skip_crds":false,"skip_tests":false,"timeout":300,"validate":false,"values":["\"autoscalingGroups\":\n- \"instanceType\": \"cx43\"\n  \"maxSize\": 2\n  \"minSize\": 0\n  \"name\": \"goingdark-autoscaler\"\n  \"region\": \"hel1\"\n\"cloudProvider\": \"hetzner\"\n\"extraEnv\":\n  \"HCLOUD_CLUSTER_CONFIG_FILE\": \"/config/cluster-config\"\n  \"HCLOUD_FIREWALL\": \"2379867\"\n  \"HCLOUD_NETWORK\": \"11368893\"\n  \"HCLOUD_PUBLIC_IPV4\": \"true\"\n  \"HCLOUD_PUBLIC_IPV6\": \"false\"\n  \"HCLOUD_SERVER_CREATION_TIMEOUT\": \"10\"\n  \"HCLOUD_SSH_KEY\": \"101120850\"\n\"extraEnvSecrets\":\n  \"HCLOUD_TOKEN\":\n    \"key\": \"token\"\n    \"name\": \"hcloud\"\n\"extraVolumeSecrets\":\n  \"cluster-autoscaler-hetzner-config\":\n    \"mountPath\": \"/config\"\n    \"name\": \"cluster-autoscaler-hetzner-config\"\n\"nodeSelector\":\n  \"node-role.kubernetes.io/control-plane\": \"\"\n\"podDisruptionBudget\":\n  \"maxUnavailable\": null\n  \"minAvailable\": 0\n\"replicaCount\": 1\n\"tolerations\":\n- \"effect\": \"NoSchedule\"\n  \"key\": \"node-role.kubernetes.io/control-plane\"\n  \"operator\": \"Exists\"\n\"topologySpreadConstraints\":\n- \"labelSelector\":\n    \"matchLabels\":\n      \"app.kubernetes.io/instance\": \"cluster-autoscaler\"\n      \"app.kubernetes.io/name\": \"hetzner-cluster-autoscaler\"\n  \"maxSkew\": 1\n  \"topologyKey\": \"kubernetes.io/hostname\"\n  \"whenUnsatisfiable\": \"ScheduleAnyway\"\n","\"extraArgs\":\n  \"balance-similar-node-groups\": \"true\"\n  \"expander\": \"least-waste\"\n  \"scale-down-delay-after-add\": \"10m\"\n  \"scale-down-delay-after-delete\": \"10m\"\n  \"scale-down-unneeded-time\": \"8m\"\n  \"scale-down-utilization-threshold\": \"0.75\"\n"],"verify":false,"version":"9.50.1","wait":false},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"helm_template","name":"hcloud_ccm","provider":"module.kubernetes.provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"schema_version":0,"attributes":{"api_versions":null,"atomic":false,"chart":"hcloud-cloud-controller-manager","crds":[],"create_namespace":false,"dependency_update":false,"description":"","devel":false,"disable_openapi_validation":false,"disable_webhooks":false,"id":"hcloud-cloud-controller-manager","include_crds":false,"is_upgrade":false,"keyring":"","kube_version":"v1.33.4","manifest":"---\n# Source: hcloud-cloud-controller-manager/templates/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: hcloud-cloud-controller-manager\n  namespace: kube-system\n---\n# Source: hcloud-cloud-controller-manager/templates/clusterrolebinding.yaml\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: \"system:hcloud-cloud-controller-manager\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: hcloud-cloud-controller-manager\n    namespace: kube-system\n---\n# Source: hcloud-cloud-controller-manager/templates/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hcloud-cloud-controller-manager\n  namespace: kube-system\nspec:\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\n      app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\n        app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\n    spec:\n      serviceAccountName: hcloud-cloud-controller-manager\n      dnsPolicy: Default\n      tolerations:\n        # Allow HCCM itself to schedule on nodes that have not yet been initialized by HCCM.\n        - key: \"node.cloudprovider.kubernetes.io/uninitialized\"\n          value: \"true\"\n          effect: \"NoSchedule\"\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n\n        # Allow HCCM to schedule on control plane nodes.\n        - key: \"node-role.kubernetes.io/master\"\n          effect: NoSchedule\n          operator: Exists\n        - key: \"node-role.kubernetes.io/control-plane\"\n          effect: NoSchedule\n          operator: Exists\n\n        - key: \"node.kubernetes.io/not-ready\"\n          effect: \"NoExecute\"\n      nodeSelector:\n        \n        node-role.kubernetes.io/control-plane: \"\"\n      hostNetwork: true\n      containers:\n        - name: hcloud-cloud-controller-manager\n          command:\n            - \"/bin/hcloud-cloud-controller-manager\"\n            - \"--allow-untagged-cloud\"\n            - \"--cloud-provider=hcloud\"\n            - \"--route-reconciliation-period=30s\"\n            - \"--webhook-secure-port=0\"\n            - \"--allocate-node-cidrs=true\"\n            - \"--cluster-cidr=10.0.128.0/17\"\n          env:\n            - name: HCLOUD_LOAD_BALANCERS_DISABLE_PRIVATE_INGRESS\n              value: \"true\"\n            - name: HCLOUD_LOAD_BALANCERS_ENABLED\n              value: \"true\"\n            - name: HCLOUD_LOAD_BALANCERS_LOCATION\n              value: hel1\n            - name: HCLOUD_LOAD_BALANCERS_USE_PRIVATE_IP\n              value: \"true\"\n            - name: HCLOUD_NETWORK_ROUTES_ENABLED\n              value: \"true\"\n            - name: HCLOUD_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  key: token\n                  name: hcloud\n            - name: ROBOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  key: robot-password\n                  name: hcloud\n                  optional: true\n            - name: ROBOT_USER\n              valueFrom:\n                secretKeyRef:\n                  key: robot-user\n                  name: hcloud\n                  optional: true\n            - name: HCLOUD_NETWORK\n              valueFrom:\n                secretKeyRef:\n                  key: network\n                  name: hcloud\n          image: docker.io/hetznercloud/hcloud-cloud-controller-manager:v1.26.0 # x-releaser-pleaser-version\n          ports:\n            - name: metrics\n              containerPort: 8233\n          resources:\n            requests:\n              cpu: 100m\n              memory: 50Mi\n      priorityClassName: system-cluster-critical\n","manifests":{"templates/clusterrolebinding.yaml":"---\n# Source: hcloud-cloud-controller-manager/templates/clusterrolebinding.yaml\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: \"system:hcloud-cloud-controller-manager\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: hcloud-cloud-controller-manager\n    namespace: kube-system\n","templates/daemonset.yaml":"---\n# Source: hcloud-cloud-controller-manager/templates/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hcloud-cloud-controller-manager\n  namespace: kube-system\nspec:\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\n      app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\n        app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\n    spec:\n      serviceAccountName: hcloud-cloud-controller-manager\n      dnsPolicy: Default\n      tolerations:\n        # Allow HCCM itself to schedule on nodes that have not yet been initialized by HCCM.\n        - key: \"node.cloudprovider.kubernetes.io/uninitialized\"\n          value: \"true\"\n          effect: \"NoSchedule\"\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n\n        # Allow HCCM to schedule on control plane nodes.\n        - key: \"node-role.kubernetes.io/master\"\n          effect: NoSchedule\n          operator: Exists\n        - key: \"node-role.kubernetes.io/control-plane\"\n          effect: NoSchedule\n          operator: Exists\n\n        - key: \"node.kubernetes.io/not-ready\"\n          effect: \"NoExecute\"\n      nodeSelector:\n        \n        node-role.kubernetes.io/control-plane: \"\"\n      hostNetwork: true\n      containers:\n        - name: hcloud-cloud-controller-manager\n          command:\n            - \"/bin/hcloud-cloud-controller-manager\"\n            - \"--allow-untagged-cloud\"\n            - \"--cloud-provider=hcloud\"\n            - \"--route-reconciliation-period=30s\"\n            - \"--webhook-secure-port=0\"\n            - \"--allocate-node-cidrs=true\"\n            - \"--cluster-cidr=10.0.128.0/17\"\n          env:\n            - name: HCLOUD_LOAD_BALANCERS_DISABLE_PRIVATE_INGRESS\n              value: \"true\"\n            - name: HCLOUD_LOAD_BALANCERS_ENABLED\n              value: \"true\"\n            - name: HCLOUD_LOAD_BALANCERS_LOCATION\n              value: hel1\n            - name: HCLOUD_LOAD_BALANCERS_USE_PRIVATE_IP\n              value: \"true\"\n            - name: HCLOUD_NETWORK_ROUTES_ENABLED\n              value: \"true\"\n            - name: HCLOUD_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  key: token\n                  name: hcloud\n            - name: ROBOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  key: robot-password\n                  name: hcloud\n                  optional: true\n            - name: ROBOT_USER\n              valueFrom:\n                secretKeyRef:\n                  key: robot-user\n                  name: hcloud\n                  optional: true\n            - name: HCLOUD_NETWORK\n              valueFrom:\n                secretKeyRef:\n                  key: network\n                  name: hcloud\n          image: docker.io/hetznercloud/hcloud-cloud-controller-manager:v1.26.0 # x-releaser-pleaser-version\n          ports:\n            - name: metrics\n              containerPort: 8233\n          resources:\n            requests:\n              cpu: 100m\n              memory: 50Mi\n      priorityClassName: system-cluster-critical\n","templates/serviceaccount.yaml":"---\n# Source: hcloud-cloud-controller-manager/templates/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: hcloud-cloud-controller-manager\n  namespace: kube-system\n"},"name":"hcloud-cloud-controller-manager","namespace":"kube-system","notes":"\n","pass_credentials":null,"postrender":null,"render_subchart_notes":false,"replace":false,"repository":"https://charts.hetzner.cloud","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":null,"set_list":null,"set_sensitive":null,"show_only":null,"skip_crds":false,"skip_tests":false,"timeout":300,"validate":false,"values":["\"env\":\n  \"HCLOUD_LOAD_BALANCERS_DISABLE_PRIVATE_INGRESS\":\n    \"value\": \"true\"\n  \"HCLOUD_LOAD_BALANCERS_ENABLED\":\n    \"value\": \"true\"\n  \"HCLOUD_LOAD_BALANCERS_LOCATION\":\n    \"value\": \"hel1\"\n  \"HCLOUD_LOAD_BALANCERS_USE_PRIVATE_IP\":\n    \"value\": \"true\"\n  \"HCLOUD_NETWORK_ROUTES_ENABLED\":\n    \"value\": \"true\"\n\"kind\": \"DaemonSet\"\n\"networking\":\n  \"clusterCIDR\": \"10.0.128.0/17\"\n  \"enabled\": true\n\"nodeSelector\":\n  \"node-role.kubernetes.io/control-plane\": \"\"\n","{}\n"],"verify":false,"version":"1.26.0","wait":false},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"helm_template","name":"hcloud_csi","provider":"module.kubernetes.provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"api_versions":null,"atomic":false,"chart":"hcloud-csi","crds":[],"create_namespace":false,"dependency_update":false,"description":"","devel":false,"disable_openapi_validation":false,"disable_webhooks":false,"id":"hcloud-csi","include_crds":false,"is_upgrade":false,"keyring":"","kube_version":"v1.33.4","manifest":"---\n# Source: hcloud-csi/templates/controller/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: hcloud-csi-controller\n  namespace: \"kube-system\"\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nautomountServiceAccountToken: true\n---\n# Source: hcloud-csi/templates/core/storageclass.yaml\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: hcloud-volumes-encrypted-xfs\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: csi.hetzner.cloud\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nreclaimPolicy: \"Retain\"\nparameters:\n  csi.storage.k8s.io/fstype: xfs\n  csi.storage.k8s.io/node-publish-secret-name: hcloud-csi-secret\n  csi.storage.k8s.io/node-publish-secret-namespace: kube-system\n  fsFormatOption: -i nrext64=1\n---\n# Source: hcloud-csi/templates/controller/clusterrole.yaml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: hcloud-csi-controller\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nrules:\n  # attacher\n  - apiGroups: [\"\"]\n    resources: [persistentvolumes]\n    verbs: [get, list, watch, update, patch]\n  - apiGroups: [\"\"]\n    resources: [nodes]\n    verbs: [get, list, watch]\n  - apiGroups: [csi.storage.k8s.io]\n    resources: [csinodeinfos]\n    verbs: [get, list, watch]\n  - apiGroups: [storage.k8s.io]\n    resources: [csinodes]\n    verbs: [get, list, watch]\n  - apiGroups: [storage.k8s.io]\n    resources: [volumeattachments]\n    verbs: [get, list, watch, update, patch]\n  - apiGroups: [storage.k8s.io]\n    resources: [volumeattachments/status]\n    verbs: [patch]\n  # provisioner\n  - apiGroups: [\"\"]\n    resources: [secrets]\n    verbs: [get, list]\n  - apiGroups: [\"\"]\n    resources: [persistentvolumes]\n    verbs: [get, list, watch, create, delete, patch]\n  - apiGroups: [\"\"]\n    resources: [persistentvolumeclaims, persistentvolumeclaims/status]\n    verbs: [get, list, watch, update, patch]\n  - apiGroups: [storage.k8s.io]\n    resources: [storageclasses]\n    verbs: [get, list, watch]\n  - apiGroups: [\"\"]\n    resources: [events]\n    verbs: [list, watch, create, update, patch]\n  - apiGroups: [snapshot.storage.k8s.io]\n    resources: [volumesnapshots]\n    verbs: [get, list]\n  - apiGroups: [snapshot.storage.k8s.io]\n    resources: [volumesnapshotcontents]\n    verbs: [get, list]\n  # resizer\n  - apiGroups: [\"\"]\n    resources: [pods]\n    verbs: [get, list, watch]\n  # node\n  - apiGroups: [\"\"]\n    resources: [events]\n    verbs: [get, list, watch, create, update, patch]\n---\n# Source: hcloud-csi/templates/controller/clusterrolebinding.yaml\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: hcloud-csi-controller\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: hcloud-csi-controller\nsubjects:\n  - kind: ServiceAccount\n    name: hcloud-csi-controller\n    namespace: \"kube-system\"\n---\n# Source: hcloud-csi/templates/node/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hcloud-csi-node\n  namespace: \"kube-system\"\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: node\n    app: hcloud-csi\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: hcloud-csi\n      app.kubernetes.io/instance: hcloud-csi\n      app.kubernetes.io/component: node\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: hcloud-csi\n        helm.sh/chart: hcloud-csi-2.17.0\n        app.kubernetes.io/instance: hcloud-csi\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: node\n    spec:\n      \n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: instance.hetzner.cloud/is-root-server\n                operator: NotIn\n                values:\n                - \"true\"\n              - key: instance.hetzner.cloud/provided-by\n                operator: NotIn\n                values:\n                - robot\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n        - key: CriticalAddonsOnly\n          operator: Exists\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      containers:\n        - name: csi-node-driver-registrar\n          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.14.0\n          imagePullPolicy: IfNotPresent\n          args:\n            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.hetzner.cloud/socket\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /run/csi\n            - name: registration-dir\n              mountPath: /registration\n          resources:\n            limits: {}\n            requests: {}\n        - name: liveness-probe\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - mountPath: /run/csi\n            name: plugin-dir\n          resources:\n            limits: {}\n            requests: {}\n        - name: hcloud-csi-driver\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\n          imagePullPolicy: IfNotPresent\n          args:\n            - -node\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /run/csi\n            - name: device-dir\n              mountPath: /dev\n          securityContext:\n            privileged: true\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///run/csi/socket\n            - name: ENABLE_METRICS\n              value: \"false\"\n          ports:\n            - name: healthz\n              protocol: TCP\n              containerPort: 9808\n          resources:\n            limits: {}\n            requests: {}\n          livenessProbe:\n            failureThreshold: 5\n            initialDelaySeconds: 10\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 3\n            httpGet:\n              path: /healthz\n              port: healthz\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/csi.hetzner.cloud/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: device-dir\n          hostPath:\n            path: /dev\n            type: Directory\n---\n# Source: hcloud-csi/templates/controller/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hcloud-csi-controller\n  namespace: \"kube-system\"\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n    app: hcloud-csi-controller\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: hcloud-csi\n      app.kubernetes.io/instance: hcloud-csi\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: hcloud-csi\n        helm.sh/chart: hcloud-csi-2.17.0\n        app.kubernetes.io/instance: hcloud-csi\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: hcloud-csi-controller\n      \n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: instance.hetzner.cloud/provided-by\n                operator: In\n                values:\n                - cloud\n            weight: 1\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/control-plane\n          operator: Exists\n      topologySpreadConstraints:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: controller\n              app.kubernetes.io/instance: hcloud-csi\n              app.kubernetes.io/name: hcloud-csi\n          maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: ScheduleAnyway\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      containers:\n        - name: csi-attacher\n          image: registry.k8s.io/sig-storage/csi-attacher:v4.9.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          args:\n            - --default-fstype=ext4\n          volumeMounts:\n          - name: socket-dir\n            mountPath: /run/csi\n\n        - name: csi-resizer\n          image: registry.k8s.io/sig-storage/csi-resizer:v1.14.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          volumeMounts:\n          - name: socket-dir\n            mountPath: /run/csi\n\n        - name: csi-provisioner\n          image: registry.k8s.io/sig-storage/csi-provisioner:v5.3.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          args:\n            - --feature-gates=Topology=true\n            - --default-fstype=ext4\n            - --extra-create-metadata\n          volumeMounts:\n          - name: socket-dir\n            mountPath: /run/csi\n\n        - name: liveness-probe\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          volumeMounts:\n          - mountPath: /run/csi\n            name: socket-dir\n\n        - name: hcloud-csi-driver\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\n          imagePullPolicy: IfNotPresent\n          args:\n            - -controller\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///run/csi/socket\n            - name: ENABLE_METRICS\n              value: \"false\"\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: spec.nodeName\n            - name: HCLOUD_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hcloud\n                  key: token\n          resources:\n            limits: {}\n            requests: {}\n          ports:\n            - name: healthz\n              protocol: TCP\n              containerPort: 9808\n          livenessProbe:\n            failureThreshold: 5\n            initialDelaySeconds: 10\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 3\n            httpGet:\n              path: /healthz\n              port: healthz\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /run/csi\n\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n---\n# Source: hcloud-csi/templates/core/csidriver.yaml\napiVersion: storage.k8s.io/v1\nkind: CSIDriver\nmetadata:\n  name: csi.hetzner.cloud\nspec:\n  attachRequired: true\n  fsGroupPolicy: File\n  podInfoOnMount: true\n  seLinuxMount: true\n  volumeLifecycleModes:\n  - Persistent\n","manifests":{"templates/controller/clusterrole.yaml":"---\n# Source: hcloud-csi/templates/controller/clusterrole.yaml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: hcloud-csi-controller\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nrules:\n  # attacher\n  - apiGroups: [\"\"]\n    resources: [persistentvolumes]\n    verbs: [get, list, watch, update, patch]\n  - apiGroups: [\"\"]\n    resources: [nodes]\n    verbs: [get, list, watch]\n  - apiGroups: [csi.storage.k8s.io]\n    resources: [csinodeinfos]\n    verbs: [get, list, watch]\n  - apiGroups: [storage.k8s.io]\n    resources: [csinodes]\n    verbs: [get, list, watch]\n  - apiGroups: [storage.k8s.io]\n    resources: [volumeattachments]\n    verbs: [get, list, watch, update, patch]\n  - apiGroups: [storage.k8s.io]\n    resources: [volumeattachments/status]\n    verbs: [patch]\n  # provisioner\n  - apiGroups: [\"\"]\n    resources: [secrets]\n    verbs: [get, list]\n  - apiGroups: [\"\"]\n    resources: [persistentvolumes]\n    verbs: [get, list, watch, create, delete, patch]\n  - apiGroups: [\"\"]\n    resources: [persistentvolumeclaims, persistentvolumeclaims/status]\n    verbs: [get, list, watch, update, patch]\n  - apiGroups: [storage.k8s.io]\n    resources: [storageclasses]\n    verbs: [get, list, watch]\n  - apiGroups: [\"\"]\n    resources: [events]\n    verbs: [list, watch, create, update, patch]\n  - apiGroups: [snapshot.storage.k8s.io]\n    resources: [volumesnapshots]\n    verbs: [get, list]\n  - apiGroups: [snapshot.storage.k8s.io]\n    resources: [volumesnapshotcontents]\n    verbs: [get, list]\n  # resizer\n  - apiGroups: [\"\"]\n    resources: [pods]\n    verbs: [get, list, watch]\n  # node\n  - apiGroups: [\"\"]\n    resources: [events]\n    verbs: [get, list, watch, create, update, patch]\n","templates/controller/clusterrolebinding.yaml":"---\n# Source: hcloud-csi/templates/controller/clusterrolebinding.yaml\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: hcloud-csi-controller\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: hcloud-csi-controller\nsubjects:\n  - kind: ServiceAccount\n    name: hcloud-csi-controller\n    namespace: \"kube-system\"\n","templates/controller/deployment.yaml":"---\n# Source: hcloud-csi/templates/controller/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hcloud-csi-controller\n  namespace: \"kube-system\"\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n    app: hcloud-csi-controller\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: hcloud-csi\n      app.kubernetes.io/instance: hcloud-csi\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: hcloud-csi\n        helm.sh/chart: hcloud-csi-2.17.0\n        app.kubernetes.io/instance: hcloud-csi\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: hcloud-csi-controller\n      \n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - preference:\n              matchExpressions:\n              - key: instance.hetzner.cloud/provided-by\n                operator: In\n                values:\n                - cloud\n            weight: 1\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      tolerations:\n        - effect: NoSchedule\n          key: node-role.kubernetes.io/control-plane\n          operator: Exists\n      topologySpreadConstraints:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/component: controller\n              app.kubernetes.io/instance: hcloud-csi\n              app.kubernetes.io/name: hcloud-csi\n          maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: ScheduleAnyway\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      containers:\n        - name: csi-attacher\n          image: registry.k8s.io/sig-storage/csi-attacher:v4.9.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          args:\n            - --default-fstype=ext4\n          volumeMounts:\n          - name: socket-dir\n            mountPath: /run/csi\n\n        - name: csi-resizer\n          image: registry.k8s.io/sig-storage/csi-resizer:v1.14.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          volumeMounts:\n          - name: socket-dir\n            mountPath: /run/csi\n\n        - name: csi-provisioner\n          image: registry.k8s.io/sig-storage/csi-provisioner:v5.3.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          args:\n            - --feature-gates=Topology=true\n            - --default-fstype=ext4\n            - --extra-create-metadata\n          volumeMounts:\n          - name: socket-dir\n            mountPath: /run/csi\n\n        - name: liveness-probe\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits: {}\n            requests: {}\n          volumeMounts:\n          - mountPath: /run/csi\n            name: socket-dir\n\n        - name: hcloud-csi-driver\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\n          imagePullPolicy: IfNotPresent\n          args:\n            - -controller\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///run/csi/socket\n            - name: ENABLE_METRICS\n              value: \"false\"\n            - name: KUBE_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: spec.nodeName\n            - name: HCLOUD_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: hcloud\n                  key: token\n          resources:\n            limits: {}\n            requests: {}\n          ports:\n            - name: healthz\n              protocol: TCP\n              containerPort: 9808\n          livenessProbe:\n            failureThreshold: 5\n            initialDelaySeconds: 10\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 3\n            httpGet:\n              path: /healthz\n              port: healthz\n          volumeMounts:\n            - name: socket-dir\n              mountPath: /run/csi\n\n      volumes:\n        - name: socket-dir\n          emptyDir: {}\n","templates/controller/serviceaccount.yaml":"---\n# Source: hcloud-csi/templates/controller/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: hcloud-csi-controller\n  namespace: \"kube-system\"\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nautomountServiceAccountToken: true\n","templates/core/csidriver.yaml":"---\n# Source: hcloud-csi/templates/core/csidriver.yaml\napiVersion: storage.k8s.io/v1\nkind: CSIDriver\nmetadata:\n  name: csi.hetzner.cloud\nspec:\n  attachRequired: true\n  fsGroupPolicy: File\n  podInfoOnMount: true\n  seLinuxMount: true\n  volumeLifecycleModes:\n  - Persistent\n","templates/core/storageclass.yaml":"---\n# Source: hcloud-csi/templates/core/storageclass.yaml\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: hcloud-volumes-encrypted-xfs\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: csi.hetzner.cloud\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nreclaimPolicy: \"Retain\"\nparameters:\n  csi.storage.k8s.io/fstype: xfs\n  csi.storage.k8s.io/node-publish-secret-name: hcloud-csi-secret\n  csi.storage.k8s.io/node-publish-secret-namespace: kube-system\n  fsFormatOption: -i nrext64=1\n","templates/node/daemonset.yaml":"---\n# Source: hcloud-csi/templates/node/daemonset.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hcloud-csi-node\n  namespace: \"kube-system\"\n  labels:\n    app.kubernetes.io/name: hcloud-csi\n    helm.sh/chart: hcloud-csi-2.17.0\n    app.kubernetes.io/instance: hcloud-csi\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: node\n    app: hcloud-csi\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: hcloud-csi\n      app.kubernetes.io/instance: hcloud-csi\n      app.kubernetes.io/component: node\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: hcloud-csi\n        helm.sh/chart: hcloud-csi-2.17.0\n        app.kubernetes.io/instance: hcloud-csi\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: node\n    spec:\n      \n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: instance.hetzner.cloud/is-root-server\n                operator: NotIn\n                values:\n                - \"true\"\n              - key: instance.hetzner.cloud/provided-by\n                operator: NotIn\n                values:\n                - robot\n      tolerations:\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n        - key: CriticalAddonsOnly\n          operator: Exists\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      containers:\n        - name: csi-node-driver-registrar\n          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.14.0\n          imagePullPolicy: IfNotPresent\n          args:\n            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.hetzner.cloud/socket\n          volumeMounts:\n            - name: plugin-dir\n              mountPath: /run/csi\n            - name: registration-dir\n              mountPath: /registration\n          resources:\n            limits: {}\n            requests: {}\n        - name: liveness-probe\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - mountPath: /run/csi\n            name: plugin-dir\n          resources:\n            limits: {}\n            requests: {}\n        - name: hcloud-csi-driver\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\n          imagePullPolicy: IfNotPresent\n          args:\n            - -node\n          volumeMounts:\n            - name: kubelet-dir\n              mountPath: /var/lib/kubelet\n              mountPropagation: \"Bidirectional\"\n            - name: plugin-dir\n              mountPath: /run/csi\n            - name: device-dir\n              mountPath: /dev\n          securityContext:\n            privileged: true\n          env:\n            - name: CSI_ENDPOINT\n              value: unix:///run/csi/socket\n            - name: ENABLE_METRICS\n              value: \"false\"\n          ports:\n            - name: healthz\n              protocol: TCP\n              containerPort: 9808\n          resources:\n            limits: {}\n            requests: {}\n          livenessProbe:\n            failureThreshold: 5\n            initialDelaySeconds: 10\n            periodSeconds: 2\n            successThreshold: 1\n            timeoutSeconds: 3\n            httpGet:\n              path: /healthz\n              port: healthz\n      volumes:\n        - name: kubelet-dir\n          hostPath:\n            path: /var/lib/kubelet\n            type: Directory\n        - name: plugin-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins/csi.hetzner.cloud/\n            type: DirectoryOrCreate\n        - name: registration-dir\n          hostPath:\n            path: /var/lib/kubelet/plugins_registry/\n            type: Directory\n        - name: device-dir\n          hostPath:\n            path: /dev\n            type: Directory\n"},"name":"hcloud-csi","namespace":"kube-system","notes":"","pass_credentials":null,"postrender":null,"render_subchart_notes":false,"replace":false,"repository":"https://charts.hetzner.cloud","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":null,"set_list":null,"set_sensitive":null,"show_only":null,"skip_crds":false,"skip_tests":false,"timeout":300,"validate":false,"values":["\"controller\":\n  \"nodeSelector\":\n    \"node-role.kubernetes.io/control-plane\": \"\"\n  \"replicaCount\": 1\n  \"tolerations\":\n  - \"effect\": \"NoSchedule\"\n    \"key\": \"node-role.kubernetes.io/control-plane\"\n    \"operator\": \"Exists\"\n  \"topologySpreadConstraints\":\n  - \"labelSelector\":\n      \"matchLabels\":\n        \"app.kubernetes.io/component\": \"controller\"\n        \"app.kubernetes.io/instance\": \"hcloud-csi\"\n        \"app.kubernetes.io/name\": \"hcloud-csi\"\n    \"maxSkew\": 1\n    \"topologyKey\": \"kubernetes.io/hostname\"\n    \"whenUnsatisfiable\": \"ScheduleAnyway\"\n  \"volumeExtraLabels\": {}\n\"storageClasses\":\n- \"defaultStorageClass\": true\n  \"extraParameters\":\n    \"csi.storage.k8s.io/fstype\": \"xfs\"\n    \"csi.storage.k8s.io/node-publish-secret-name\": \"hcloud-csi-secret\"\n    \"csi.storage.k8s.io/node-publish-secret-namespace\": \"kube-system\"\n    \"fsFormatOption\": \"-i nrext64=1\"\n  \"name\": \"hcloud-volumes-encrypted-xfs\"\n  \"reclaimPolicy\": \"Retain\"\n","{}\n"],"verify":false,"version":"2.17.0","wait":false},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"helm_template","name":"metrics_server","provider":"module.kubernetes.provider[\"registry.opentofu.org/hashicorp/helm\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"api_versions":null,"atomic":false,"chart":"metrics-server","crds":[],"create_namespace":false,"dependency_update":false,"description":"","devel":false,"disable_openapi_validation":false,"disable_webhooks":false,"id":"metrics-server","include_crds":false,"is_upgrade":false,"keyring":"","kube_version":"v1.33.4","manifest":"---\n# Source: metrics-server/templates/pdb.yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  minAvailable: 1\n\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: metrics-server\n---\n# Source: metrics-server/templates/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\n---\n# Source: metrics-server/templates/clusterrole-aggregated-reader.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:metrics-server-aggregated-reader\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\nrules:\n  - apiGroups:\n      - metrics.k8s.io\n    resources:\n      - pods\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n---\n# Source: metrics-server/templates/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:metrics-server\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nrules:\n  - apiGroups:\n    - \"\"\n    resources:\n    - nodes/metrics\n    verbs:\n    - get\n  - apiGroups:\n    - \"\"\n    resources:\n      - pods\n      - nodes\n      - namespaces\n      - configmaps\n    verbs:\n      - get\n      - list\n      - watch\n---\n# Source: metrics-server/templates/clusterrolebinding-auth-delegator.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: metrics-server:system:auth-delegator\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n---\n# Source: metrics-server/templates/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:metrics-server\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:metrics-server\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n---\n# Source: metrics-server/templates/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: metrics-server-auth-reader\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: extension-apiserver-authentication-reader\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n---\n# Source: metrics-server/templates/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - name: https\n      port: 443\n      protocol: TCP\n      targetPort: https\n      appProtocol: https\n  selector:\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n---\n# Source: metrics-server/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: metrics-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: metrics-server\n    spec:\n      serviceAccountName: metrics-server\n      priorityClassName: \"system-cluster-critical\"\n      containers:\n        - name: metrics-server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            readOnlyRootFilesystem: true\n            runAsNonRoot: true\n            runAsUser: 1000\n            seccompProfile:\n              type: RuntimeDefault\n          image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n          imagePullPolicy: IfNotPresent\n          args:\n            - --secure-port=10250\n            - --cert-dir=/tmp\n            - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n            - --kubelet-use-node-status-port\n            - --metric-resolution=15s\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /livez\n              port: https\n              scheme: HTTPS\n            initialDelaySeconds: 0\n            periodSeconds: 10\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /readyz\n              port: https\n              scheme: HTTPS\n            initialDelaySeconds: 20\n            periodSeconds: 10\n          volumeMounts:\n            - name: tmp\n              mountPath: /tmp\n          resources:\n            requests:\n              cpu: 100m\n              memory: 200Mi\n      volumes:\n        - name: tmp\n          emptyDir: {}\n      topologySpreadConstraints:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/instance: metrics-server\n              app.kubernetes.io/name: metrics-server\n          maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: ScheduleAnyway\n---\n# Source: metrics-server/templates/apiservice.yaml\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta1.metrics.k8s.io\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\nspec:\n  group: metrics.k8s.io\n  groupPriorityMinimum: 100\n  insecureSkipTLSVerify: true\n  service:\n    name: metrics-server\n    namespace: kube-system\n    port: 443\n  version: v1beta1\n  versionPriority: 100\n","manifests":{"templates/apiservice.yaml":"---\n# Source: metrics-server/templates/apiservice.yaml\napiVersion: apiregistration.k8s.io/v1\nkind: APIService\nmetadata:\n  name: v1beta1.metrics.k8s.io\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\nspec:\n  group: metrics.k8s.io\n  groupPriorityMinimum: 100\n  insecureSkipTLSVerify: true\n  service:\n    name: metrics-server\n    namespace: kube-system\n    port: 443\n  version: v1beta1\n  versionPriority: 100\n","templates/clusterrole-aggregated-reader.yaml":"---\n# Source: metrics-server/templates/clusterrole-aggregated-reader.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:metrics-server-aggregated-reader\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\nrules:\n  - apiGroups:\n      - metrics.k8s.io\n    resources:\n      - pods\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n","templates/clusterrole.yaml":"---\n# Source: metrics-server/templates/clusterrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:metrics-server\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nrules:\n  - apiGroups:\n    - \"\"\n    resources:\n    - nodes/metrics\n    verbs:\n    - get\n  - apiGroups:\n    - \"\"\n    resources:\n      - pods\n      - nodes\n      - namespaces\n      - configmaps\n    verbs:\n      - get\n      - list\n      - watch\n","templates/clusterrolebinding-auth-delegator.yaml":"---\n# Source: metrics-server/templates/clusterrolebinding-auth-delegator.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: metrics-server:system:auth-delegator\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n","templates/clusterrolebinding.yaml":"---\n# Source: metrics-server/templates/clusterrolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:metrics-server\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:metrics-server\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n","templates/deployment.yaml":"---\n# Source: metrics-server/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: metrics-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: metrics-server\n    spec:\n      serviceAccountName: metrics-server\n      priorityClassName: \"system-cluster-critical\"\n      containers:\n        - name: metrics-server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n              - ALL\n            readOnlyRootFilesystem: true\n            runAsNonRoot: true\n            runAsUser: 1000\n            seccompProfile:\n              type: RuntimeDefault\n          image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n          imagePullPolicy: IfNotPresent\n          args:\n            - --secure-port=10250\n            - --cert-dir=/tmp\n            - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n            - --kubelet-use-node-status-port\n            - --metric-resolution=15s\n          ports:\n          - name: https\n            protocol: TCP\n            containerPort: 10250\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /livez\n              port: https\n              scheme: HTTPS\n            initialDelaySeconds: 0\n            periodSeconds: 10\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /readyz\n              port: https\n              scheme: HTTPS\n            initialDelaySeconds: 20\n            periodSeconds: 10\n          volumeMounts:\n            - name: tmp\n              mountPath: /tmp\n          resources:\n            requests:\n              cpu: 100m\n              memory: 200Mi\n      volumes:\n        - name: tmp\n          emptyDir: {}\n      topologySpreadConstraints:\n        - labelSelector:\n            matchLabels:\n              app.kubernetes.io/instance: metrics-server\n              app.kubernetes.io/name: metrics-server\n          maxSkew: 1\n          topologyKey: kubernetes.io/hostname\n          whenUnsatisfiable: ScheduleAnyway\n","templates/pdb.yaml":"---\n# Source: metrics-server/templates/pdb.yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  minAvailable: 1\n\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: metrics-server\n","templates/rolebinding.yaml":"---\n# Source: metrics-server/templates/rolebinding.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: metrics-server-auth-reader\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: extension-apiserver-authentication-reader\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n","templates/service.yaml":"---\n# Source: metrics-server/templates/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n    - name: https\n      port: 443\n      protocol: TCP\n      targetPort: https\n      appProtocol: https\n  selector:\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n","templates/serviceaccount.yaml":"---\n# Source: metrics-server/templates/serviceaccount.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: metrics-server\n    app.kubernetes.io/version: \"0.8.0\"\n    app.kubernetes.io/managed-by: Helm\n"},"name":"metrics-server","namespace":"kube-system","notes":"***********************************************************************\n* Metrics Server                                                      *\n***********************************************************************\n  Chart version: 3.13.0\n  App version:   0.8.0\n  Image tag:     registry.k8s.io/metrics-server/metrics-server:v0.8.0\n***********************************************************************\n","pass_credentials":null,"postrender":null,"render_subchart_notes":false,"replace":false,"repository":"https://kubernetes-sigs.github.io/metrics-server","repository_ca_file":null,"repository_cert_file":null,"repository_key_file":null,"repository_password":null,"repository_username":null,"reset_values":false,"reuse_values":false,"set":null,"set_list":null,"set_sensitive":null,"show_only":null,"skip_crds":false,"skip_tests":false,"timeout":300,"validate":false,"values":["\"nodeSelector\": {}\n\"podDisruptionBudget\":\n  \"enabled\": true\n  \"minAvailable\": 1\n\"replicas\": 2\n\"tolerations\": []\n\"topologySpreadConstraints\":\n- \"labelSelector\":\n    \"matchLabels\":\n      \"app.kubernetes.io/instance\": \"metrics-server\"\n      \"app.kubernetes.io/name\": \"metrics-server\"\n  \"maxSkew\": 1\n  \"topologyKey\": \"kubernetes.io/hostname\"\n  \"whenUnsatisfiable\": \"ScheduleAnyway\"\n","{}\n"],"verify":false,"version":"3.13.0","wait":false},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"http","name":"kube_api_health","provider":"provider[\"registry.opentofu.org/hashicorp/http\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"body":"{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Unauthorized\",\"reason\":\"Unauthorized\",\"code\":401}\n","ca_cert_pem":null,"client_cert_pem":null,"client_key_pem":null,"id":"https://46.62.164.172:6443/version","insecure":true,"method":null,"request_body":null,"request_headers":null,"request_timeout_ms":null,"response_body":"{\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Unauthorized\",\"reason\":\"Unauthorized\",\"code\":401}\n","response_body_base64":"eyJraW5kIjoiU3RhdHVzIiwiYXBpVmVyc2lvbiI6InYxIiwibWV0YWRhdGEiOnt9LCJzdGF0dXMiOiJGYWlsdXJlIiwibWVzc2FnZSI6IlVuYXV0aG9yaXplZCIsInJlYXNvbiI6IlVuYXV0aG9yaXplZCIsImNvZGUiOjQwMX0K","response_headers":{"Audit-Id":"2fe19196-6427-4890-8211-7e1e5c17876c","Cache-Control":"no-cache, private","Content-Length":"129","Content-Type":"application/json","Date":"Sun, 21 Dec 2025 17:00:02 GMT"},"retry":{"attempts":60,"max_delay_ms":5000,"min_delay_ms":5000},"status_code":401,"url":"https://46.62.164.172:6443/version"},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"talos_client_configuration","name":"this","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"schema_version":0,"attributes":{"client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","client_key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"},"cluster_name":"goingdark","endpoints":["46.62.164.172"],"id":"goingdark","nodes":["10.0.64.1"],"talos_config":"context: goingdark\ncontexts:\n    goingdark:\n        endpoints:\n            - 46.62.164.172\n        nodes:\n            - 10.0.64.1\n        ca: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n        key: LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K\n"},"sensitive_attributes":[[{"type":"get_attr","value":"client_configuration"},{"type":"get_attr","value":"client_key"}]]}]},{"module":"module.kubernetes","mode":"data","type":"talos_image_factory_extensions_versions","name":"this","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"extensions_info":[{"author":"Markus Reiter","description":"[extra] This system extension provides the QEMU Guest Agent service.\n","digest":"sha256:84b42d779721ddab71e0d5c12e10399d6bdd03af0aaa0dafd240e2724d724675","name":"siderolabs/qemu-guest-agent","ref":"ghcr.io/siderolabs/qemu-guest-agent:10.0.2"}],"filters":{"names":["siderolabs/qemu-guest-agent"]},"id":"extensions_info","talos_version":"v1.11.1"},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"talos_image_factory_urls","name":"amd64","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"schema_version":0,"attributes":{"architecture":"amd64","id":"","platform":"hcloud","sbc":null,"schematic_id":"ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515","talos_version":"v1.11.1","urls":{"disk_image":"https://factory.talos.dev/image/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515/v1.11.1/hcloud-amd64.raw.xz","disk_image_secureboot":null,"initramfs":null,"installer":"factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1","installer_secureboot":null,"iso":null,"iso_secureboot":null,"kernel":null,"kernel_command_line":null,"pxe":null,"uki":null}},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"talos_image_factory_urls","name":"arm64","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"schema_version":0,"attributes":{"architecture":"arm64","id":"","platform":"hcloud","sbc":null,"schematic_id":"ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515","talos_version":"v1.11.1","urls":{"disk_image":"https://factory.talos.dev/image/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515/v1.11.1/hcloud-arm64.raw.xz","disk_image_secureboot":null,"initramfs":null,"installer":"factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1","installer_secureboot":null,"iso":null,"iso_secureboot":null,"kernel":null,"kernel_command_line":null,"pxe":null,"uki":null}},"sensitive_attributes":[]}]},{"module":"module.kubernetes","mode":"data","type":"talos_machine_configuration","name":"cluster_autoscaler","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"index_key":"autoscaler","schema_version":0,"attributes":{"cluster_endpoint":"https://10.0.64.126:6443","cluster_name":"goingdark","config_patches":["\"cluster\":\n  \"discovery\":\n    \"enabled\": true\n    \"registries\":\n      \"kubernetes\":\n        \"disabled\": true\n      \"service\":\n        \"disabled\": false\n  \"network\":\n    \"cni\":\n      \"name\": \"none\"\n    \"dnsDomain\": \"cluster.local\"\n    \"podSubnets\":\n    - \"10.0.128.0/17\"\n    \"serviceSubnets\":\n    - \"10.0.96.0/19\"\n  \"proxy\":\n    \"disabled\": true\n\"machine\":\n  \"certSANs\":\n  - \"10.0.64.1\"\n  - \"10.0.64.126\"\n  - \"10.0.64.254\"\n  - \"127.0.0.1\"\n  - \"46.62.164.172\"\n  - \"::1\"\n  - \"localhost\"\n  \"features\":\n    \"hostDNS\":\n      \"enabled\": true\n      \"forwardKubeDNSToHost\": false\n      \"resolveMemberNames\": true\n  \"install\":\n    \"extraKernelArgs\": []\n    \"image\": \"factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\"\n  \"kernel\":\n    \"modules\": null\n  \"kubelet\":\n    \"extraArgs\":\n      \"cloud-provider\": \"external\"\n      \"rotate-server-certificates\": true\n    \"extraConfig\":\n      \"kubeReserved\":\n        \"cpu\": \"100m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"350Mi\"\n      \"registerWithTaints\":\n      - \"effect\": \"NoSchedule\"\n        \"key\": \"autoscaler-node\"\n        \"value\": \"true\"\n      \"shutdownGracePeriod\": \"90s\"\n      \"shutdownGracePeriodCriticalPods\": \"15s\"\n      \"systemReserved\":\n        \"cpu\": \"100m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"300Mi\"\n    \"extraMounts\": []\n    \"nodeIP\":\n      \"validSubnets\":\n      - \"10.0.64.0/19\"\n  \"logging\":\n    \"destinations\": []\n  \"network\":\n    \"extraHostEntries\": []\n    \"interfaces\":\n    - \"dhcp\": true\n      \"dhcpOptions\":\n        \"ipv4\": true\n        \"ipv6\": false\n      \"interface\": \"eth0\"\n    - \"dhcp\": true\n      \"interface\": \"eth1\"\n      \"routes\": []\n    \"nameservers\":\n    - \"185.12.64.1\"\n    - \"185.12.64.2\"\n    - \"2a01:4ff:ff00::add:1\"\n    - \"2a01:4ff:ff00::add:2\"\n  \"nodeAnnotations\": {}\n  \"nodeLabels\":\n    \"autoscaler-node\": \"true\"\n    \"nodepool\": \"autoscaler\"\n  \"registries\": null\n  \"sysctls\":\n    \"net.core.netdev_max_backlog\": \"4096\"\n    \"net.core.somaxconn\": \"65535\"\n    \"net.ipv6.conf.all.disable_ipv6\": 0\n    \"net.ipv6.conf.default.disable_ipv6\": 0\n  \"systemDiskEncryption\":\n    \"ephemeral\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n    \"state\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n  \"time\":\n    \"servers\":\n    - \"ntp1.hetzner.de\"\n    - \"ntp2.hetzner.com\"\n    - \"ntp3.hetzner.net\"\n"],"docs":false,"examples":false,"id":"goingdark","kubernetes_version":"v1.33.4","machine_configuration":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: worker\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints:\n                - effect: NoSchedule\n                  key: autoscaler-node\n                  value: \"true\"\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        autoscaler-node: \"true\"\n        nodepool: autoscaler\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    proxy:\n        disabled: true\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n","machine_secrets":{"certs":{"etcd":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_aggregator":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_serviceaccount":{"key":"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"},"os":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"}},"cluster":{"id":"PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=","secret":"z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU="},"secrets":{"aescbc_encryption_secret":null,"bootstrap_token":"qztdq7.ktojonbxqnrcsz3q","secretbox_encryption_secret":"vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE="},"trustdinfo":{"token":"7or9kx.41p55ltb292998sw"}},"machine_type":"worker","talos_version":"v1.11.1"},"sensitive_attributes":[[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"secretbox_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"aescbc_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"bootstrap_token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"trustdinfo"},{"type":"get_attr","value":"token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"etcd"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_aggregator"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_serviceaccount"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"os"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"cluster"},{"type":"get_attr","value":"secret"}]]}]},{"module":"module.kubernetes","mode":"data","type":"talos_machine_configuration","name":"control_plane","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"index_key":"goingdark-control-1","schema_version":0,"attributes":{"cluster_endpoint":"https://10.0.64.126:6443","cluster_name":"goingdark","config_patches":["\"cluster\":\n  \"adminKubeconfig\":\n    \"certLifetime\": \"87600h\"\n  \"allowSchedulingOnControlPlanes\": false\n  \"apiServer\":\n    \"admissionControl\": []\n    \"certSANs\":\n    - \"10.0.64.1\"\n    - \"10.0.64.126\"\n    - \"10.0.64.254\"\n    - \"127.0.0.1\"\n    - \"46.62.164.172\"\n    - \"::1\"\n    - \"localhost\"\n    \"extraArgs\":\n      \"enable-aggregator-routing\": true\n  \"controllerManager\":\n    \"extraArgs\":\n      \"bind-address\": \"0.0.0.0\"\n      \"cloud-provider\": \"external\"\n  \"coreDNS\":\n    \"disabled\": false\n  \"discovery\":\n    \"enabled\": true\n    \"registries\":\n      \"kubernetes\":\n        \"disabled\": true\n      \"service\":\n        \"disabled\": false\n  \"etcd\":\n    \"advertisedSubnets\":\n    - \"10.0.64.0/25\"\n    \"extraArgs\":\n      \"listen-metrics-urls\": \"http://0.0.0.0:2381\"\n  \"externalCloudProvider\":\n    \"enabled\": true\n    \"manifests\":\n    - \"https://raw.githubusercontent.com/siderolabs/talos-cloud-controller-manager/v1.10.1/docs/deploy/cloud-controller-manager-daemonset.yml\"\n    - \"https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.85.0/stripped-down-crds.yaml\"\n    - \"https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.3.0/standard-install.yaml\"\n  \"inlineManifests\":\n  - \"contents\": |\n      \"apiVersion\": \"v1\"\n      \"data\":\n        \"network\": \"MTEzNjg4OTM=\"\n        \"token\": \"bGtiVlNCcEtRZjBYTXhZalhjeEJMU1VsOURZSEVvV0ZucXVaMnIwdGFaVFVZMWxsRmU5cmFIMWhRWHFaOVpmUQ==\"\n      \"kind\": \"Secret\"\n      \"metadata\":\n        \"name\": \"hcloud\"\n        \"namespace\": \"kube-system\"\n      \"type\": \"Opaque\"\n    \"name\": \"hcloud-secret\"\n  - \"contents\": \"null\\n...\\n\\n---\\n---\\n# Source: cilium/templates/cilium-secrets-namespace.yaml\\napiVersion:\n      v1\\nkind: Namespace\\nmetadata:\\n  name: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of:\n      cilium\\n  annotations:\\n---\\n# Source: cilium/templates/cilium-agent/serviceaccount.yaml\\napiVersion:\n      v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n#\n      Source: cilium/templates/cilium-agent/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind:\n      Role\\nmetadata:\\n  name: cilium-config-agent\\n  namespace: kube-system\\n  labels:\\n\n      \\   app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n\n      \\ - configmaps\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n---\\n# Source: cilium/templates/cilium-agent/role.yaml\\napiVersion:\n      rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n\n      \\ namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n-\n      apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - get\\n  - list\\n\n      \\ - watch\\n---\\n# Source: cilium/templates/cilium-operator/role.yaml\\napiVersion:\n      rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n\n      \\ namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n-\n      apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - create\\n  - delete\\n\n      \\ - update\\n  - patch\\n---\\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion:\n      rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-config-agent\\n\n      \\ namespace: kube-system\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n\n      \\ apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-config-agent\\nsubjects:\\n\n      \\ - kind: ServiceAccount\\n    name: \\\"cilium\\\"\\n    namespace: kube-system\\n---\\n#\n      Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind:\n      RoleBinding\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n  namespace:\n      \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n\n      \\ apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-tlsinterception-secrets\\nsubjects:\\n-\n      kind: ServiceAccount\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source:\n      cilium/templates/cilium-operator/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind:\n      RoleBinding\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n  namespace:\n      \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n\n      \\ apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-operator-tlsinterception-secrets\\nsubjects:\\n-\n      kind: ServiceAccount\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n---\\n#\n      Source: cilium/templates/cilium-envoy/service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n\n      \\ name: cilium-envoy\\n  namespace: kube-system\\n  annotations:\\n    prometheus.io/scrape:\n      \\\"true\\\"\\n    prometheus.io/port: \\\"9964\\\"\\n  labels:\\n    k8s-app: cilium-envoy\\n\n      \\   app.kubernetes.io/name: cilium-envoy\\n    app.kubernetes.io/part-of: cilium\\n\n      \\   io.cilium/app: proxy\\nspec:\\n  clusterIP: None\\n  type: ClusterIP\\n  selector:\\n\n      \\   k8s-app: cilium-envoy\\n  ports:\\n  - name: envoy-metrics\\n    port: 9964\\n\n      \\   protocol: TCP\\n    targetPort: envoy-metrics\\n---\\n# Source: cilium/templates/cilium-agent/daemonset.yaml\\napiVersion:\n      apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: cilium\\n  namespace: kube-system\\n\n      \\ labels:\\n    k8s-app: cilium\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name:\n      cilium-agent\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cilium\\n\n      \\ updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n    type: RollingUpdate\\n\n      \\ template:\\n    metadata:\\n      annotations:\\n        prometheus.io/port:\n      \\\"9962\\\"\\n        prometheus.io/scrape: \\\"true\\\"\\n        kubectl.kubernetes.io/default-container:\n      cilium-agent\\n      labels:\\n        k8s-app: cilium\\n        app.kubernetes.io/name:\n      cilium-agent\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n\n      \\       appArmorProfile:\\n          type: Unconfined\\n        seccompProfile:\\n\n      \\         type: Unconfined\\n      containers:\\n      - name: cilium-agent\\n\n      \\       image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-agent\\n\n      \\       args:\\n        - --config-dir=/tmp/cilium/config-map\\n        startupProbe:\\n\n      \\         httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n\n      \\           port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n\n      \\           - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n          failureThreshold:\n      300\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds:\n      5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n\n      \\           path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n\n      \\           httpHeaders:\\n            - name: \\\"brief\\\"\\n              value:\n      \\\"true\\\"\\n            - name: \\\"require-k8s-connectivity\\\"\\n              value:\n      \\\"false\\\"\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold:\n      10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n\n      \\           host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port:\n      9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name:\n      \\\"brief\\\"\\n              value: \\\"true\\\"\\n          periodSeconds: 30\\n          successThreshold:\n      1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n\n      \\       - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n\n      \\             apiVersion: v1\\n              fieldPath: spec.nodeName\\n        -\n      name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion:\n      v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_CLUSTERMESH_CONFIG\\n\n      \\         value: /var/lib/cilium/clustermesh/\\n        - name: GOMEMLIMIT\\n\n      \\         valueFrom:\\n            resourceFieldRef:\\n              resource:\n      limits.memory\\n              divisor: '1'\\n        - name: KUBERNETES_SERVICE_HOST\\n\n      \\         value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value:\n      \\\"7445\\\"\\n        - name: KUBE_CLIENT_BACKOFF_BASE\\n          value: \\\"1\\\"\\n\n      \\       - name: KUBE_CLIENT_BACKOFF_DURATION\\n          value: \\\"120\\\"\\n        lifecycle:\\n\n      \\         postStart:\\n            exec:\\n              command:\\n              -\n      \\\"bash\\\"\\n              - \\\"-c\\\"\\n              - |\\n                    set\n      -o errexit\\n                    set -o pipefail\\n                    set -o\n      nounset\\n                    \\n                    # When running in AWS ENI\n      mode, it's likely that 'aws-node' has\\n                    # had a chance to\n      install SNAT iptables rules. These can result\\n                    # in dropped\n      traffic, so we should attempt to remove them.\\n                    # We do it\n      using a 'postStart' hook since this may need to run\\n                    # for\n      nodes which might have already been init'ed but may still\\n                    #\n      have dangling rules. This is safe because there are no\\n                    #\n      dependencies on anything that is part of the startup script\\n                    #\n      itself, and can be safely run multiple times per node (e.g. in\\n                    #\n      case of a restart).\\n                    if [[ \\\"$(iptables-save | grep -E -c\n      'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n                    then\\n\n      \\                       echo 'Deleting iptables rules created by the AWS CNI\n      VPC plugin'\\n                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\n      | iptables-restore\\n                    fi\\n                    echo 'Done!'\\n\n      \\                   \\n          preStop:\\n            exec:\\n              command:\\n\n      \\             - /cni-uninstall.sh\\n        ports:\\n        - name: peer-service\\n\n      \\         containerPort: 4244\\n          hostPort: 4244\\n          protocol:\n      TCP\\n        - name: prometheus\\n          containerPort: 9962\\n          hostPort:\n      9962\\n          protocol: TCP\\n        securityContext:\\n          seLinuxOptions:\\n\n      \\           level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n\n      \\             - CHOWN\\n              - KILL\\n              - NET_ADMIN\\n              -\n      NET_RAW\\n              - IPC_LOCK\\n              - SYS_ADMIN\\n              -\n      SYS_RESOURCE\\n              - DAC_OVERRIDE\\n              - FOWNER\\n              -\n      SETGID\\n              - SETUID\\n            drop:\\n              - ALL\\n        terminationMessagePolicy:\n      FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n\n      \\         mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n\n      \\       # Unprivileged containers need to mount /proc/sys/net from the host\\n\n      \\       # to have write access\\n        - mountPath: /host/proc/sys/net\\n          name:\n      host-proc-sys-net\\n        # Unprivileged containers need to mount /proc/sys/kernel\n      from the host\\n        # to have write access\\n        - mountPath: /host/proc/sys/kernel\\n\n      \\         name: host-proc-sys-kernel\\n        - name: bpf-maps\\n          mountPath:\n      /sys/fs/bpf\\n          # Unprivileged containers can't set mount propagation\n      to bidirectional\\n          # in this case we will mount the bpf fs from an\n      init container that\\n          # is privileged and set the mount propagation\n      from host to container\\n          # in Cilium.\\n          mountPropagation:\n      HostToContainer\\n        # Check for duplicate mounts before mounting\\n        -\n      name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n        - name: cilium-run\\n\n      \\         mountPath: /var/run/cilium\\n        - name: cilium-netns\\n          mountPath:\n      /var/run/cilium/netns\\n          mountPropagation: HostToContainer\\n        -\n      name: etc-cni-netd\\n          mountPath: /host/etc/cni/net.d\\n        - name:\n      clustermesh-secrets\\n          mountPath: /var/lib/cilium/clustermesh\\n          readOnly:\n      true\\n          # Needed to be able to load kernel modules\\n        - name:\n      lib-modules\\n          mountPath: /lib/modules\\n          readOnly: true\\n        -\n      name: xtables-lock\\n          mountPath: /run/xtables.lock\\n        - name:\n      tmp\\n          mountPath: /tmp\\n        \\n      initContainers:\\n      - name:\n      config\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-dbg\\n\n      \\       - build-config\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n\n      \\           fieldRef:\\n              apiVersion: v1\\n              fieldPath:\n      spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n\n      \\           fieldRef:\\n              apiVersion: v1\\n              fieldPath:\n      metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n          value:\n      \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n\n      \\       volumeMounts:\\n        - name: tmp\\n          mountPath: /tmp\\n        terminationMessagePolicy:\n      FallbackToLogsOnError\\n      - name: apply-sysctl-overwrites\\n        image:\n      \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        env:\\n        - name: BIN_PATH\\n\n      \\         value: /opt/cni/bin\\n        command:\\n        - sh\\n        - -ec\\n\n      \\       # The statically linked Go program binary is invoked to avoid any\\n\n      \\       # dependency on utilities like sh that can be missing on certain\\n        #\n      distros installed on the underlying host. Copy the binary to the\\n        #\n      same directory where we install cilium cni plugin so that exec permissions\\n\n      \\       # are available.\\n        - |\\n          cp /usr/bin/cilium-sysctlfix\n      /hostbin/cilium-sysctlfix;\\n          nsenter --mount=/hostproc/1/ns/mnt \\\"${BIN_PATH}/cilium-sysctlfix\\\";\\n\n      \\         rm /hostbin/cilium-sysctlfix\\n        volumeMounts:\\n        - name:\n      hostproc\\n          mountPath: /hostproc\\n        - name: cni-path\\n          mountPath:\n      /hostbin\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n\n      \\         seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n\n      \\         capabilities:\\n            add:\\n              - SYS_ADMIN\\n              -\n      SYS_CHROOT\\n              - SYS_PTRACE\\n            drop:\\n              - ALL\\n\n      \\     # Mount the bpf fs if it is not mounted. We will perform this task\\n      #\n      from a privileged container because the mount propagation bidirectional\\n      #\n      only works from privileged containers.\\n      - name: mount-bpf-fs\\n        image:\n      \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        args:\\n        - 'mount | grep\n      \\\"/sys/fs/bpf type bpf\\\" || mount -t bpf bpf /sys/fs/bpf'\\n        command:\\n\n      \\       - /bin/bash\\n        - -c\\n        - --\\n        terminationMessagePolicy:\n      FallbackToLogsOnError\\n        securityContext:\\n          privileged: true\\n\n      \\       volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n\n      \\         mountPropagation: Bidirectional\\n      - name: clean-cilium-state\\n\n      \\       image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        command:\\n        - /init-container.sh\\n\n      \\       env:\\n        - name: CILIUM_ALL_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n\n      \\             name: cilium-config\\n              key: clean-cilium-state\\n              optional:\n      true\\n        - name: CILIUM_BPF_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n\n      \\             name: cilium-config\\n              key: clean-cilium-bpf-state\\n\n      \\             optional: true\\n        - name: WRITE_CNI_CONF_WHEN_READY\\n          valueFrom:\\n\n      \\           configMapKeyRef:\\n              name: cilium-config\\n              key:\n      write-cni-conf-when-ready\\n              optional: true\\n        - name: KUBERNETES_SERVICE_HOST\\n\n      \\         value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value:\n      \\\"7445\\\"\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n\n      \\         seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n\n      \\         capabilities:\\n            add:\\n              - NET_ADMIN\\n              -\n      SYS_ADMIN\\n              - SYS_RESOURCE\\n            drop:\\n              -\n      ALL\\n        volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n\n      \\         # Required to mount cgroup filesystem from the host to cilium agent\n      pod\\n        - name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n          mountPropagation:\n      HostToContainer\\n        - name: cilium-run\\n          mountPath: /var/run/cilium\n      # wait-for-kube-proxy\\n      # Install the CNI binaries in an InitContainer\n      so we don't have a writable host mount in the agent\\n      - name: install-cni-binaries\\n\n      \\       image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        command:\\n          - \\\"/install-plugin.sh\\\"\\n\n      \\       resources:\\n          requests:\\n            cpu: 100m\\n            memory:\n      10Mi\\n        securityContext:\\n          seLinuxOptions:\\n            level:\n      s0\\n            type: spc_t\\n          capabilities:\\n            drop:\\n              -\n      ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n\n      \\         - name: cni-path\\n            mountPath: /host/opt/cni/bin # .Values.cni.install\\n\n      \\     restartPolicy: Always\\n      priorityClassName: system-node-critical\\n\n      \\     serviceAccountName: \\\"cilium\\\"\\n      automountServiceAccountToken: true\\n\n      \\     terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n\n      \\       podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n\n      \\         - labelSelector:\\n              matchLabels:\\n                k8s-app:\n      cilium\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n\n      \\       kubernetes.io/os: linux\\n      tolerations:\\n        - operator: Exists\\n\n      \\     volumes:\\n        # For sharing configuration between the \\\"config\\\" initContainer\n      and the agent\\n      - name: tmp\\n        emptyDir: {}\\n        # To keep state\n      between restarts / upgrades\\n      - name: cilium-run\\n        hostPath:\\n          path:\n      /var/run/cilium\\n          type: DirectoryOrCreate\\n        # To exec into pod\n      network namespaces\\n      - name: cilium-netns\\n        hostPath:\\n          path:\n      /var/run/netns\\n          type: DirectoryOrCreate\\n        # To keep state between\n      restarts / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n\n      \\         path: /sys/fs/bpf\\n          type: DirectoryOrCreate\\n      # To mount\n      cgroup2 filesystem on the host or apply sysctlfix\\n      - name: hostproc\\n\n      \\       hostPath:\\n          path: /proc\\n          type: Directory\\n      #\n      To keep state between restarts / upgrades for cgroup2 filesystem\\n      - name:\n      cilium-cgroup\\n        hostPath:\\n          path: /sys/fs/cgroup\\n          type:\n      DirectoryOrCreate\\n      # To install cilium cni plugin in the host\\n      -\n      name: cni-path\\n        hostPath:\\n          path:  /opt/cni/bin\\n          type:\n      DirectoryOrCreate\\n        # To install cilium cni configuration in the host\\n\n      \\     - name: etc-cni-netd\\n        hostPath:\\n          path: /etc/cni/net.d\\n\n      \\         type: DirectoryOrCreate\\n        # To be able to load kernel modules\\n\n      \\     - name: lib-modules\\n        hostPath:\\n          path: /lib/modules\\n\n      \\       # To access iptables concurrently with other processes (e.g. kube-proxy)\\n\n      \\     - name: xtables-lock\\n        hostPath:\\n          path: /run/xtables.lock\\n\n      \\         type: FileOrCreate\\n      # Sharing socket with Cilium Envoy on the\n      same node by using a host path\\n      - name: envoy-sockets\\n        hostPath:\\n\n      \\         path: \\\"/var/run/cilium/envoy/sockets\\\"\\n          type: DirectoryOrCreate\\n\n      \\       # To read the clustermesh configuration\\n      - name: clustermesh-secrets\\n\n      \\       projected:\\n          # note: the leading zero means this number is\n      in octal representation: do not remove it\\n          defaultMode: 0400\\n          sources:\\n\n      \\         - secret:\\n              name: cilium-clustermesh\\n              optional:\n      true\\n              # note: items are not explicitly listed here, since the\n      entries of this secret\\n              # depend on the peers configured, and\n      that would cause a restart of all agents\\n              # at every addition/removal.\n      Leaving the field empty makes each secret entry\\n              # to be automatically\n      projected into the volume as a file whose name is the key.\\n          - secret:\\n\n      \\             name: clustermesh-apiserver-remote-cert\\n              optional:\n      true\\n              items:\\n              - key: tls.key\\n                path:\n      common-etcd-client.key\\n              - key: tls.crt\\n                path:\n      common-etcd-client.crt\\n              - key: ca.crt\\n                path: common-etcd-client-ca.crt\\n\n      \\         # note: we configure the volume for the kvstoremesh-specific certificate\\n\n      \\         # regardless of whether KVStoreMesh is enabled or not, so that it\n      can be\\n          # automatically mounted in case KVStoreMesh gets subsequently\n      enabled,\\n          # without requiring an agent restart.\\n          - secret:\\n\n      \\             name: clustermesh-apiserver-local-cert\\n              optional:\n      true\\n              items:\\n              - key: tls.key\\n                path:\n      local-etcd-client.key\\n              - key: tls.crt\\n                path: local-etcd-client.crt\\n\n      \\             - key: ca.crt\\n                path: local-etcd-client-ca.crt\\n\n      \\     - name: host-proc-sys-net\\n        hostPath:\\n          path: /proc/sys/net\\n\n      \\         type: Directory\\n      - name: host-proc-sys-kernel\\n        hostPath:\\n\n      \\         path: /proc/sys/kernel\\n          type: Directory\\n---\\n# Source:\n      cilium/templates/cilium-envoy/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n\n      \\ name: cilium-envoy\\n  namespace: kube-system\\n  labels:\\n    k8s-app: cilium-envoy\\n\n      \\   app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-envoy\\n\n      \\   name: cilium-envoy\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app:\n      cilium-envoy\\n  updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n\n      \\   type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n      labels:\\n\n      \\       k8s-app: cilium-envoy\\n        name: cilium-envoy\\n        app.kubernetes.io/name:\n      cilium-envoy\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n\n      \\       appArmorProfile:\\n          type: Unconfined\\n      containers:\\n      -\n      name: cilium-envoy\\n        image: \\\"quay.io/cilium/cilium-envoy:v1.34.4-1754895458-68cffdfa568b6b226d70a7ef81fc65dda3b890bf@sha256:247e908700012f7ef56f75908f8c965215c26a27762f296068645eb55450bda2\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        command:\\n        - /usr/bin/cilium-envoy-starter\\n\n      \\       args:\\n        - '--'\\n        - '-c /var/run/cilium/envoy/bootstrap-config.json'\\n\n      \\       - '--base-id 0'\\n        - '--log-level info'\\n        startupProbe:\\n\n      \\         httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n\n      \\           port: 9878\\n            scheme: HTTP\\n          failureThreshold:\n      105\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds:\n      5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n\n      \\           path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n\n      \\         periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold:\n      10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n\n      \\           host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port:\n      9878\\n            scheme: HTTP\\n          periodSeconds: 30\\n          successThreshold:\n      1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n\n      \\       - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n\n      \\             apiVersion: v1\\n              fieldPath: spec.nodeName\\n        -\n      name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion:\n      v1\\n              fieldPath: metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n\n      \\         value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value:\n      \\\"7445\\\"\\n        ports:\\n        - name: envoy-metrics\\n          containerPort:\n      9964\\n          hostPort: 9964\\n          protocol: TCP\\n        securityContext:\\n\n      \\         seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n\n      \\         capabilities:\\n            add:\\n              - NET_ADMIN\\n              -\n      SYS_ADMIN\\n            drop:\\n              - ALL\\n        terminationMessagePolicy:\n      FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n\n      \\         mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n\n      \\       - name: envoy-artifacts\\n          mountPath: /var/run/cilium/envoy/artifacts\\n\n      \\         readOnly: true\\n        - name: envoy-config\\n          mountPath:\n      /var/run/cilium/envoy/\\n          readOnly: true\\n        - name: bpf-maps\\n\n      \\         mountPath: /sys/fs/bpf\\n          mountPropagation: HostToContainer\\n\n      \\     restartPolicy: Always\\n      priorityClassName: system-node-critical\\n\n      \\     serviceAccountName: \\\"cilium-envoy\\\"\\n      automountServiceAccountToken:\n      true\\n      terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n\n      \\       nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n\n      \\           nodeSelectorTerms:\\n            - matchExpressions:\\n              -\n      key: cilium.io/no-schedule\\n                operator: NotIn\\n                values:\\n\n      \\               - \\\"true\\\"\\n        podAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n\n      \\         - labelSelector:\\n              matchLabels:\\n                k8s-app:\n      cilium\\n            topologyKey: kubernetes.io/hostname\\n        podAntiAffinity:\\n\n      \\         requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n\n      \\             matchLabels:\\n                k8s-app: cilium-envoy\\n            topologyKey:\n      kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n\n      \\     tolerations:\\n        - operator: Exists\\n      volumes:\\n      - name:\n      envoy-sockets\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/sockets\\\"\\n\n      \\         type: DirectoryOrCreate\\n      - name: envoy-artifacts\\n        hostPath:\\n\n      \\         path: \\\"/var/run/cilium/envoy/artifacts\\\"\\n          type: DirectoryOrCreate\\n\n      \\     - name: envoy-config\\n        configMap:\\n          name: \\\"cilium-envoy-config\\\"\\n\n      \\         # note: the leading zero means this number is in octal representation:\n      do not remove it\\n          defaultMode: 0400\\n          items:\\n            -\n      key: bootstrap-config.json\\n              path: bootstrap-config.json\\n        #\n      To keep state between restarts / upgrades\\n        # To keep state between restarts\n      / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n          path:\n      /sys/fs/bpf\\n          type: DirectoryOrCreate\\n---\\n# Source: cilium/templates/cilium-operator/deployment.yaml\\napiVersion:\n      apps/v1\\nkind: Deployment\\nmetadata:\\n  name: cilium-operator\\n  namespace:\n      kube-system\\n  labels:\\n    io.cilium/app: operator\\n    name: cilium-operator\\n\n      \\   app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-operator\\nspec:\\n\n      \\ # See docs on ServerCapabilities.LeasesResourceLock in file pkg/k8s/version/version.go\\n\n      \\ # for more details.\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      io.cilium/app:\n      operator\\n      name: cilium-operator\\n  # ensure operator update on single\n      node k8s clusters, by using rolling update with maxUnavailable=100% in case\\n\n      \\ # of one replica and no user configured Recreate strategy.\\n  # otherwise\n      an update might get stuck due to the default maxUnavailable=50% in combination\n      with the\\n  # podAntiAffinity which prevents deployments of multiple operator\n      replicas on the same node.\\n  strategy:\\n    rollingUpdate:\\n      maxSurge:\n      25%\\n      maxUnavailable: 100%\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n\n      \\     annotations:\\n        prometheus.io/port: \\\"9963\\\"\\n        prometheus.io/scrape:\n      \\\"true\\\"\\n      labels:\\n        io.cilium/app: operator\\n        name: cilium-operator\\n\n      \\       app.kubernetes.io/part-of: cilium\\n        app.kubernetes.io/name: cilium-operator\\n\n      \\   spec:\\n      securityContext:\\n        seccompProfile:\\n          type:\n      RuntimeDefault\\n      containers:\\n      - name: cilium-operator\\n        image:\n      \\\"quay.io/cilium/operator-generic:v1.18.1@sha256:97f4553afa443465bdfbc1cc4927c93f16ac5d78e4dd2706736e7395382201bc\\\"\\n\n      \\       imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-operator-generic\\n\n      \\       args:\\n        - --config-dir=/tmp/cilium/config-map\\n        - --debug=$(CILIUM_DEBUG)\\n\n      \\       env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n\n      \\             apiVersion: v1\\n              fieldPath: spec.nodeName\\n        -\n      name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion:\n      v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_DEBUG\\n\n      \\         valueFrom:\\n            configMapKeyRef:\\n              key: debug\\n\n      \\             name: cilium-config\\n              optional: true\\n        - name:\n      KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n\n      \\         value: \\\"7445\\\"\\n        ports:\\n        - name: prometheus\\n          containerPort:\n      9963\\n          hostPort: 9963\\n          protocol: TCP\\n        livenessProbe:\\n\n      \\         httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n\n      \\           port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds:\n      60\\n          periodSeconds: 10\\n          timeoutSeconds: 3\\n        readinessProbe:\\n\n      \\         httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n\n      \\           port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds:\n      0\\n          periodSeconds: 5\\n          timeoutSeconds: 3\\n          failureThreshold:\n      5\\n        volumeMounts:\\n        - name: cilium-config-path\\n          mountPath:\n      /tmp/cilium/config-map\\n          readOnly: true\\n        securityContext:\\n\n      \\         allowPrivilegeEscalation: false\\n          capabilities:\\n            drop:\\n\n      \\           - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n\n      \\     hostNetwork: true\\n      restartPolicy: Always\\n      priorityClassName:\n      system-cluster-critical\\n      serviceAccountName: \\\"cilium-operator\\\"\\n      automountServiceAccountToken:\n      true\\n      # In HA mode, cilium-operator pods must not be scheduled on the\n      same\\n      # node as they will clash with each other.\\n      affinity:\\n        podAntiAffinity:\\n\n      \\         requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n\n      \\             matchLabels:\\n                io.cilium/app: operator\\n            topologyKey:\n      kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n\n      \\       node-role.kubernetes.io/control-plane: \\\"\\\"\\n      tolerations:\\n        -\n      operator: Exists\\n        - key: node.cilium.io/agent-not-ready\\n          operator:\n      Exists\\n      \\n      volumes:\\n        # To read the configuration from the\n      config map\\n      - name: cilium-config-path\\n        configMap:\\n          name:\n      cilium-config\\n---\\n# Source: cilium/templates/cilium-envoy/serviceaccount.yaml\\napiVersion:\n      v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-envoy\\\"\\n  namespace:\n      kube-system\\n---\\n# Source: cilium/templates/cilium-operator/serviceaccount.yaml\\napiVersion:\n      v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-operator\\\"\\n  namespace:\n      kube-system\\n---\\n# Source: cilium/templates/cilium-configmap.yaml\\napiVersion:\n      v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-config\\n  namespace: kube-system\\ndata:\\n\\n\n      \\ # Identity allocation mode selects how identities are shared between cilium\\n\n      \\ # nodes by setting how they are stored. The options are \\\"crd\\\", \\\"kvstore\\\"\n      or\\n  # \\\"doublewrite-readkvstore\\\" / \\\"doublewrite-readcrd\\\".\\n  # - \\\"crd\\\"\n      stores identities in kubernetes as CRDs (custom resource definition).\\n  #   These\n      can be queried with:\\n  #     kubectl get ciliumid\\n  # - \\\"kvstore\\\" stores\n      identities in an etcd kvstore, that is\\n  #   configured below. Cilium versions\n      before 1.6 supported only the kvstore\\n  #   backend. Upgrades from these older\n      cilium versions should continue using\\n  #   the kvstore by commenting out the\n      identity-allocation-mode below, or\\n  #   setting it to \\\"kvstore\\\".\\n  # -\n      \\\"doublewrite\\\" modes store identities in both the kvstore and CRDs. This is\n      useful\\n  #   for seamless migrations from the kvstore mode to the crd mode.\n      Consult the\\n  #   documentation for more information on how to perform the\n      migration.\\n  identity-allocation-mode: crd\\n\\n  identity-heartbeat-timeout:\n      \\\"30m0s\\\"\\n  identity-gc-interval: \\\"15m0s\\\"\\n  cilium-endpoint-gc-interval:\n      \\\"5m0s\\\"\\n  nodes-gc-interval: \\\"5m0s\\\"\\n\\n  # If you want to run cilium in\n      debug mode change this value to true\\n  debug: \\\"false\\\"\\n  debug-verbose: \\\"\\\"\\n\n      \\ metrics-sampling-interval: \\\"5m\\\"\\n  # The agent can be put into the following\n      three policy enforcement modes\\n  # default, always and never.\\n  # https://docs.cilium.io/en/latest/security/policy/intro/#policy-enforcement-modes\\n\n      \\ enable-policy: \\\"default\\\"\\n  policy-cidr-match-mode: \\\"\\\"\\n  # If you want\n      metrics enabled in all of your Cilium agents, set the port for\\n  # which the\n      Cilium agents will have their metrics exposed.\\n  # This option deprecates the\n      \\\"prometheus-serve-addr\\\" in the\\n  # \\\"cilium-metrics-config\\\" ConfigMap\\n\n      \\ # NOTE that this will open the port on ALL nodes where Cilium pods are\\n  #\n      scheduled.\\n  prometheus-serve-addr: \\\":9962\\\"\\n  # A space-separated list of\n      controller groups for which to enable metrics.\\n  # The special values of \\\"all\\\"\n      and \\\"none\\\" are supported.\\n  controller-group-metrics:\\n    write-cni-file\\n\n      \\   sync-host-ips\\n    sync-lb-maps-with-k8s-services\\n  # If you want metrics\n      enabled in cilium-operator, set the port for\\n  # which the Cilium Operator\n      will have their metrics exposed.\\n  # NOTE that this will open the port on the\n      nodes where Cilium operator pod\\n  # is scheduled.\\n  operator-prometheus-serve-addr:\n      \\\":9963\\\"\\n  enable-metrics: \\\"true\\\"\\n  enable-policy-secrets-sync: \\\"true\\\"\\n\n      \\ policy-secrets-only-from-secrets-namespace: \\\"true\\\"\\n  policy-secrets-namespace:\n      \\\"cilium-secrets\\\"\\n\\n  # Enable IPv4 addressing. If enabled, all endpoints\n      are allocated an IPv4\\n  # address.\\n  enable-ipv4: \\\"true\\\"\\n\\n  # Enable IPv6\n      addressing. If enabled, all endpoints are allocated an IPv6\\n  # address.\\n\n      \\ enable-ipv6: \\\"false\\\"\\n  # Users who wish to specify their own custom CNI\n      configuration file must set\\n  # custom-cni-conf to \\\"true\\\", otherwise Cilium\n      may overwrite the configuration.\\n  custom-cni-conf: \\\"false\\\"\\n  enable-bpf-clock-probe:\n      \\\"false\\\"\\n  # If you want cilium monitor to aggregate tracing for packets,\n      set this level\\n  # to \\\"low\\\", \\\"medium\\\", or \\\"maximum\\\". The higher the level,\n      the less packets\\n  # that will be seen in monitor output.\\n  monitor-aggregation:\n      medium\\n\\n  # The monitor aggregation interval governs the typical time between\n      monitor\\n  # notification events for each allowed connection.\\n  #\\n  # Only\n      effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n  monitor-aggregation-interval:\n      \\\"5s\\\"\\n\\n  # The monitor aggregation flags determine which TCP flags which,\n      upon the\\n  # first observation, cause monitor notifications to be generated.\\n\n      \\ #\\n  # Only effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n\n      \\ monitor-aggregation-flags: all\\n  # Specifies the ratio (0.0-1.0] of total\n      system memory to use for dynamic\\n  # sizing of the TCP CT, non-TCP CT, NAT\n      and policy BPF maps.\\n  bpf-map-dynamic-size-ratio: \\\"0.0025\\\"\\n  enable-host-legacy-routing:\n      \\\"false\\\"\\n  # bpf-policy-map-max specifies the maximum number of entries in\n      endpoint\\n  # policy map (per endpoint)\\n  bpf-policy-map-max: \\\"16384\\\"\\n  #\n      bpf-policy-stats-map-max specifies the maximum number of entries in global\\n\n      \\ # policy stats map\\n  bpf-policy-stats-map-max: \\\"65536\\\"\\n  # bpf-lb-map-max\n      specifies the maximum number of entries in bpf lb service,\\n  # backend and\n      affinity maps.\\n  bpf-lb-map-max: \\\"65536\\\"\\n  bpf-lb-external-clusterip: \\\"false\\\"\\n\n      \\ bpf-lb-source-range-all-types: \\\"false\\\"\\n  bpf-lb-algorithm-annotation: \\\"false\\\"\\n\n      \\ bpf-lb-mode-annotation: \\\"false\\\"\\n\\n  bpf-distributed-lru: \\\"false\\\"\\n  bpf-events-drop-enabled:\n      \\\"true\\\"\\n  bpf-events-policy-verdict-enabled: \\\"true\\\"\\n  bpf-events-trace-enabled:\n      \\\"true\\\"\\n\\n  # Pre-allocation of map entries allows per-packet latency to be\n      reduced, at\\n  # the expense of up-front memory allocation for the entries in\n      the maps. The\\n  # default value below will minimize memory usage in the default\n      installation;\\n  # users who are sensitive to latency may consider setting this\n      to \\\"true\\\".\\n  #\\n  # This option was introduced in Cilium 1.4. Cilium 1.3\n      and earlier ignore\\n  # this option and behave as though it is set to \\\"true\\\".\\n\n      \\ #\\n  # If this value is modified, then during the next Cilium startup the\n      restore\\n  # of existing endpoints and tracking of ongoing connections may be\n      disrupted.\\n  # As a result, reply packets may be dropped and the load-balancing\n      decisions\\n  # for established connections may change.\\n  #\\n  # If this option\n      is set to \\\"false\\\" during an upgrade from 1.3 or earlier to\\n  # 1.4 or later,\n      then it may cause one-time disruptions during the upgrade.\\n  preallocate-bpf-maps:\n      \\\"false\\\"\\n\\n  # Name of the cluster. Only relevant when building a mesh of\n      clusters.\\n  cluster-name: \\\"default\\\"\\n  # Unique ID of the cluster. Must be\n      unique across all conneted clusters and\\n  # in the range of 1 and 255. Only\n      relevant when building a mesh of clusters.\\n  cluster-id: \\\"0\\\"\\n\\n  # Encapsulation\n      mode for communication between nodes\\n  # Possible values:\\n  #   - disabled\\n\n      \\ #   - vxlan (default)\\n  #   - geneve\\n\\n  routing-mode: \\\"native\\\"\\n  tunnel-protocol:\n      \\\"vxlan\\\"\\n  tunnel-source-port-range: \\\"0-0\\\"\\n  service-no-backend-response:\n      \\\"reject\\\"\\n\\n\\n  # Enables L7 proxy for L7 policy enforcement and visibility\\n\n      \\ enable-l7-proxy: \\\"true\\\"\\n  enable-ipv4-masquerade: \\\"true\\\"\\n  enable-ipv4-big-tcp:\n      \\\"false\\\"\\n  enable-ipv6-big-tcp: \\\"false\\\"\\n  enable-ipv6-masquerade: \\\"true\\\"\\n\n      \\ enable-tcx: \\\"true\\\"\\n  datapath-mode: \\\"veth\\\"\\n  enable-bpf-masquerade:\n      \\\"true\\\"\\n  enable-masquerade-to-route-source: \\\"false\\\"\\n  enable-wireguard:\n      \\\"true\\\"\\n  wireguard-persistent-keepalive: \\\"0s\\\"\\n\\n  enable-xt-socket-fallback:\n      \\\"true\\\"\\n  install-no-conntrack-iptables-rules: \\\"true\\\"\\n  iptables-random-fully:\n      \\\"false\\\"\\n\\n  auto-direct-node-routes: \\\"false\\\"\\n  direct-routing-skip-unreachable:\n      \\\"false\\\"\\n\\n\\n  ipv4-native-routing-cidr: 10.0.0.0/16\\n\\n  kube-proxy-replacement:\n      \\\"true\\\"\\n  kube-proxy-replacement-healthz-bind-address: \\\"0.0.0.0:10256\\\"\\n\n      \\ bpf-lb-sock: \\\"false\\\"\\n  nodeport-addresses: \\\"\\\"\\n  enable-health-check-nodeport:\n      \\\"true\\\"\\n  enable-health-check-loadbalancer-ip: \\\"false\\\"\\n  node-port-bind-protection:\n      \\\"true\\\"\\n  enable-auto-protect-node-port-range: \\\"true\\\"\\n  bpf-lb-acceleration:\n      \\\"native\\\"\\n  enable-svc-source-range-check: \\\"true\\\"\\n  enable-l2-neigh-discovery:\n      \\\"false\\\"\\n  k8s-require-ipv4-pod-cidr: \\\"true\\\"\\n  k8s-require-ipv6-pod-cidr:\n      \\\"false\\\"\\n  enable-k8s-networkpolicy: \\\"true\\\"\\n  enable-endpoint-lockdown-on-policy-overflow:\n      \\\"false\\\"\\n  # Tell the agent to generate and write a CNI configuration file\\n\n      \\ write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist\\n  cni-exclusive:\n      \\\"true\\\"\\n  cni-log-file: \\\"/var/run/cilium/cilium-cni.log\\\"\\n  enable-endpoint-health-checking:\n      \\\"true\\\"\\n  enable-health-checking: \\\"true\\\"\\n  health-check-icmp-failure-threshold:\n      \\\"3\\\"\\n  enable-well-known-identities: \\\"false\\\"\\n  enable-node-selector-labels:\n      \\\"false\\\"\\n  synchronize-k8s-nodes: \\\"true\\\"\\n  operator-api-serve-addr: \\\"127.0.0.1:9234\\\"\\n\\n\n      \\ enable-hubble: \\\"false\\\"\\n  ipam: \\\"kubernetes\\\"\\n  ipam-cilium-node-update-rate:\n      \\\"15s\\\"\\n\\n  default-lb-service-ipam: \\\"lbipam\\\"\\n  egress-gateway-reconciliation-trigger-interval:\n      \\\"1s\\\"\\n  enable-vtep: \\\"false\\\"\\n  vtep-endpoint: \\\"\\\"\\n  vtep-cidr: \\\"\\\"\\n\n      \\ vtep-mask: \\\"\\\"\\n  vtep-mac: \\\"\\\"\\n  procfs: \\\"/host/proc\\\"\\n  bpf-root: \\\"/sys/fs/bpf\\\"\\n\n      \\ cgroup-root: \\\"/sys/fs/cgroup\\\"\\n\\n  identity-management-mode: \\\"agent\\\"\\n\n      \\ enable-sctp: \\\"false\\\"\\n  remove-cilium-node-taints: \\\"true\\\"\\n  set-cilium-node-taints:\n      \\\"true\\\"\\n  set-cilium-is-up-condition: \\\"true\\\"\\n  unmanaged-pod-watcher-interval:\n      \\\"15\\\"\\n  # explicit setting gets precedence\\n  dnsproxy-enable-transparent-mode:\n      \\\"true\\\"\\n  dnsproxy-socket-linger-timeout: \\\"10\\\"\\n  tofqdns-dns-reject-response-code:\n      \\\"refused\\\"\\n  tofqdns-enable-dns-compression: \\\"true\\\"\\n  tofqdns-endpoint-max-ip-per-hostname:\n      \\\"1000\\\"\\n  tofqdns-idle-connection-grace-period: \\\"0s\\\"\\n  tofqdns-max-deferred-connection-deletes:\n      \\\"10000\\\"\\n  tofqdns-proxy-response-max-delay: \\\"100ms\\\"\\n  tofqdns-preallocate-identities:\n      \\ \\\"true\\\"\\n  agent-not-ready-taint-key: \\\"node.cilium.io/agent-not-ready\\\"\\n\\n\n      \\ mesh-auth-enabled: \\\"true\\\"\\n  mesh-auth-queue-size: \\\"1024\\\"\\n  mesh-auth-rotated-identities-queue-size:\n      \\\"1024\\\"\\n  mesh-auth-gc-interval: \\\"5m0s\\\"\\n\\n  proxy-xff-num-trusted-hops-ingress:\n      \\\"0\\\"\\n  proxy-xff-num-trusted-hops-egress: \\\"0\\\"\\n  proxy-connect-timeout:\n      \\\"2\\\"\\n  proxy-initial-fetch-timeout: \\\"30\\\"\\n  proxy-max-requests-per-connection:\n      \\\"0\\\"\\n  proxy-max-connection-duration-seconds: \\\"0\\\"\\n  proxy-idle-timeout-seconds:\n      \\\"60\\\"\\n  proxy-max-concurrent-retries: \\\"128\\\"\\n  http-retry-count: \\\"3\\\"\\n\\n\n      \\ external-envoy-proxy: \\\"true\\\"\\n  envoy-base-id: \\\"0\\\"\\n  envoy-access-log-buffer-size:\n      \\\"4096\\\"\\n  envoy-keep-cap-netbindservice: \\\"false\\\"\\n  max-connected-clusters:\n      \\\"255\\\"\\n  clustermesh-enable-endpoint-sync: \\\"false\\\"\\n  clustermesh-enable-mcs-api:\n      \\\"false\\\"\\n  policy-default-local-cluster: \\\"false\\\"\\n\\n  nat-map-stats-entries:\n      \\\"32\\\"\\n  nat-map-stats-interval: \\\"30s\\\"\\n  enable-internal-traffic-policy:\n      \\\"true\\\"\\n  enable-lb-ipam: \\\"true\\\"\\n  enable-non-default-deny-policies: \\\"true\\\"\\n\n      \\ enable-source-ip-verification: \\\"true\\\"\\n\\n# Extra config allows adding arbitrary\n      properties to the cilium config.\\n# By putting it at the end of the ConfigMap,\n      it's also possible to override existing properties.\\n---\\n# Source: cilium/templates/cilium-envoy/configmap.yaml\\napiVersion:\n      v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-envoy-config\\n  namespace: kube-system\\ndata:\\n\n      \\ # Keep the key name as bootstrap-config.json to avoid breaking changes\\n  bootstrap-config.json:\n      |\\n    {\\\"admin\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}},\\\"applicationLogConfig\\\":{\\\"logFormat\\\":{\\\"textFormat\\\":\\\"[%Y-%m-%d\n      %T.%e][%t][%l][%n] [%g:%#] %v\\\"}},\\\"bootstrapExtensions\\\":[{\\\"name\\\":\\\"envoy.bootstrap.internal_listener\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener\\\"}}],\\\"dynamicResources\\\":{\\\"cdsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"},\\\"ldsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"}},\\\"node\\\":{\\\"cluster\\\":\\\"ingress-cluster\\\",\\\"id\\\":\\\"host~127.0.0.1~no-id~localdomain\\\"},\\\"overloadManager\\\":{\\\"resourceMonitors\\\":[{\\\"name\\\":\\\"envoy.resource_monitors.global_downstream_max_connections\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig\\\",\\\"max_active_downstream_connections\\\":\\\"50000\\\"}}]},\\\"staticResources\\\":{\\\"clusters\\\":[{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/xds.sock\\\"}}}}]}]},\\\"name\\\":\\\"xds-grpc-cilium\\\",\\\"type\\\":\\\"STATIC\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"explicitHttpConfig\\\":{\\\"http2ProtocolOptions\\\":{}}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"/envoy-admin\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}}}]}]},\\\"name\\\":\\\"/envoy-admin\\\",\\\"type\\\":\\\"STATIC\\\"}],\\\"listeners\\\":[{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"0.0.0.0\\\",\\\"portValue\\\":9964}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtualHosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/metrics\\\"},\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/stats/prometheus\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-prometheus-metrics-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-prometheus-metrics-listener\\\"},{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"127.0.0.1\\\",\\\"portValue\\\":9878}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtual_hosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"health\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/healthz\\\"},\\\"name\\\":\\\"health\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/ready\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-health-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-health-listener\\\"}]}}\\n---\\n#\n      Source: cilium/templates/cilium-agent/clusterrole.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind:\n      ClusterRole\\nmetadata:\\n  name: cilium\\n  labels:\\n    app.kubernetes.io/part-of:\n      cilium\\nrules:\\n- apiGroups:\\n  - networking.k8s.io\\n  resources:\\n  - networkpolicies\\n\n      \\ verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - discovery.k8s.io\\n\n      \\ resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n-\n      apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - namespaces\\n  - services\\n  - pods\\n\n      \\ - endpoints\\n  - nodes\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n\n      \\ - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n\n      \\ - list\\n  - watch\\n  # This is used when validating policies in preflight.\n      This will need to stay\\n  # until we figure out how to avoid \\\"get\\\" inside\n      the preflight, and then\\n  # should be removed ideally.\\n  - get\\n- apiGroups:\\n\n      \\ - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools\\n  - ciliumbgppeeringpolicies\\n\n      \\ - ciliumbgpnodeconfigs\\n  - ciliumbgpadvertisements\\n  - ciliumbgppeerconfigs\\n\n      \\ - ciliumclusterwideenvoyconfigs\\n  - ciliumclusterwidenetworkpolicies\\n  -\n      ciliumegressgatewaypolicies\\n  - ciliumendpoints\\n  - ciliumendpointslices\\n\n      \\ - ciliumenvoyconfigs\\n  - ciliumidentities\\n  - ciliumlocalredirectpolicies\\n\n      \\ - ciliumnetworkpolicies\\n  - ciliumnodes\\n  - ciliumnodeconfigs\\n  - ciliumcidrgroups\\n\n      \\ - ciliuml2announcementpolicies\\n  - ciliumpodippools\\n  verbs:\\n  - list\\n\n      \\ - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumidentities\\n\n      \\ - ciliumendpoints\\n  - ciliumnodes\\n  verbs:\\n  - create\\n- apiGroups:\\n  -\n      cilium.io\\n  # To synchronize garbage collection of such resources\\n  resources:\\n\n      \\ - ciliumidentities\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n\n      \\ - ciliumendpoints\\n  verbs:\\n  - delete\\n  - get\\n- apiGroups:\\n  - cilium.io\\n\n      \\ resources:\\n  - ciliumnodes\\n  - ciliumnodes/status\\n  verbs:\\n  - get\\n  -\n      update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints/status\\n\n      \\ - ciliumendpoints\\n  - ciliuml2announcementpolicies/status\\n  - ciliumbgpnodeconfigs/status\\n\n      \\ verbs:\\n  - patch\\n---\\n# Source: cilium/templates/cilium-operator/clusterrole.yaml\\napiVersion:\n      rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cilium-operator\\n\n      \\ labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  -\n      \\\"\\\"\\n  resources:\\n  - pods\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n  # to\n      automatically delete [core|kube]dns pods so that are starting to being\\n  #\n      managed by Cilium\\n  - delete\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - configmaps\\n\n      \\ resourceNames:\\n  - cilium-config\\n  verbs:\\n   # allow patching of the configmap\n      to set annotations\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - nodes\\n\n      \\ verbs:\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # To\n      remove node taints\\n  - nodes\\n  # To set NetworkUnavailable false on startup\\n\n      \\ - nodes/status\\n  verbs:\\n  - patch\\n- apiGroups:\\n  - discovery.k8s.io\\n\n      \\ resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n-\n      apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to perform LB IP allocation for BGP\\n\n      \\ - services/status\\n  verbs:\\n  - update\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n\n      \\ resources:\\n  # to check apiserver connectivity\\n  - namespaces\\n  - secrets\\n\n      \\ verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n\n      \\ # to perform the translation of a CNP that contains `ToGroup` to its endpoints\\n\n      \\ - services\\n  - endpoints\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n\n      \\ - cilium.io\\n  resources:\\n  - ciliumnetworkpolicies\\n  - ciliumclusterwidenetworkpolicies\\n\n      \\ verbs:\\n  # Create auto-generated CNPs and CCNPs from Policies that have 'toGroups'\\n\n      \\ - create\\n  - update\\n  - deletecollection\\n  # To update the status of the\n      CNPs and CCNPs\\n  - patch\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n\n      \\ resources:\\n  - ciliumnetworkpolicies/status\\n  - ciliumclusterwidenetworkpolicies/status\\n\n      \\ verbs:\\n  # Update the auto-generated CNPs and CCNPs status.\\n  - patch\\n\n      \\ - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints\\n\n      \\ - ciliumidentities\\n  verbs:\\n  # To perform garbage collection of such resources\\n\n      \\ - delete\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n\n      \\ - ciliumidentities\\n  verbs:\\n  # To synchronize garbage collection of such\n      resources\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes\\n\n      \\ verbs:\\n  - create\\n  - update\\n  - get\\n  - list\\n  - watch\\n    # To perform\n      CiliumNode garbage collector\\n  - delete\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n\n      \\ - ciliumnodes/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n\n      \\ resources:\\n  - ciliumendpointslices\\n  - ciliumenvoyconfigs\\n  - ciliumbgppeerconfigs\\n\n      \\ - ciliumbgpadvertisements\\n  - ciliumbgpnodeconfigs\\n  verbs:\\n  - create\\n\n      \\ - update\\n  - get\\n  - list\\n  - watch\\n  - delete\\n  - patch\\n- apiGroups:\\n\n      \\ - cilium.io\\n  resources:\\n  - ciliumbgpclusterconfigs/status\\n  - ciliumbgppeerconfigs/status\\n\n      \\ verbs:\\n  - update\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n\n      \\ - customresourcedefinitions\\n  verbs:\\n  - create\\n  - get\\n  - list\\n  -\n      watch\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n\n      \\ verbs:\\n  - update\\n  resourceNames:\\n  - ciliumloadbalancerippools.cilium.io\\n\n      \\ - ciliumbgppeeringpolicies.cilium.io\\n  - ciliumbgpclusterconfigs.cilium.io\\n\n      \\ - ciliumbgppeerconfigs.cilium.io\\n  - ciliumbgpadvertisements.cilium.io\\n\n      \\ - ciliumbgpnodeconfigs.cilium.io\\n  - ciliumbgpnodeconfigoverrides.cilium.io\\n\n      \\ - ciliumclusterwideenvoyconfigs.cilium.io\\n  - ciliumclusterwidenetworkpolicies.cilium.io\\n\n      \\ - ciliumegressgatewaypolicies.cilium.io\\n  - ciliumendpoints.cilium.io\\n  -\n      ciliumendpointslices.cilium.io\\n  - ciliumenvoyconfigs.cilium.io\\n  - ciliumidentities.cilium.io\\n\n      \\ - ciliumlocalredirectpolicies.cilium.io\\n  - ciliumnetworkpolicies.cilium.io\\n\n      \\ - ciliumnodes.cilium.io\\n  - ciliumnodeconfigs.cilium.io\\n  - ciliumcidrgroups.cilium.io\\n\n      \\ - ciliuml2announcementpolicies.cilium.io\\n  - ciliumpodippools.cilium.io\\n\n      \\ - ciliumgatewayclassconfigs.cilium.io\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n\n      \\ - ciliumloadbalancerippools\\n  - ciliumpodippools\\n  - ciliumbgppeeringpolicies\\n\n      \\ - ciliumbgpclusterconfigs\\n  - ciliumbgpnodeconfigoverrides\\n  - ciliumbgppeerconfigs\\n\n      \\ verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n    - cilium.io\\n  resources:\\n\n      \\   - ciliumpodippools\\n  verbs:\\n    - create\\n- apiGroups:\\n  - cilium.io\\n\n      \\ resources:\\n  - ciliumloadbalancerippools/status\\n  verbs:\\n  - patch\\n# For\n      cilium-operator running in HA mode.\\n#\\n# Cilium operator running in HA mode\n      requires the use of ResourceLock for Leader Election\\n# between multiple running\n      instances.\\n# The preferred way of doing this is to use LeasesResourceLock as\n      edits to Leases are less\\n# common and fewer objects in the cluster watch \\\"all\n      Leases\\\".\\n- apiGroups:\\n  - coordination.k8s.io\\n  resources:\\n  - leases\\n\n      \\ verbs:\\n  - create\\n  - get\\n  - update\\n---\\n# Source: cilium/templates/cilium-agent/clusterrolebinding.yaml\\napiVersion:\n      rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium\\n\n      \\ labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n\n      \\ kind: ClusterRole\\n  name: cilium\\nsubjects:\\n- kind: ServiceAccount\\n  name:\n      \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/clusterrolebinding.yaml\\napiVersion:\n      rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium-operator\\n\n      \\ labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n\n      \\ kind: ClusterRole\\n  name: cilium-operator\\nsubjects:\\n- kind: ServiceAccount\\n\n      \\ name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n\\n\"\n    \"name\": \"cilium\"\n  - \"contents\": \"---\\n# Source: hcloud-cloud-controller-manager/templates/serviceaccount.yaml\\napiVersion:\n      v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n\n      \\ namespace: kube-system\\n---\\n# Source: hcloud-cloud-controller-manager/templates/clusterrolebinding.yaml\\nkind:\n      ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name:\n      \\\"system:hcloud-cloud-controller-manager\\\"\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n\n      \\ kind: ClusterRole\\n  name: cluster-admin\\nsubjects:\\n  - kind: ServiceAccount\\n\n      \\   name: hcloud-cloud-controller-manager\\n    namespace: kube-system\\n---\\n#\n      Source: hcloud-cloud-controller-manager/templates/daemonset.yaml\\napiVersion:\n      apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n\n      \\ namespace: kube-system\\nspec:\\n  revisionHistoryLimit: 2\\n  selector:\\n    matchLabels:\\n\n      \\     app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n      app.kubernetes.io/name:\n      'hcloud-cloud-controller-manager'\\n  template:\\n    metadata:\\n      labels:\\n\n      \\       app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n        app.kubernetes.io/name:\n      'hcloud-cloud-controller-manager'\\n    spec:\\n      serviceAccountName: hcloud-cloud-controller-manager\\n\n      \\     dnsPolicy: Default\\n      tolerations:\\n        # Allow HCCM itself to\n      schedule on nodes that have not yet been initialized by HCCM.\\n        - key:\n      \\\"node.cloudprovider.kubernetes.io/uninitialized\\\"\\n          value: \\\"true\\\"\\n\n      \\         effect: \\\"NoSchedule\\\"\\n        - key: \\\"CriticalAddonsOnly\\\"\\n          operator:\n      \\\"Exists\\\"\\n\\n        # Allow HCCM to schedule on control plane nodes.\\n        -\n      key: \\\"node-role.kubernetes.io/master\\\"\\n          effect: NoSchedule\\n          operator:\n      Exists\\n        - key: \\\"node-role.kubernetes.io/control-plane\\\"\\n          effect:\n      NoSchedule\\n          operator: Exists\\n\\n        - key: \\\"node.kubernetes.io/not-ready\\\"\\n\n      \\         effect: \\\"NoExecute\\\"\\n      nodeSelector:\\n        \\n        node-role.kubernetes.io/control-plane:\n      \\\"\\\"\\n      hostNetwork: true\\n      containers:\\n        - name: hcloud-cloud-controller-manager\\n\n      \\         command:\\n            - \\\"/bin/hcloud-cloud-controller-manager\\\"\\n\n      \\           - \\\"--allow-untagged-cloud\\\"\\n            - \\\"--cloud-provider=hcloud\\\"\\n\n      \\           - \\\"--route-reconciliation-period=30s\\\"\\n            - \\\"--webhook-secure-port=0\\\"\\n\n      \\           - \\\"--allocate-node-cidrs=true\\\"\\n            - \\\"--cluster-cidr=10.0.128.0/17\\\"\\n\n      \\         env:\\n            - name: HCLOUD_LOAD_BALANCERS_DISABLE_PRIVATE_INGRESS\\n\n      \\             value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_ENABLED\\n\n      \\             value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_LOCATION\\n\n      \\             value: hel1\\n            - name: HCLOUD_LOAD_BALANCERS_USE_PRIVATE_IP\\n\n      \\             value: \\\"true\\\"\\n            - name: HCLOUD_NETWORK_ROUTES_ENABLED\\n\n      \\             value: \\\"true\\\"\\n            - name: HCLOUD_TOKEN\\n              valueFrom:\\n\n      \\               secretKeyRef:\\n                  key: token\\n                  name:\n      hcloud\\n            - name: ROBOT_PASSWORD\\n              valueFrom:\\n                secretKeyRef:\\n\n      \\                 key: robot-password\\n                  name: hcloud\\n                  optional:\n      true\\n            - name: ROBOT_USER\\n              valueFrom:\\n                secretKeyRef:\\n\n      \\                 key: robot-user\\n                  name: hcloud\\n                  optional:\n      true\\n            - name: HCLOUD_NETWORK\\n              valueFrom:\\n                secretKeyRef:\\n\n      \\                 key: network\\n                  name: hcloud\\n          image:\n      docker.io/hetznercloud/hcloud-cloud-controller-manager:v1.26.0 # x-releaser-pleaser-version\\n\n      \\         ports:\\n            - name: metrics\\n              containerPort:\n      8233\\n          resources:\\n            requests:\\n              cpu: 100m\\n\n      \\             memory: 50Mi\\n      priorityClassName: system-cluster-critical\\n\"\n    \"name\": \"hcloud-ccm\"\n  - \"contents\": \"\\\"apiVersion\\\": \\\"v1\\\"\\n\\\"data\\\":\\n  \\\"encryption-passphrase\\\": \\\"aXlJaUllVnpKY1l0cFFyWCZIQURkQ1lzWnd2RnRTTXB0dUgmRG51Q2VxJldic3Fl\\\"\\n\\\"kind\\\":\n      \\\"Secret\\\"\\n\\\"metadata\\\":\\n  \\\"name\\\": \\\"hcloud-csi-secret\\\"\\n  \\\"namespace\\\":\n      \\\"kube-system\\\"\\n\\\"type\\\": \\\"Opaque\\\"\\n\\n---\\n---\\n# Source: hcloud-csi/templates/controller/serviceaccount.yaml\\napiVersion:\n      v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace:\n      \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart:\n      hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by:\n      Helm\\n    app.kubernetes.io/component: controller\\nautomountServiceAccountToken:\n      true\\n---\\n# Source: hcloud-csi/templates/core/storageclass.yaml\\nkind: StorageClass\\napiVersion:\n      storage.k8s.io/v1\\nmetadata:\\n  name: hcloud-volumes-encrypted-xfs\\n  annotations:\\n\n      \\   storageclass.kubernetes.io/is-default-class: \\\"true\\\"\\nprovisioner: csi.hetzner.cloud\\nvolumeBindingMode:\n      WaitForFirstConsumer\\nallowVolumeExpansion: true\\nreclaimPolicy: \\\"Retain\\\"\\nparameters:\\n\n      \\ csi.storage.k8s.io/fstype: xfs\\n  csi.storage.k8s.io/node-publish-secret-name:\n      hcloud-csi-secret\\n  csi.storage.k8s.io/node-publish-secret-namespace: kube-system\\n\n      \\ fsFormatOption: -i nrext64=1\\n---\\n# Source: hcloud-csi/templates/controller/clusterrole.yaml\\nkind:\n      ClusterRole\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: hcloud-csi-controller\\n\n      \\ labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n\n      \\   app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by:\n      Helm\\n    app.kubernetes.io/component: controller\\nrules:\\n  # attacher\\n  -\n      apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n    verbs: [get, list,\n      watch, update, patch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [nodes]\\n    verbs:\n      [get, list, watch]\\n  - apiGroups: [csi.storage.k8s.io]\\n    resources: [csinodeinfos]\\n\n      \\   verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n    resources:\n      [csinodes]\\n    verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n\n      \\   resources: [volumeattachments]\\n    verbs: [get, list, watch, update, patch]\\n\n      \\ - apiGroups: [storage.k8s.io]\\n    resources: [volumeattachments/status]\\n\n      \\   verbs: [patch]\\n  # provisioner\\n  - apiGroups: [\\\"\\\"]\\n    resources: [secrets]\\n\n      \\   verbs: [get, list]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n\n      \\   verbs: [get, list, watch, create, delete, patch]\\n  - apiGroups: [\\\"\\\"]\\n\n      \\   resources: [persistentvolumeclaims, persistentvolumeclaims/status]\\n    verbs:\n      [get, list, watch, update, patch]\\n  - apiGroups: [storage.k8s.io]\\n    resources:\n      [storageclasses]\\n    verbs: [get, list, watch]\\n  - apiGroups: [\\\"\\\"]\\n    resources:\n      [events]\\n    verbs: [list, watch, create, update, patch]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n\n      \\   resources: [volumesnapshots]\\n    verbs: [get, list]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n\n      \\   resources: [volumesnapshotcontents]\\n    verbs: [get, list]\\n  # resizer\\n\n      \\ - apiGroups: [\\\"\\\"]\\n    resources: [pods]\\n    verbs: [get, list, watch]\\n\n      \\ # node\\n  - apiGroups: [\\\"\\\"]\\n    resources: [events]\\n    verbs: [get, list,\n      watch, create, update, patch]\\n---\\n# Source: hcloud-csi/templates/controller/clusterrolebinding.yaml\\nkind:\n      ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name:\n      hcloud-csi-controller\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart:\n      hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by:\n      Helm\\n    app.kubernetes.io/component: controller\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n\n      \\ kind: ClusterRole\\n  name: hcloud-csi-controller\\nsubjects:\\n  - kind: ServiceAccount\\n\n      \\   name: hcloud-csi-controller\\n    namespace: \\\"kube-system\\\"\\n---\\n# Source:\n      hcloud-csi/templates/node/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n\n      \\ name: hcloud-csi-node\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name:\n      hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance:\n      hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component:\n      node\\n    app: hcloud-csi\\nspec:\\n  updateStrategy:\\n    type: RollingUpdate\\n\n      \\ selector:\\n    matchLabels:\\n      app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance:\n      hcloud-csi\\n      app.kubernetes.io/component: node\\n  template:\\n    metadata:\\n\n      \\     labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart:\n      hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by:\n      Helm\\n        app.kubernetes.io/component: node\\n    spec:\\n      \\n      affinity:\\n\n      \\       nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n\n      \\           nodeSelectorTerms:\\n            - matchExpressions:\\n              -\n      key: instance.hetzner.cloud/is-root-server\\n                operator: NotIn\\n\n      \\               values:\\n                - \\\"true\\\"\\n              - key: instance.hetzner.cloud/provided-by\\n\n      \\               operator: NotIn\\n                values:\\n                -\n      robot\\n      tolerations:\\n        - effect: NoExecute\\n          operator:\n      Exists\\n        - effect: NoSchedule\\n          operator: Exists\\n        -\n      key: CriticalAddonsOnly\\n          operator: Exists\\n      securityContext:\\n\n      \\       fsGroup: 1001\\n      initContainers:\\n      containers:\\n        - name:\n      csi-node-driver-registrar\\n          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.14.0\\n\n      \\         imagePullPolicy: IfNotPresent\\n          args:\\n            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.hetzner.cloud/socket\\n\n      \\         volumeMounts:\\n            - name: plugin-dir\\n              mountPath:\n      /run/csi\\n            - name: registration-dir\\n              mountPath: /registration\\n\n      \\         resources:\\n            limits: {}\\n            requests: {}\\n        -\n      name: liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n\n      \\         imagePullPolicy: IfNotPresent\\n          volumeMounts:\\n          -\n      mountPath: /run/csi\\n            name: plugin-dir\\n          resources:\\n            limits:\n      {}\\n            requests: {}\\n        - name: hcloud-csi-driver\\n          image:\n      docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n\n      \\         imagePullPolicy: IfNotPresent\\n          args:\\n            - -node\\n\n      \\         volumeMounts:\\n            - name: kubelet-dir\\n              mountPath:\n      /var/lib/kubelet\\n              mountPropagation: \\\"Bidirectional\\\"\\n            -\n      name: plugin-dir\\n              mountPath: /run/csi\\n            - name: device-dir\\n\n      \\             mountPath: /dev\\n          securityContext:\\n            privileged:\n      true\\n          env:\\n            - name: CSI_ENDPOINT\\n              value:\n      unix:///run/csi/socket\\n            - name: ENABLE_METRICS\\n              value:\n      \\\"false\\\"\\n          ports:\\n            - name: healthz\\n              protocol:\n      TCP\\n              containerPort: 9808\\n          resources:\\n            limits:\n      {}\\n            requests: {}\\n          livenessProbe:\\n            failureThreshold:\n      5\\n            initialDelaySeconds: 10\\n            periodSeconds: 2\\n            successThreshold:\n      1\\n            timeoutSeconds: 3\\n            httpGet:\\n              path:\n      /healthz\\n              port: healthz\\n      volumes:\\n        - name: kubelet-dir\\n\n      \\         hostPath:\\n            path: /var/lib/kubelet\\n            type: Directory\\n\n      \\       - name: plugin-dir\\n          hostPath:\\n            path: /var/lib/kubelet/plugins/csi.hetzner.cloud/\\n\n      \\           type: DirectoryOrCreate\\n        - name: registration-dir\\n          hostPath:\\n\n      \\           path: /var/lib/kubelet/plugins_registry/\\n            type: Directory\\n\n      \\       - name: device-dir\\n          hostPath:\\n            path: /dev\\n            type:\n      Directory\\n---\\n# Source: hcloud-csi/templates/controller/deployment.yaml\\napiVersion:\n      apps/v1\\nkind: Deployment\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace:\n      \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart:\n      hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by:\n      Helm\\n    app.kubernetes.io/component: controller\\n    app: hcloud-csi-controller\\nspec:\\n\n      \\ replicas: 1\\n  strategy:\\n    type: RollingUpdate\\n  selector:\\n    matchLabels:\\n\n      \\     app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance:\n      hcloud-csi\\n      app.kubernetes.io/component: controller\\n  template:\\n    metadata:\\n\n      \\     labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart:\n      hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by:\n      Helm\\n        app.kubernetes.io/component: controller\\n    spec:\\n      serviceAccountName:\n      hcloud-csi-controller\\n      \\n      affinity:\\n        nodeAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n\n      \\         - preference:\\n              matchExpressions:\\n              - key:\n      instance.hetzner.cloud/provided-by\\n                operator: In\\n                values:\\n\n      \\               - cloud\\n            weight: 1\\n      nodeSelector:\\n        node-role.kubernetes.io/control-plane:\n      \\\"\\\"\\n      tolerations:\\n        - effect: NoSchedule\\n          key: node-role.kubernetes.io/control-plane\\n\n      \\         operator: Exists\\n      topologySpreadConstraints:\\n        - labelSelector:\\n\n      \\           matchLabels:\\n              app.kubernetes.io/component: controller\\n\n      \\             app.kubernetes.io/instance: hcloud-csi\\n              app.kubernetes.io/name:\n      hcloud-csi\\n          maxSkew: 1\\n          topologyKey: kubernetes.io/hostname\\n\n      \\         whenUnsatisfiable: ScheduleAnyway\\n      securityContext:\\n        fsGroup:\n      1001\\n      initContainers:\\n      containers:\\n        - name: csi-attacher\\n\n      \\         image: registry.k8s.io/sig-storage/csi-attacher:v4.9.0\\n          imagePullPolicy:\n      IfNotPresent\\n          resources:\\n            limits: {}\\n            requests:\n      {}\\n          args:\\n            - --default-fstype=ext4\\n          volumeMounts:\\n\n      \\         - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name:\n      csi-resizer\\n          image: registry.k8s.io/sig-storage/csi-resizer:v1.14.0\\n\n      \\         imagePullPolicy: IfNotPresent\\n          resources:\\n            limits:\n      {}\\n            requests: {}\\n          volumeMounts:\\n          - name: socket-dir\\n\n      \\           mountPath: /run/csi\\n\\n        - name: csi-provisioner\\n          image:\n      registry.k8s.io/sig-storage/csi-provisioner:v5.3.0\\n          imagePullPolicy:\n      IfNotPresent\\n          resources:\\n            limits: {}\\n            requests:\n      {}\\n          args:\\n            - --feature-gates=Topology=true\\n            -\n      --default-fstype=ext4\\n            - --extra-create-metadata\\n          volumeMounts:\\n\n      \\         - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name:\n      liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n\n      \\         imagePullPolicy: IfNotPresent\\n          resources:\\n            limits:\n      {}\\n            requests: {}\\n          volumeMounts:\\n          - mountPath:\n      /run/csi\\n            name: socket-dir\\n\\n        - name: hcloud-csi-driver\\n\n      \\         image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n\n      \\         imagePullPolicy: IfNotPresent\\n          args:\\n            - -controller\\n\n      \\         env:\\n            - name: CSI_ENDPOINT\\n              value: unix:///run/csi/socket\\n\n      \\           - name: ENABLE_METRICS\\n              value: \\\"false\\\"\\n            -\n      name: KUBE_NODE_NAME\\n              valueFrom:\\n                fieldRef:\\n\n      \\                 apiVersion: v1\\n                  fieldPath: spec.nodeName\\n\n      \\           - name: HCLOUD_TOKEN\\n              valueFrom:\\n                secretKeyRef:\\n\n      \\                 name: hcloud\\n                  key: token\\n          resources:\\n\n      \\           limits: {}\\n            requests: {}\\n          ports:\\n            -\n      name: healthz\\n              protocol: TCP\\n              containerPort: 9808\\n\n      \\         livenessProbe:\\n            failureThreshold: 5\\n            initialDelaySeconds:\n      10\\n            periodSeconds: 2\\n            successThreshold: 1\\n            timeoutSeconds:\n      3\\n            httpGet:\\n              path: /healthz\\n              port: healthz\\n\n      \\         volumeMounts:\\n            - name: socket-dir\\n              mountPath:\n      /run/csi\\n\\n      volumes:\\n        - name: socket-dir\\n          emptyDir:\n      {}\\n---\\n# Source: hcloud-csi/templates/core/csidriver.yaml\\napiVersion: storage.k8s.io/v1\\nkind:\n      CSIDriver\\nmetadata:\\n  name: csi.hetzner.cloud\\nspec:\\n  attachRequired: true\\n\n      \\ fsGroupPolicy: File\\n  podInfoOnMount: true\\n  seLinuxMount: true\\n  volumeLifecycleModes:\\n\n      \\ - Persistent\\n\\n\"\n    \"name\": \"hcloud-csi\"\n  - \"contents\": |+\n      \"apiVersion\": \"talos.dev/v1alpha1\"\n      \"kind\": \"ServiceAccount\"\n      \"metadata\":\n        \"name\": \"talos-backup-secrets\"\n        \"namespace\": \"kube-system\"\n      \"spec\":\n        \"roles\":\n        - \"os:etcd:backup\"\n\n      ---\n      \"apiVersion\": \"v1\"\n      \"data\":\n        \"access_key\": \"YzYwNDY2MDQ4ODdjMDgzZTk2OWUyMWJjNTVhMjQxNzQ=\"\n        \"secret_key\": \"MzI2NTEzNjY0ZWU4N2RiMTVmMWNjOGViZTcyMDBhYTEwMGI5YTFlNzRhZTM1NzZkNWJhMDg4MGQxMjg2YTlmNg==\"\n      \"kind\": \"Secret\"\n      \"metadata\":\n        \"name\": \"talos-backup-s3-secrets\"\n        \"namespace\": \"kube-system\"\n      \"type\": \"Opaque\"\n\n      ---\n      \"apiVersion\": \"batch/v1\"\n      \"kind\": \"CronJob\"\n      \"metadata\":\n        \"name\": \"talos-backup\"\n        \"namespace\": \"kube-system\"\n      \"spec\":\n        \"concurrencyPolicy\": \"Forbid\"\n        \"jobTemplate\":\n          \"spec\":\n            \"template\":\n              \"spec\":\n                \"containers\":\n                - \"env\":\n                  - \"name\": \"AWS_ACCESS_KEY_ID\"\n                    \"valueFrom\":\n                      \"secretKeyRef\":\n                        \"key\": \"access_key\"\n                        \"name\": \"talos-backup-s3-secrets\"\n                  - \"name\": \"AWS_SECRET_ACCESS_KEY\"\n                    \"valueFrom\":\n                      \"secretKeyRef\":\n                        \"key\": \"secret_key\"\n                        \"name\": \"talos-backup-s3-secrets\"\n                  - \"name\": \"AGE_X25519_PUBLIC_KEY\"\n                    \"value\": null\n                  - \"name\": \"DISABLE_ENCRYPTION\"\n                    \"value\": \"true\"\n                  - \"name\": \"AWS_REGION\"\n                    \"value\": \"auto\"\n                  - \"name\": \"CUSTOM_S3_ENDPOINT\"\n                    \"value\": \"https://a694d529ab7d7176bcac8585f8bafdf4.r2.cloudflarestorage.com\"\n                  - \"name\": \"BUCKET\"\n                    \"value\": \"etcd\"\n                  - \"name\": \"CLUSTER_NAME\"\n                    \"value\": \"goingdark\"\n                  - \"name\": \"S3_PREFIX\"\n                    \"value\": null\n                  - \"name\": \"USE_PATH_STYLE\"\n                    \"value\": \"true\"\n                  \"image\": \"ghcr.io/siderolabs/talos-backup:v0.1.0-beta.2-1-g9ccc125\"\n                  \"imagePullPolicy\": \"IfNotPresent\"\n                  \"name\": \"talos-backup\"\n                  \"resources\":\n                    \"limits\":\n                      \"cpu\": \"500m\"\n                      \"memory\": \"256Mi\"\n                    \"requests\":\n                      \"cpu\": \"250m\"\n                      \"memory\": \"128Mi\"\n                  \"securityContext\":\n                    \"allowPrivilegeEscalation\": false\n                    \"capabilities\":\n                      \"drop\":\n                      - \"ALL\"\n                    \"runAsGroup\": 1000\n                    \"runAsNonRoot\": true\n                    \"runAsUser\": 1000\n                    \"seccompProfile\":\n                      \"type\": \"RuntimeDefault\"\n                  \"volumeMounts\":\n                  - \"mountPath\": \"/tmp\"\n                    \"name\": \"tmp\"\n                  - \"mountPath\": \"/var/run/secrets/talos.dev\"\n                    \"name\": \"talos-secrets\"\n                  \"workingDir\": \"/tmp\"\n                \"restartPolicy\": \"OnFailure\"\n                \"tolerations\":\n                - \"effect\": \"NoSchedule\"\n                  \"key\": \"node-role.kubernetes.io/control-plane\"\n                  \"operator\": \"Exists\"\n                \"volumes\":\n                - \"emptyDir\": {}\n                  \"name\": \"tmp\"\n                - \"name\": \"talos-secrets\"\n                  \"secret\":\n                    \"secretName\": \"talos-backup-secrets\"\n        \"schedule\": \"0 * * * *\"\n        \"suspend\": false\n\n    \"name\": \"talos-backup\"\n  - \"contents\": |\n      ---\n      # Source: metrics-server/templates/pdb.yaml\n      apiVersion: policy/v1\n      kind: PodDisruptionBudget\n      metadata:\n        name: metrics-server\n        namespace: kube-system\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        minAvailable: 1\n\n        selector:\n          matchLabels:\n            app.kubernetes.io/name: metrics-server\n            app.kubernetes.io/instance: metrics-server\n      ---\n      # Source: metrics-server/templates/serviceaccount.yaml\n      apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: metrics-server\n        namespace: kube-system\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      ---\n      # Source: metrics-server/templates/clusterrole-aggregated-reader.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: ClusterRole\n      metadata:\n        name: system:metrics-server-aggregated-reader\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n          rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n          rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n          rbac.authorization.k8s.io/aggregate-to-view: \"true\"\n      rules:\n        - apiGroups:\n            - metrics.k8s.io\n          resources:\n            - pods\n            - nodes\n          verbs:\n            - get\n            - list\n            - watch\n      ---\n      # Source: metrics-server/templates/clusterrole.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: ClusterRole\n      metadata:\n        name: system:metrics-server\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      rules:\n        - apiGroups:\n          - \"\"\n          resources:\n          - nodes/metrics\n          verbs:\n          - get\n        - apiGroups:\n          - \"\"\n          resources:\n            - pods\n            - nodes\n            - namespaces\n            - configmaps\n          verbs:\n            - get\n            - list\n            - watch\n      ---\n      # Source: metrics-server/templates/clusterrolebinding-auth-delegator.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: ClusterRoleBinding\n      metadata:\n        name: metrics-server:system:auth-delegator\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      roleRef:\n        apiGroup: rbac.authorization.k8s.io\n        kind: ClusterRole\n        name: system:auth-delegator\n      subjects:\n        - kind: ServiceAccount\n          name: metrics-server\n          namespace: kube-system\n      ---\n      # Source: metrics-server/templates/clusterrolebinding.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: ClusterRoleBinding\n      metadata:\n        name: system:metrics-server\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      roleRef:\n        apiGroup: rbac.authorization.k8s.io\n        kind: ClusterRole\n        name: system:metrics-server\n      subjects:\n        - kind: ServiceAccount\n          name: metrics-server\n          namespace: kube-system\n      ---\n      # Source: metrics-server/templates/rolebinding.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: RoleBinding\n      metadata:\n        name: metrics-server-auth-reader\n        namespace: kube-system\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      roleRef:\n        apiGroup: rbac.authorization.k8s.io\n        kind: Role\n        name: extension-apiserver-authentication-reader\n      subjects:\n        - kind: ServiceAccount\n          name: metrics-server\n          namespace: kube-system\n      ---\n      # Source: metrics-server/templates/service.yaml\n      apiVersion: v1\n      kind: Service\n      metadata:\n        name: metrics-server\n        namespace: kube-system\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        type: ClusterIP\n        ports:\n          - name: https\n            port: 443\n            protocol: TCP\n            targetPort: https\n            appProtocol: https\n        selector:\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n      ---\n      # Source: metrics-server/templates/deployment.yaml\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: metrics-server\n        namespace: kube-system\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        replicas: 2\n        selector:\n          matchLabels:\n            app.kubernetes.io/name: metrics-server\n            app.kubernetes.io/instance: metrics-server\n        template:\n          metadata:\n            labels:\n              app.kubernetes.io/name: metrics-server\n              app.kubernetes.io/instance: metrics-server\n          spec:\n            serviceAccountName: metrics-server\n            priorityClassName: \"system-cluster-critical\"\n            containers:\n              - name: metrics-server\n                securityContext:\n                  allowPrivilegeEscalation: false\n                  capabilities:\n                    drop:\n                    - ALL\n                  readOnlyRootFilesystem: true\n                  runAsNonRoot: true\n                  runAsUser: 1000\n                  seccompProfile:\n                    type: RuntimeDefault\n                image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n                imagePullPolicy: IfNotPresent\n                args:\n                  - --secure-port=10250\n                  - --cert-dir=/tmp\n                  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n                  - --kubelet-use-node-status-port\n                  - --metric-resolution=15s\n                ports:\n                - name: https\n                  protocol: TCP\n                  containerPort: 10250\n                livenessProbe:\n                  failureThreshold: 3\n                  httpGet:\n                    path: /livez\n                    port: https\n                    scheme: HTTPS\n                  initialDelaySeconds: 0\n                  periodSeconds: 10\n                readinessProbe:\n                  failureThreshold: 3\n                  httpGet:\n                    path: /readyz\n                    port: https\n                    scheme: HTTPS\n                  initialDelaySeconds: 20\n                  periodSeconds: 10\n                volumeMounts:\n                  - name: tmp\n                    mountPath: /tmp\n                resources:\n                  requests:\n                    cpu: 100m\n                    memory: 200Mi\n            volumes:\n              - name: tmp\n                emptyDir: {}\n            topologySpreadConstraints:\n              - labelSelector:\n                  matchLabels:\n                    app.kubernetes.io/instance: metrics-server\n                    app.kubernetes.io/name: metrics-server\n                maxSkew: 1\n                topologyKey: kubernetes.io/hostname\n                whenUnsatisfiable: ScheduleAnyway\n      ---\n      # Source: metrics-server/templates/apiservice.yaml\n      apiVersion: apiregistration.k8s.io/v1\n      kind: APIService\n      metadata:\n        name: v1beta1.metrics.k8s.io\n        labels:\n          helm.sh/chart: metrics-server-3.13.0\n          app.kubernetes.io/name: metrics-server\n          app.kubernetes.io/instance: metrics-server\n          app.kubernetes.io/version: \"0.8.0\"\n          app.kubernetes.io/managed-by: Helm\n        annotations:\n      spec:\n        group: metrics.k8s.io\n        groupPriorityMinimum: 100\n        insecureSkipTLSVerify: true\n        service:\n          name: metrics-server\n          namespace: kube-system\n          port: 443\n        version: v1beta1\n        versionPriority: 100\n    \"name\": \"metrics-server\"\n  - \"contents\": |+\n      ---\n      # Source: cluster-autoscaler/templates/pdb.yaml\n      apiVersion: policy/v1\n      kind: PodDisruptionBudget\n      metadata:\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n        namespace: kube-system\n      spec:\n        selector:\n          matchLabels:\n            app.kubernetes.io/instance: \"cluster-autoscaler\"\n            app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n      ---\n      # Source: cluster-autoscaler/templates/serviceaccount.yaml\n      apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n        namespace: kube-system\n      automountServiceAccountToken: true\n      ---\n      # Source: cluster-autoscaler/templates/clusterrole.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: ClusterRole\n      metadata:\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n      rules:\n        - apiGroups:\n            - \"\"\n          resources:\n            - events\n            - endpoints\n          verbs:\n            - create\n            - patch\n        - apiGroups:\n          - \"\"\n          resources:\n          - pods/eviction\n          verbs:\n          - create\n        - apiGroups:\n            - \"\"\n          resources:\n            - pods/status\n          verbs:\n            - update\n        - apiGroups:\n            - \"\"\n          resources:\n            - endpoints\n          resourceNames:\n            - cluster-autoscaler\n          verbs:\n            - get\n            - update\n        - apiGroups:\n            - \"\"\n          resources:\n            - nodes\n          verbs:\n          - watch\n          - list\n          - create\n          - delete\n          - get\n          - update\n        - apiGroups:\n          - \"\"\n          resources:\n            - namespaces\n            - pods\n            - services\n            - replicationcontrollers\n            - persistentvolumeclaims\n            - persistentvolumes\n          verbs:\n            - watch\n            - list\n            - get\n        - apiGroups:\n          - batch\n          resources:\n            - jobs\n            - cronjobs\n          verbs:\n            - watch\n            - list\n            - get\n        - apiGroups:\n          - batch\n          - extensions\n          resources:\n          - jobs\n          verbs:\n          - get\n          - list\n          - patch\n          - watch\n        - apiGroups:\n            - extensions\n          resources:\n            - replicasets\n            - daemonsets\n          verbs:\n            - watch\n            - list\n            - get\n        - apiGroups:\n            - policy\n          resources:\n            - poddisruptionbudgets\n          verbs:\n            - watch\n            - list\n        - apiGroups:\n          - apps\n          resources:\n          - daemonsets\n          - replicasets\n          - statefulsets\n          verbs:\n          - watch\n          - list\n          - get\n        - apiGroups:\n          - storage.k8s.io\n          resources:\n          - storageclasses\n          - csinodes\n          - csidrivers\n          - csistoragecapacities\n          - volumeattachments\n          verbs:\n          - watch\n          - list\n          - get\n        - apiGroups:\n            - \"\"\n          resources:\n            - configmaps\n          verbs:\n            - list\n            - watch\n            - get\n        - apiGroups:\n          - coordination.k8s.io\n          resources:\n          - leases\n          verbs:\n          - create\n        - apiGroups:\n          - coordination.k8s.io\n          resourceNames:\n          - cluster-autoscaler\n          resources:\n          - leases\n          verbs:\n          - get\n          - update\n      ---\n      # Source: cluster-autoscaler/templates/clusterrolebinding.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: ClusterRoleBinding\n      metadata:\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n      roleRef:\n        apiGroup: rbac.authorization.k8s.io\n        kind: ClusterRole\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n      subjects:\n        - kind: ServiceAccount\n          name: cluster-autoscaler-hetzner-cluster-autoscaler\n          namespace: kube-system\n      ---\n      # Source: cluster-autoscaler/templates/role.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: Role\n      metadata:\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n        namespace: kube-system\n      rules:\n        - apiGroups:\n            - \"\"\n          resources:\n            - configmaps\n          verbs:\n            - create\n        - apiGroups:\n            - \"\"\n          resources:\n            - configmaps\n          resourceNames:\n            - cluster-autoscaler-status\n          verbs:\n            - delete\n            - get\n            - update\n      ---\n      # Source: cluster-autoscaler/templates/rolebinding.yaml\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: RoleBinding\n      metadata:\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n        namespace: kube-system\n      roleRef:\n        apiGroup: rbac.authorization.k8s.io\n        kind: Role\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n      subjects:\n        - kind: ServiceAccount\n          name: cluster-autoscaler-hetzner-cluster-autoscaler\n          namespace: kube-system\n      ---\n      # Source: cluster-autoscaler/templates/service.yaml\n      apiVersion: v1\n      kind: Service\n      metadata:\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n        namespace: kube-system\n      spec:\n        ports:\n          - port: 8085\n            protocol: TCP\n            targetPort: 8085\n            name: http\n        selector:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n        type: \"ClusterIP\"\n      ---\n      # Source: cluster-autoscaler/templates/deployment.yaml\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        annotations:\n          {}\n        labels:\n          app.kubernetes.io/instance: \"cluster-autoscaler\"\n          app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          app.kubernetes.io/managed-by: \"Helm\"\n          helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n        name: cluster-autoscaler-hetzner-cluster-autoscaler\n        namespace: kube-system\n      spec:\n        replicas: 1\n        revisionHistoryLimit: 10\n        selector:\n          matchLabels:\n            app.kubernetes.io/instance: \"cluster-autoscaler\"\n            app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n        template:\n          metadata:\n            labels:\n              app.kubernetes.io/instance: \"cluster-autoscaler\"\n              app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n          spec:\n            priorityClassName: \"system-cluster-critical\"\n            dnsPolicy: \"ClusterFirst\"\n            containers:\n              - name: hetzner-cluster-autoscaler\n                image: \"registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0\"\n                imagePullPolicy: \"IfNotPresent\"\n                command:\n                  - ./cluster-autoscaler\n                  - --cloud-provider=hetzner\n                  - --namespace=kube-system\n                  - --nodes=0:2:cx43:hel1:goingdark-autoscaler\n                  - --balance-similar-node-groups=true\n                  - --expander=least-waste\n                  - --logtostderr=true\n                  - --scale-down-delay-after-add=10m\n                  - --scale-down-delay-after-delete=10m\n                  - --scale-down-unneeded-time=8m\n                  - --scale-down-utilization-threshold=0.75\n                  - --stderrthreshold=info\n                  - --v=4\n                env:\n                  - name: POD_NAMESPACE\n                    valueFrom:\n                      fieldRef:\n                        fieldPath: metadata.namespace\n                  - name: SERVICE_ACCOUNT\n                    valueFrom:\n                      fieldRef:\n                        fieldPath: spec.serviceAccountName\n                  - name: HCLOUD_CLUSTER_CONFIG_FILE\n                    value: \"/config/cluster-config\"\n                  - name: HCLOUD_FIREWALL\n                    value: \"2379867\"\n                  - name: HCLOUD_NETWORK\n                    value: \"11368893\"\n                  - name: HCLOUD_PUBLIC_IPV4\n                    value: \"true\"\n                  - name: HCLOUD_PUBLIC_IPV6\n                    value: \"false\"\n                  - name: HCLOUD_SERVER_CREATION_TIMEOUT\n                    value: \"10\"\n                  - name: HCLOUD_SSH_KEY\n                    value: \"101120850\"\n                  - name: HCLOUD_TOKEN\n                    valueFrom:\n                      secretKeyRef:\n                        name: hcloud\n                        key: token\n                livenessProbe:\n                  httpGet:\n                    path: /health-check\n                    port: 8085\n                ports:\n                  - containerPort: 8085\n                resources:\n                  {}\n                volumeMounts:\n                  - name: cluster-autoscaler-hetzner-config\n                    mountPath: /config\n                    readOnly: true\n            nodeSelector:\n              node-role.kubernetes.io/control-plane: \"\"\n            serviceAccountName: cluster-autoscaler-hetzner-cluster-autoscaler\n            tolerations:\n              - effect: NoSchedule\n                key: node-role.kubernetes.io/control-plane\n                operator: Exists\n            topologySpreadConstraints:\n              - labelSelector:\n                  matchLabels:\n                    app.kubernetes.io/instance: cluster-autoscaler\n                    app.kubernetes.io/name: hetzner-cluster-autoscaler\n                maxSkew: 1\n                topologyKey: kubernetes.io/hostname\n                whenUnsatisfiable: ScheduleAnyway\n            volumes:\n              - name: cluster-autoscaler-hetzner-config\n                secret:\n                  secretName: cluster-autoscaler-hetzner-config\n\n      ---\n      \"apiVersion\": \"v1\"\n      \"data\":\n        \"cluster-config\": \"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\"\n      \"kind\": \"Secret\"\n      \"metadata\":\n        \"name\": \"cluster-autoscaler-hetzner-config\"\n        \"namespace\": \"kube-system\"\n      \"type\": \"Opaque\"\n\n    \"name\": \"cluster-autoscaler\"\n  - \"contents\": \"apiVersion: cilium.io/v2alpha1\\nkind: CiliumL2AnnouncementPolicy\\nmetadata:\\n\n      \\ name: default-l2-announcement-policy\\n  namespace: kube-system\\nspec:\\n  externalIPs:\n      true\\n  loadBalancerIPs: true\\n\\n---\\napiVersion: cilium.io/v2alpha1\\nkind:\n      CiliumLoadBalancerIPPool\\nmetadata:\\n  name: service-pool\\nspec:\\n  blocks:\\n\n      \\   - start: 10.0.96.240\\n      stop: 10.0.96.250\\n\\n---\\napiVersion: v1\\nkind:\n      ConfigMap\\nmetadata:\\n  name: cilium-helm-values\\n  namespace: kube-system\\n\n      \\ labels:\\n    app.kubernetes.io/name: cilium-helm-values\\n    app.kubernetes.io/managed-by:\n      talos-inline\\n  data:\\n    values.yaml: |-\\n# https://github.com/cilium/cilium/blob/main/install/kubernetes/cilium/values.yaml\\n\n      \\     cluster:\\n        name: talos\\n        id: 1\\n      \\n      # Correct\n      boolean, not a nested map\\n      kubeProxyReplacement: true\\n      \\n        #\n      Talos specific\\n      k8sServiceHost: localhost\\n      k8sServicePort: 7445\\n\n      \\     securityContext:\\n        capabilities:\\n          ciliumAgent:\\n            [CHOWN,\n      KILL, NET_ADMIN, NET_RAW, IPC_LOCK, SYS_ADMIN, SYS_RESOURCE, DAC_OVERRIDE, FOWNER,\n      SETGID, SETUID]\\n          cleanCiliumState: [NET_ADMIN, SYS_ADMIN, SYS_RESOURCE]\\n\n      \\     \\n      cgroup:\\n        autoMount:\\n          enabled: false\\n        hostRoot:\n      /sys/fs/cgroup\\n      \\n      # https://www.talos.dev/latest/talos-guides/network/host-dns/#forwarding-kube-dns-to-host-dns\\n\n      \\     # https://docs.cilium.io/en/stable/operations/performance/tuning/#ebpf-host-routing\\n\n      \\     bpf:\\n        hostLegacyRouting: true\\n      \\n      # https://docs.cilium.io/en/stable/network/concepts/ipam/\\n\n      \\     ipam:\\n        mode: kubernetes\\n        multiPoolPreAllocation: ''\\n\n      \\     \\n      enableMulticast: false\\n      multicast:\\n        enabled: false\\n\n      \\     \\n      \\n      operator:\\n        rollOutPods: true\\n        resources:\\n\n      \\         requests:\\n            cpu: 50m\\n            memory: 128Mi\\n          limits:\\n\n      \\           memory: 256Mi\\n      \\n      # Roll out cilium agent pods automatically\n      when ConfigMap is updated.\\n      rollOutCiliumPods: true\\n      agent: true\\n\n      \\     agentConfig:\\n        tolerations:\\n          - key: gpu\\n            operator:\n      Equal\\n            value: \\\"true\\\"\\n            effect: NoSchedule\\n        resources:\\n\n      \\         requests:\\n            cpu: 100m      # Reduced from 500m - now Burstable\n      QoS\\n            memory: 128Mi   # Reduced from 512Mi\\n          limits:\\n            memory:\n      384Mi   # Remove CPU limit to prevent throttling\\n      \\n      \\n      \\n      #debug:\\n\n      \\     #  enabled: true\\n      \\n      # Increase rate limit when doing L2 announcements\\n\n      \\     k8sClientRateLimit:\\n        qps: 20\\n        burst: 100\\n      \\n      l2announcements:\\n\n      \\       enabled: true\\n      \\n      externalIPs:\\n        enabled: true\\n      \\n\n      \\     #enableCiliumEndpointSlice: true\\n      \\n      loadBalancer:\\n        #\n      https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#maglev-consistent-hashing\\n\n      \\       algorithm: maglev\\n      \\n      gatewayAPI:\\n        enabled: true\\n\n      \\       gatewayClass:\\n          create: \\\"true\\\"\\n        enableAlpn: true\\n\n      \\       xdsServer:\\n          enabled: true\\n      envoy:\\n        securityContext:\\n\n      \\         capabilities:\\n            keepCapNetBindService: true\\n            envoy:\n      [NET_BIND_SERVICE, NET_ADMIN, PERFMON, BPF]\\n      \\n      hubble:\\n        peerService:\\n\n      \\         clusterDomain: cluster.local\\n        enabled: true\\n        relay:\\n\n      \\         enabled: true\\n          rollOutPods: true\\n          resources:\\n\n      \\           requests:\\n              cpu: 100m\\n              memory: 128Mi\\n\n      \\           limits:\\n              cpu: 200m\\n              memory: 256Mi\\n\n      \\       ui:\\n          enabled: true\\n          rollOutPods: true\\n          resources:\\n\n      \\           requests:\\n              cpu: 100m\\n              memory: 128Mi\\n\n      \\           limits:\\n              cpu: 200m\\n              memory: 256Mi\\n\n      \\     \\n      ingressController:\\n        enabled: false\\n        default: true\\n\n      \\       loadbalancerMode: shared\\n        service:\\n          annotations:\\n\n      \\           io.cilium/lb-ipam-ips: 10.0.96.243\\n      \\n      # mTLS\\n      authentication:\\n\n      \\       enabled: false\\n        mutual:\\n          spire:\\n            enabled:\n      false\\n            install:\\n              server:\\n                dataStorage:\\n\n      \\                 storageClass: cilium-spire-sc\\n\"\n    \"name\": \"cilium-settings\"\n  \"network\":\n    \"cni\":\n      \"name\": \"none\"\n    \"dnsDomain\": \"cluster.local\"\n    \"podSubnets\":\n    - \"10.0.128.0/17\"\n    \"serviceSubnets\":\n    - \"10.0.96.0/19\"\n  \"proxy\":\n    \"disabled\": true\n  \"scheduler\":\n    \"extraArgs\":\n      \"bind-address\": \"0.0.0.0\"\n\"machine\":\n  \"certSANs\":\n  - \"10.0.64.1\"\n  - \"10.0.64.126\"\n  - \"10.0.64.254\"\n  - \"127.0.0.1\"\n  - \"46.62.164.172\"\n  - \"::1\"\n  - \"localhost\"\n  \"features\":\n    \"hostDNS\":\n      \"enabled\": true\n      \"forwardKubeDNSToHost\": false\n      \"resolveMemberNames\": true\n    \"kubernetesTalosAPIAccess\":\n      \"allowedKubernetesNamespaces\":\n      - \"kube-system\"\n      \"allowedRoles\":\n      - \"os:reader\"\n      - \"os:etcd:backup\"\n      \"enabled\": true\n  \"install\":\n    \"extraKernelArgs\": []\n    \"image\": \"factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\"\n  \"kernel\":\n    \"modules\": null\n  \"kubelet\":\n    \"extraArgs\":\n      \"cloud-provider\": \"external\"\n      \"rotate-server-certificates\": true\n    \"extraConfig\":\n      \"kubeReserved\":\n        \"cpu\": \"250m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"350Mi\"\n      \"registerWithTaints\":\n      - \"effect\": \"NoSchedule\"\n        \"key\": \"node-role.kubernetes.io/control-plane\"\n        \"value\": \"\"\n      \"shutdownGracePeriod\": \"90s\"\n      \"shutdownGracePeriodCriticalPods\": \"15s\"\n      \"systemReserved\":\n        \"cpu\": \"250m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"300Mi\"\n    \"extraMounts\": []\n    \"nodeIP\":\n      \"validSubnets\":\n      - \"10.0.64.0/19\"\n  \"logging\":\n    \"destinations\": []\n  \"network\":\n    \"extraHostEntries\": []\n    \"hostname\": \"goingdark-control-1\"\n    \"interfaces\":\n    - \"dhcp\": true\n      \"dhcpOptions\":\n        \"ipv4\": true\n        \"ipv6\": false\n      \"interface\": \"eth0\"\n      \"vip\": null\n    - \"dhcp\": true\n      \"interface\": \"eth1\"\n      \"routes\": []\n      \"vip\":\n        \"hcloud\":\n          \"apiToken\": \"lkbVSBpKQf0XMxYjXcxBLSUl9DYHEoWFnquZ2r0taZTUY1llFe9raH1hQXqZ9ZfQ\"\n        \"ip\": \"10.0.64.126\"\n    \"nameservers\":\n    - \"185.12.64.1\"\n    - \"185.12.64.2\"\n    - \"2a01:4ff:ff00::add:1\"\n    - \"2a01:4ff:ff00::add:2\"\n  \"nodeAnnotations\": {}\n  \"nodeLabels\":\n    \"nodepool\": \"control\"\n  \"nodeTaints\":\n    \"node-role.kubernetes.io/control-plane\": \":NoSchedule\"\n  \"registries\": null\n  \"sysctls\":\n    \"net.core.netdev_max_backlog\": \"4096\"\n    \"net.core.somaxconn\": \"65535\"\n    \"net.ipv6.conf.all.disable_ipv6\": 0\n    \"net.ipv6.conf.default.disable_ipv6\": 0\n  \"systemDiskEncryption\":\n    \"ephemeral\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n    \"state\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n  \"time\":\n    \"servers\":\n    - \"ntp1.hetzner.de\"\n    - \"ntp2.hetzner.com\"\n    - \"ntp3.hetzner.net\"\n"],"docs":false,"examples":false,"id":"goingdark","kubernetes_version":"v1.33.4","machine_configuration":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: controlplane\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 250m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints:\n                - effect: NoSchedule\n                  key: node-role.kubernetes.io/control-plane\n                  value: \"\"\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 250m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-control-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n              vip:\n                ip: 10.0.64.126\n                hcloud:\n                    apiToken: lkbVSBpKQf0XMxYjXcxBLSUl9DYHEoWFnquZ2r0taZTUY1llFe9raH1hQXqZ9ZfQ\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        kubernetesTalosAPIAccess:\n            enabled: true\n            allowedRoles:\n                - os:reader\n                - os:etcd:backup\n            allowedKubernetesNamespaces:\n                - kube-system\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        node.kubernetes.io/exclude-from-external-load-balancers: \"\"\n        nodepool: control\n    nodeTaints:\n        node-role.kubernetes.io/control-plane: :NoSchedule\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    secretboxEncryptionSecret: vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE=\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n    aggregatorCA:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n        key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n    serviceAccount:\n        key: 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\n    apiServer:\n        image: registry.k8s.io/kube-apiserver:v1.33.4\n        extraArgs:\n            enable-aggregator-routing: \"true\"\n        certSANs:\n            - 10.0.64.126\n            - 10.0.64.1\n            - 10.0.64.126\n            - 10.0.64.254\n            - 127.0.0.1\n            - 46.62.164.172\n            - ::1\n            - localhost\n        disablePodSecurityPolicy: true\n        admissionControl:\n            - name: PodSecurity\n              configuration:\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n                defaults:\n                    audit: restricted\n                    audit-version: latest\n                    enforce: baseline\n                    enforce-version: latest\n                    warn: restricted\n                    warn-version: latest\n                exemptions:\n                    namespaces:\n                        - kube-system\n                    runtimeClasses: []\n                    usernames: []\n                kind: PodSecurityConfiguration\n        auditPolicy:\n            apiVersion: audit.k8s.io/v1\n            kind: Policy\n            rules:\n                - level: Metadata\n    controllerManager:\n        image: registry.k8s.io/kube-controller-manager:v1.33.4\n        extraArgs:\n            bind-address: 0.0.0.0\n            cloud-provider: external\n    proxy:\n        disabled: true\n        image: registry.k8s.io/kube-proxy:v1.33.4\n    scheduler:\n        image: registry.k8s.io/kube-scheduler:v1.33.4\n        extraArgs:\n            bind-address: 0.0.0.0\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n    etcd:\n        ca:\n            crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n            key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n        extraArgs:\n            listen-metrics-urls: http://0.0.0.0:2381\n        advertisedSubnets:\n            - 10.0.64.0/25\n    coreDNS:\n        disabled: false\n    externalCloudProvider:\n        enabled: true\n        manifests:\n            - https://raw.githubusercontent.com/siderolabs/talos-cloud-controller-manager/v1.10.1/docs/deploy/cloud-controller-manager-daemonset.yml\n            - https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.85.0/stripped-down-crds.yaml\n            - https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.3.0/standard-install.yaml\n    inlineManifests:\n        - name: hcloud-secret\n          contents: |\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"network\": \"MTEzNjg4OTM=\"\n              \"token\": \"bGtiVlNCcEtRZjBYTXhZalhjeEJMU1VsOURZSEVvV0ZucXVaMnIwdGFaVFVZMWxsRmU5cmFIMWhRWHFaOVpmUQ==\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"hcloud\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n        - name: cilium\n          contents: \"null\\n...\\n\\n---\\n---\\n# Source: cilium/templates/cilium-secrets-namespace.yaml\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\n  annotations:\\n---\\n# Source: cilium/templates/cilium-agent/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-agent/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-config-agent\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - configmaps\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n---\\n# Source: cilium/templates/cilium-agent/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n---\\n# Source: cilium/templates/cilium-operator/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - create\\n  - delete\\n  - update\\n  - patch\\n---\\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-config-agent\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-config-agent\\nsubjects:\\n  - kind: ServiceAccount\\n    name: \\\"cilium\\\"\\n    namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-tlsinterception-secrets\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-operator-tlsinterception-secrets\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-envoy/service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: cilium-envoy\\n  namespace: kube-system\\n  annotations:\\n    prometheus.io/scrape: \\\"true\\\"\\n    prometheus.io/port: \\\"9964\\\"\\n  labels:\\n    k8s-app: cilium-envoy\\n    app.kubernetes.io/name: cilium-envoy\\n    app.kubernetes.io/part-of: cilium\\n    io.cilium/app: proxy\\nspec:\\n  clusterIP: None\\n  type: ClusterIP\\n  selector:\\n    k8s-app: cilium-envoy\\n  ports:\\n  - name: envoy-metrics\\n    port: 9964\\n    protocol: TCP\\n    targetPort: envoy-metrics\\n---\\n# Source: cilium/templates/cilium-agent/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: cilium\\n  namespace: kube-system\\n  labels:\\n    k8s-app: cilium\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-agent\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cilium\\n  updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n        prometheus.io/port: \\\"9962\\\"\\n        prometheus.io/scrape: \\\"true\\\"\\n        kubectl.kubernetes.io/default-container: cilium-agent\\n      labels:\\n        k8s-app: cilium\\n        app.kubernetes.io/name: cilium-agent\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n        appArmorProfile:\\n          type: Unconfined\\n        seccompProfile:\\n          type: Unconfined\\n      containers:\\n      - name: cilium-agent\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-agent\\n        args:\\n        - --config-dir=/tmp/cilium/config-map\\n        startupProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n          failureThreshold: 300\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds: 5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n            - name: \\\"require-k8s-connectivity\\\"\\n              value: \\\"false\\\"\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_CLUSTERMESH_CONFIG\\n          value: /var/lib/cilium/clustermesh/\\n        - name: GOMEMLIMIT\\n          valueFrom:\\n            resourceFieldRef:\\n              resource: limits.memory\\n              divisor: '1'\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        - name: KUBE_CLIENT_BACKOFF_BASE\\n          value: \\\"1\\\"\\n        - name: KUBE_CLIENT_BACKOFF_DURATION\\n          value: \\\"120\\\"\\n        lifecycle:\\n          postStart:\\n            exec:\\n              command:\\n              - \\\"bash\\\"\\n              - \\\"-c\\\"\\n              - |\\n                    set -o errexit\\n                    set -o pipefail\\n                    set -o nounset\\n                    \\n                    # When running in AWS ENI mode, it's likely that 'aws-node' has\\n                    # had a chance to install SNAT iptables rules. These can result\\n                    # in dropped traffic, so we should attempt to remove them.\\n                    # We do it using a 'postStart' hook since this may need to run\\n                    # for nodes which might have already been init'ed but may still\\n                    # have dangling rules. This is safe because there are no\\n                    # dependencies on anything that is part of the startup script\\n                    # itself, and can be safely run multiple times per node (e.g. in\\n                    # case of a restart).\\n                    if [[ \\\"$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n                    then\\n                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'\\n                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore\\n                    fi\\n                    echo 'Done!'\\n                    \\n          preStop:\\n            exec:\\n              command:\\n              - /cni-uninstall.sh\\n        ports:\\n        - name: peer-service\\n          containerPort: 4244\\n          hostPort: 4244\\n          protocol: TCP\\n        - name: prometheus\\n          containerPort: 9962\\n          hostPort: 9962\\n          protocol: TCP\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - CHOWN\\n              - KILL\\n              - NET_ADMIN\\n              - NET_RAW\\n              - IPC_LOCK\\n              - SYS_ADMIN\\n              - SYS_RESOURCE\\n              - DAC_OVERRIDE\\n              - FOWNER\\n              - SETGID\\n              - SETUID\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n          mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n        # Unprivileged containers need to mount /proc/sys/net from the host\\n        # to have write access\\n        - mountPath: /host/proc/sys/net\\n          name: host-proc-sys-net\\n        # Unprivileged containers need to mount /proc/sys/kernel from the host\\n        # to have write access\\n        - mountPath: /host/proc/sys/kernel\\n          name: host-proc-sys-kernel\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          # Unprivileged containers can't set mount propagation to bidirectional\\n          # in this case we will mount the bpf fs from an init container that\\n          # is privileged and set the mount propagation from host to container\\n          # in Cilium.\\n          mountPropagation: HostToContainer\\n        # Check for duplicate mounts before mounting\\n        - name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n        - name: cilium-run\\n          mountPath: /var/run/cilium\\n        - name: cilium-netns\\n          mountPath: /var/run/cilium/netns\\n          mountPropagation: HostToContainer\\n        - name: etc-cni-netd\\n          mountPath: /host/etc/cni/net.d\\n        - name: clustermesh-secrets\\n          mountPath: /var/lib/cilium/clustermesh\\n          readOnly: true\\n          # Needed to be able to load kernel modules\\n        - name: lib-modules\\n          mountPath: /lib/modules\\n          readOnly: true\\n        - name: xtables-lock\\n          mountPath: /run/xtables.lock\\n        - name: tmp\\n          mountPath: /tmp\\n        \\n      initContainers:\\n      - name: config\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-dbg\\n        - build-config\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        volumeMounts:\\n        - name: tmp\\n          mountPath: /tmp\\n        terminationMessagePolicy: FallbackToLogsOnError\\n      - name: apply-sysctl-overwrites\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        env:\\n        - name: BIN_PATH\\n          value: /opt/cni/bin\\n        command:\\n        - sh\\n        - -ec\\n        # The statically linked Go program binary is invoked to avoid any\\n        # dependency on utilities like sh that can be missing on certain\\n        # distros installed on the underlying host. Copy the binary to the\\n        # same directory where we install cilium cni plugin so that exec permissions\\n        # are available.\\n        - |\\n          cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\\n          nsenter --mount=/hostproc/1/ns/mnt \\\"${BIN_PATH}/cilium-sysctlfix\\\";\\n          rm /hostbin/cilium-sysctlfix\\n        volumeMounts:\\n        - name: hostproc\\n          mountPath: /hostproc\\n        - name: cni-path\\n          mountPath: /hostbin\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - SYS_ADMIN\\n              - SYS_CHROOT\\n              - SYS_PTRACE\\n            drop:\\n              - ALL\\n      # Mount the bpf fs if it is not mounted. We will perform this task\\n      # from a privileged container because the mount propagation bidirectional\\n      # only works from privileged containers.\\n      - name: mount-bpf-fs\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        args:\\n        - 'mount | grep \\\"/sys/fs/bpf type bpf\\\" || mount -t bpf bpf /sys/fs/bpf'\\n        command:\\n        - /bin/bash\\n        - -c\\n        - --\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          privileged: true\\n        volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          mountPropagation: Bidirectional\\n      - name: clean-cilium-state\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - /init-container.sh\\n        env:\\n        - name: CILIUM_ALL_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: clean-cilium-state\\n              optional: true\\n        - name: CILIUM_BPF_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: clean-cilium-bpf-state\\n              optional: true\\n        - name: WRITE_CNI_CONF_WHEN_READY\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: write-cni-conf-when-ready\\n              optional: true\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - NET_ADMIN\\n              - SYS_ADMIN\\n              - SYS_RESOURCE\\n            drop:\\n              - ALL\\n        volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          # Required to mount cgroup filesystem from the host to cilium agent pod\\n        - name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n          mountPropagation: HostToContainer\\n        - name: cilium-run\\n          mountPath: /var/run/cilium # wait-for-kube-proxy\\n      # Install the CNI binaries in an InitContainer so we don't have a writable host mount in the agent\\n      - name: install-cni-binaries\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n          - \\\"/install-plugin.sh\\\"\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 10Mi\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n          - name: cni-path\\n            mountPath: /host/opt/cni/bin # .Values.cni.install\\n      restartPolicy: Always\\n      priorityClassName: system-node-critical\\n      serviceAccountName: \\\"cilium\\\"\\n      automountServiceAccountToken: true\\n      terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      tolerations:\\n        - operator: Exists\\n      volumes:\\n        # For sharing configuration between the \\\"config\\\" initContainer and the agent\\n      - name: tmp\\n        emptyDir: {}\\n        # To keep state between restarts / upgrades\\n      - name: cilium-run\\n        hostPath:\\n          path: /var/run/cilium\\n          type: DirectoryOrCreate\\n        # To exec into pod network namespaces\\n      - name: cilium-netns\\n        hostPath:\\n          path: /var/run/netns\\n          type: DirectoryOrCreate\\n        # To keep state between restarts / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n          path: /sys/fs/bpf\\n          type: DirectoryOrCreate\\n      # To mount cgroup2 filesystem on the host or apply sysctlfix\\n      - name: hostproc\\n        hostPath:\\n          path: /proc\\n          type: Directory\\n      # To keep state between restarts / upgrades for cgroup2 filesystem\\n      - name: cilium-cgroup\\n        hostPath:\\n          path: /sys/fs/cgroup\\n          type: DirectoryOrCreate\\n      # To install cilium cni plugin in the host\\n      - name: cni-path\\n        hostPath:\\n          path:  /opt/cni/bin\\n          type: DirectoryOrCreate\\n        # To install cilium cni configuration in the host\\n      - name: etc-cni-netd\\n        hostPath:\\n          path: /etc/cni/net.d\\n          type: DirectoryOrCreate\\n        # To be able to load kernel modules\\n      - name: lib-modules\\n        hostPath:\\n          path: /lib/modules\\n        # To access iptables concurrently with other processes (e.g. kube-proxy)\\n      - name: xtables-lock\\n        hostPath:\\n          path: /run/xtables.lock\\n          type: FileOrCreate\\n      # Sharing socket with Cilium Envoy on the same node by using a host path\\n      - name: envoy-sockets\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/sockets\\\"\\n          type: DirectoryOrCreate\\n        # To read the clustermesh configuration\\n      - name: clustermesh-secrets\\n        projected:\\n          # note: the leading zero means this number is in octal representation: do not remove it\\n          defaultMode: 0400\\n          sources:\\n          - secret:\\n              name: cilium-clustermesh\\n              optional: true\\n              # note: items are not explicitly listed here, since the entries of this secret\\n              # depend on the peers configured, and that would cause a restart of all agents\\n              # at every addition/removal. Leaving the field empty makes each secret entry\\n              # to be automatically projected into the volume as a file whose name is the key.\\n          - secret:\\n              name: clustermesh-apiserver-remote-cert\\n              optional: true\\n              items:\\n              - key: tls.key\\n                path: common-etcd-client.key\\n              - key: tls.crt\\n                path: common-etcd-client.crt\\n              - key: ca.crt\\n                path: common-etcd-client-ca.crt\\n          # note: we configure the volume for the kvstoremesh-specific certificate\\n          # regardless of whether KVStoreMesh is enabled or not, so that it can be\\n          # automatically mounted in case KVStoreMesh gets subsequently enabled,\\n          # without requiring an agent restart.\\n          - secret:\\n              name: clustermesh-apiserver-local-cert\\n              optional: true\\n              items:\\n              - key: tls.key\\n                path: local-etcd-client.key\\n              - key: tls.crt\\n                path: local-etcd-client.crt\\n              - key: ca.crt\\n                path: local-etcd-client-ca.crt\\n      - name: host-proc-sys-net\\n        hostPath:\\n          path: /proc/sys/net\\n          type: Directory\\n      - name: host-proc-sys-kernel\\n        hostPath:\\n          path: /proc/sys/kernel\\n          type: Directory\\n---\\n# Source: cilium/templates/cilium-envoy/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: cilium-envoy\\n  namespace: kube-system\\n  labels:\\n    k8s-app: cilium-envoy\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-envoy\\n    name: cilium-envoy\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cilium-envoy\\n  updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n      labels:\\n        k8s-app: cilium-envoy\\n        name: cilium-envoy\\n        app.kubernetes.io/name: cilium-envoy\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n        appArmorProfile:\\n          type: Unconfined\\n      containers:\\n      - name: cilium-envoy\\n        image: \\\"quay.io/cilium/cilium-envoy:v1.34.4-1754895458-68cffdfa568b6b226d70a7ef81fc65dda3b890bf@sha256:247e908700012f7ef56f75908f8c965215c26a27762f296068645eb55450bda2\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - /usr/bin/cilium-envoy-starter\\n        args:\\n        - '--'\\n        - '-c /var/run/cilium/envoy/bootstrap-config.json'\\n        - '--base-id 0'\\n        - '--log-level info'\\n        startupProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          failureThreshold: 105\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds: 5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        ports:\\n        - name: envoy-metrics\\n          containerPort: 9964\\n          hostPort: 9964\\n          protocol: TCP\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - NET_ADMIN\\n              - SYS_ADMIN\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n          mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n        - name: envoy-artifacts\\n          mountPath: /var/run/cilium/envoy/artifacts\\n          readOnly: true\\n        - name: envoy-config\\n          mountPath: /var/run/cilium/envoy/\\n          readOnly: true\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          mountPropagation: HostToContainer\\n      restartPolicy: Always\\n      priorityClassName: system-node-critical\\n      serviceAccountName: \\\"cilium-envoy\\\"\\n      automountServiceAccountToken: true\\n      terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n        nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            nodeSelectorTerms:\\n            - matchExpressions:\\n              - key: cilium.io/no-schedule\\n                operator: NotIn\\n                values:\\n                - \\\"true\\\"\\n        podAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium\\n            topologyKey: kubernetes.io/hostname\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium-envoy\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      tolerations:\\n        - operator: Exists\\n      volumes:\\n      - name: envoy-sockets\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/sockets\\\"\\n          type: DirectoryOrCreate\\n      - name: envoy-artifacts\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/artifacts\\\"\\n          type: DirectoryOrCreate\\n      - name: envoy-config\\n        configMap:\\n          name: \\\"cilium-envoy-config\\\"\\n          # note: the leading zero means this number is in octal representation: do not remove it\\n          defaultMode: 0400\\n          items:\\n            - key: bootstrap-config.json\\n              path: bootstrap-config.json\\n        # To keep state between restarts / upgrades\\n        # To keep state between restarts / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n          path: /sys/fs/bpf\\n          type: DirectoryOrCreate\\n---\\n# Source: cilium/templates/cilium-operator/deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: cilium-operator\\n  namespace: kube-system\\n  labels:\\n    io.cilium/app: operator\\n    name: cilium-operator\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-operator\\nspec:\\n  # See docs on ServerCapabilities.LeasesResourceLock in file pkg/k8s/version/version.go\\n  # for more details.\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      io.cilium/app: operator\\n      name: cilium-operator\\n  # ensure operator update on single node k8s clusters, by using rolling update with maxUnavailable=100% in case\\n  # of one replica and no user configured Recreate strategy.\\n  # otherwise an update might get stuck due to the default maxUnavailable=50% in combination with the\\n  # podAntiAffinity which prevents deployments of multiple operator replicas on the same node.\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 25%\\n      maxUnavailable: 100%\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n        prometheus.io/port: \\\"9963\\\"\\n        prometheus.io/scrape: \\\"true\\\"\\n      labels:\\n        io.cilium/app: operator\\n        name: cilium-operator\\n        app.kubernetes.io/part-of: cilium\\n        app.kubernetes.io/name: cilium-operator\\n    spec:\\n      securityContext:\\n        seccompProfile:\\n          type: RuntimeDefault\\n      containers:\\n      - name: cilium-operator\\n        image: \\\"quay.io/cilium/operator-generic:v1.18.1@sha256:97f4553afa443465bdfbc1cc4927c93f16ac5d78e4dd2706736e7395382201bc\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-operator-generic\\n        args:\\n        - --config-dir=/tmp/cilium/config-map\\n        - --debug=$(CILIUM_DEBUG)\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_DEBUG\\n          valueFrom:\\n            configMapKeyRef:\\n              key: debug\\n              name: cilium-config\\n              optional: true\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        ports:\\n        - name: prometheus\\n          containerPort: 9963\\n          hostPort: 9963\\n          protocol: TCP\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds: 60\\n          periodSeconds: 10\\n          timeoutSeconds: 3\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds: 0\\n          periodSeconds: 5\\n          timeoutSeconds: 3\\n          failureThreshold: 5\\n        volumeMounts:\\n        - name: cilium-config-path\\n          mountPath: /tmp/cilium/config-map\\n          readOnly: true\\n        securityContext:\\n          allowPrivilegeEscalation: false\\n          capabilities:\\n            drop:\\n            - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n      hostNetwork: true\\n      restartPolicy: Always\\n      priorityClassName: system-cluster-critical\\n      serviceAccountName: \\\"cilium-operator\\\"\\n      automountServiceAccountToken: true\\n      # In HA mode, cilium-operator pods must not be scheduled on the same\\n      # node as they will clash with each other.\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                io.cilium/app: operator\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      tolerations:\\n        - operator: Exists\\n        - key: node.cilium.io/agent-not-ready\\n          operator: Exists\\n      \\n      volumes:\\n        # To read the configuration from the config map\\n      - name: cilium-config-path\\n        configMap:\\n          name: cilium-config\\n---\\n# Source: cilium/templates/cilium-envoy/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-envoy\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-configmap.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-config\\n  namespace: kube-system\\ndata:\\n\\n  # Identity allocation mode selects how identities are shared between cilium\\n  # nodes by setting how they are stored. The options are \\\"crd\\\", \\\"kvstore\\\" or\\n  # \\\"doublewrite-readkvstore\\\" / \\\"doublewrite-readcrd\\\".\\n  # - \\\"crd\\\" stores identities in kubernetes as CRDs (custom resource definition).\\n  #   These can be queried with:\\n  #     kubectl get ciliumid\\n  # - \\\"kvstore\\\" stores identities in an etcd kvstore, that is\\n  #   configured below. Cilium versions before 1.6 supported only the kvstore\\n  #   backend. Upgrades from these older cilium versions should continue using\\n  #   the kvstore by commenting out the identity-allocation-mode below, or\\n  #   setting it to \\\"kvstore\\\".\\n  # - \\\"doublewrite\\\" modes store identities in both the kvstore and CRDs. This is useful\\n  #   for seamless migrations from the kvstore mode to the crd mode. Consult the\\n  #   documentation for more information on how to perform the migration.\\n  identity-allocation-mode: crd\\n\\n  identity-heartbeat-timeout: \\\"30m0s\\\"\\n  identity-gc-interval: \\\"15m0s\\\"\\n  cilium-endpoint-gc-interval: \\\"5m0s\\\"\\n  nodes-gc-interval: \\\"5m0s\\\"\\n\\n  # If you want to run cilium in debug mode change this value to true\\n  debug: \\\"false\\\"\\n  debug-verbose: \\\"\\\"\\n  metrics-sampling-interval: \\\"5m\\\"\\n  # The agent can be put into the following three policy enforcement modes\\n  # default, always and never.\\n  # https://docs.cilium.io/en/latest/security/policy/intro/#policy-enforcement-modes\\n  enable-policy: \\\"default\\\"\\n  policy-cidr-match-mode: \\\"\\\"\\n  # If you want metrics enabled in all of your Cilium agents, set the port for\\n  # which the Cilium agents will have their metrics exposed.\\n  # This option deprecates the \\\"prometheus-serve-addr\\\" in the\\n  # \\\"cilium-metrics-config\\\" ConfigMap\\n  # NOTE that this will open the port on ALL nodes where Cilium pods are\\n  # scheduled.\\n  prometheus-serve-addr: \\\":9962\\\"\\n  # A space-separated list of controller groups for which to enable metrics.\\n  # The special values of \\\"all\\\" and \\\"none\\\" are supported.\\n  controller-group-metrics:\\n    write-cni-file\\n    sync-host-ips\\n    sync-lb-maps-with-k8s-services\\n  # If you want metrics enabled in cilium-operator, set the port for\\n  # which the Cilium Operator will have their metrics exposed.\\n  # NOTE that this will open the port on the nodes where Cilium operator pod\\n  # is scheduled.\\n  operator-prometheus-serve-addr: \\\":9963\\\"\\n  enable-metrics: \\\"true\\\"\\n  enable-policy-secrets-sync: \\\"true\\\"\\n  policy-secrets-only-from-secrets-namespace: \\\"true\\\"\\n  policy-secrets-namespace: \\\"cilium-secrets\\\"\\n\\n  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4\\n  # address.\\n  enable-ipv4: \\\"true\\\"\\n\\n  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6\\n  # address.\\n  enable-ipv6: \\\"false\\\"\\n  # Users who wish to specify their own custom CNI configuration file must set\\n  # custom-cni-conf to \\\"true\\\", otherwise Cilium may overwrite the configuration.\\n  custom-cni-conf: \\\"false\\\"\\n  enable-bpf-clock-probe: \\\"false\\\"\\n  # If you want cilium monitor to aggregate tracing for packets, set this level\\n  # to \\\"low\\\", \\\"medium\\\", or \\\"maximum\\\". The higher the level, the less packets\\n  # that will be seen in monitor output.\\n  monitor-aggregation: medium\\n\\n  # The monitor aggregation interval governs the typical time between monitor\\n  # notification events for each allowed connection.\\n  #\\n  # Only effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n  monitor-aggregation-interval: \\\"5s\\\"\\n\\n  # The monitor aggregation flags determine which TCP flags which, upon the\\n  # first observation, cause monitor notifications to be generated.\\n  #\\n  # Only effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n  monitor-aggregation-flags: all\\n  # Specifies the ratio (0.0-1.0] of total system memory to use for dynamic\\n  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.\\n  bpf-map-dynamic-size-ratio: \\\"0.0025\\\"\\n  enable-host-legacy-routing: \\\"false\\\"\\n  # bpf-policy-map-max specifies the maximum number of entries in endpoint\\n  # policy map (per endpoint)\\n  bpf-policy-map-max: \\\"16384\\\"\\n  # bpf-policy-stats-map-max specifies the maximum number of entries in global\\n  # policy stats map\\n  bpf-policy-stats-map-max: \\\"65536\\\"\\n  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,\\n  # backend and affinity maps.\\n  bpf-lb-map-max: \\\"65536\\\"\\n  bpf-lb-external-clusterip: \\\"false\\\"\\n  bpf-lb-source-range-all-types: \\\"false\\\"\\n  bpf-lb-algorithm-annotation: \\\"false\\\"\\n  bpf-lb-mode-annotation: \\\"false\\\"\\n\\n  bpf-distributed-lru: \\\"false\\\"\\n  bpf-events-drop-enabled: \\\"true\\\"\\n  bpf-events-policy-verdict-enabled: \\\"true\\\"\\n  bpf-events-trace-enabled: \\\"true\\\"\\n\\n  # Pre-allocation of map entries allows per-packet latency to be reduced, at\\n  # the expense of up-front memory allocation for the entries in the maps. The\\n  # default value below will minimize memory usage in the default installation;\\n  # users who are sensitive to latency may consider setting this to \\\"true\\\".\\n  #\\n  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore\\n  # this option and behave as though it is set to \\\"true\\\".\\n  #\\n  # If this value is modified, then during the next Cilium startup the restore\\n  # of existing endpoints and tracking of ongoing connections may be disrupted.\\n  # As a result, reply packets may be dropped and the load-balancing decisions\\n  # for established connections may change.\\n  #\\n  # If this option is set to \\\"false\\\" during an upgrade from 1.3 or earlier to\\n  # 1.4 or later, then it may cause one-time disruptions during the upgrade.\\n  preallocate-bpf-maps: \\\"false\\\"\\n\\n  # Name of the cluster. Only relevant when building a mesh of clusters.\\n  cluster-name: \\\"default\\\"\\n  # Unique ID of the cluster. Must be unique across all conneted clusters and\\n  # in the range of 1 and 255. Only relevant when building a mesh of clusters.\\n  cluster-id: \\\"0\\\"\\n\\n  # Encapsulation mode for communication between nodes\\n  # Possible values:\\n  #   - disabled\\n  #   - vxlan (default)\\n  #   - geneve\\n\\n  routing-mode: \\\"native\\\"\\n  tunnel-protocol: \\\"vxlan\\\"\\n  tunnel-source-port-range: \\\"0-0\\\"\\n  service-no-backend-response: \\\"reject\\\"\\n\\n\\n  # Enables L7 proxy for L7 policy enforcement and visibility\\n  enable-l7-proxy: \\\"true\\\"\\n  enable-ipv4-masquerade: \\\"true\\\"\\n  enable-ipv4-big-tcp: \\\"false\\\"\\n  enable-ipv6-big-tcp: \\\"false\\\"\\n  enable-ipv6-masquerade: \\\"true\\\"\\n  enable-tcx: \\\"true\\\"\\n  datapath-mode: \\\"veth\\\"\\n  enable-bpf-masquerade: \\\"true\\\"\\n  enable-masquerade-to-route-source: \\\"false\\\"\\n  enable-wireguard: \\\"true\\\"\\n  wireguard-persistent-keepalive: \\\"0s\\\"\\n\\n  enable-xt-socket-fallback: \\\"true\\\"\\n  install-no-conntrack-iptables-rules: \\\"true\\\"\\n  iptables-random-fully: \\\"false\\\"\\n\\n  auto-direct-node-routes: \\\"false\\\"\\n  direct-routing-skip-unreachable: \\\"false\\\"\\n\\n\\n  ipv4-native-routing-cidr: 10.0.0.0/16\\n\\n  kube-proxy-replacement: \\\"true\\\"\\n  kube-proxy-replacement-healthz-bind-address: \\\"0.0.0.0:10256\\\"\\n  bpf-lb-sock: \\\"false\\\"\\n  nodeport-addresses: \\\"\\\"\\n  enable-health-check-nodeport: \\\"true\\\"\\n  enable-health-check-loadbalancer-ip: \\\"false\\\"\\n  node-port-bind-protection: \\\"true\\\"\\n  enable-auto-protect-node-port-range: \\\"true\\\"\\n  bpf-lb-acceleration: \\\"native\\\"\\n  enable-svc-source-range-check: \\\"true\\\"\\n  enable-l2-neigh-discovery: \\\"false\\\"\\n  k8s-require-ipv4-pod-cidr: \\\"true\\\"\\n  k8s-require-ipv6-pod-cidr: \\\"false\\\"\\n  enable-k8s-networkpolicy: \\\"true\\\"\\n  enable-endpoint-lockdown-on-policy-overflow: \\\"false\\\"\\n  # Tell the agent to generate and write a CNI configuration file\\n  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist\\n  cni-exclusive: \\\"true\\\"\\n  cni-log-file: \\\"/var/run/cilium/cilium-cni.log\\\"\\n  enable-endpoint-health-checking: \\\"true\\\"\\n  enable-health-checking: \\\"true\\\"\\n  health-check-icmp-failure-threshold: \\\"3\\\"\\n  enable-well-known-identities: \\\"false\\\"\\n  enable-node-selector-labels: \\\"false\\\"\\n  synchronize-k8s-nodes: \\\"true\\\"\\n  operator-api-serve-addr: \\\"127.0.0.1:9234\\\"\\n\\n  enable-hubble: \\\"false\\\"\\n  ipam: \\\"kubernetes\\\"\\n  ipam-cilium-node-update-rate: \\\"15s\\\"\\n\\n  default-lb-service-ipam: \\\"lbipam\\\"\\n  egress-gateway-reconciliation-trigger-interval: \\\"1s\\\"\\n  enable-vtep: \\\"false\\\"\\n  vtep-endpoint: \\\"\\\"\\n  vtep-cidr: \\\"\\\"\\n  vtep-mask: \\\"\\\"\\n  vtep-mac: \\\"\\\"\\n  procfs: \\\"/host/proc\\\"\\n  bpf-root: \\\"/sys/fs/bpf\\\"\\n  cgroup-root: \\\"/sys/fs/cgroup\\\"\\n\\n  identity-management-mode: \\\"agent\\\"\\n  enable-sctp: \\\"false\\\"\\n  remove-cilium-node-taints: \\\"true\\\"\\n  set-cilium-node-taints: \\\"true\\\"\\n  set-cilium-is-up-condition: \\\"true\\\"\\n  unmanaged-pod-watcher-interval: \\\"15\\\"\\n  # explicit setting gets precedence\\n  dnsproxy-enable-transparent-mode: \\\"true\\\"\\n  dnsproxy-socket-linger-timeout: \\\"10\\\"\\n  tofqdns-dns-reject-response-code: \\\"refused\\\"\\n  tofqdns-enable-dns-compression: \\\"true\\\"\\n  tofqdns-endpoint-max-ip-per-hostname: \\\"1000\\\"\\n  tofqdns-idle-connection-grace-period: \\\"0s\\\"\\n  tofqdns-max-deferred-connection-deletes: \\\"10000\\\"\\n  tofqdns-proxy-response-max-delay: \\\"100ms\\\"\\n  tofqdns-preallocate-identities:  \\\"true\\\"\\n  agent-not-ready-taint-key: \\\"node.cilium.io/agent-not-ready\\\"\\n\\n  mesh-auth-enabled: \\\"true\\\"\\n  mesh-auth-queue-size: \\\"1024\\\"\\n  mesh-auth-rotated-identities-queue-size: \\\"1024\\\"\\n  mesh-auth-gc-interval: \\\"5m0s\\\"\\n\\n  proxy-xff-num-trusted-hops-ingress: \\\"0\\\"\\n  proxy-xff-num-trusted-hops-egress: \\\"0\\\"\\n  proxy-connect-timeout: \\\"2\\\"\\n  proxy-initial-fetch-timeout: \\\"30\\\"\\n  proxy-max-requests-per-connection: \\\"0\\\"\\n  proxy-max-connection-duration-seconds: \\\"0\\\"\\n  proxy-idle-timeout-seconds: \\\"60\\\"\\n  proxy-max-concurrent-retries: \\\"128\\\"\\n  http-retry-count: \\\"3\\\"\\n\\n  external-envoy-proxy: \\\"true\\\"\\n  envoy-base-id: \\\"0\\\"\\n  envoy-access-log-buffer-size: \\\"4096\\\"\\n  envoy-keep-cap-netbindservice: \\\"false\\\"\\n  max-connected-clusters: \\\"255\\\"\\n  clustermesh-enable-endpoint-sync: \\\"false\\\"\\n  clustermesh-enable-mcs-api: \\\"false\\\"\\n  policy-default-local-cluster: \\\"false\\\"\\n\\n  nat-map-stats-entries: \\\"32\\\"\\n  nat-map-stats-interval: \\\"30s\\\"\\n  enable-internal-traffic-policy: \\\"true\\\"\\n  enable-lb-ipam: \\\"true\\\"\\n  enable-non-default-deny-policies: \\\"true\\\"\\n  enable-source-ip-verification: \\\"true\\\"\\n\\n# Extra config allows adding arbitrary properties to the cilium config.\\n# By putting it at the end of the ConfigMap, it's also possible to override existing properties.\\n---\\n# Source: cilium/templates/cilium-envoy/configmap.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-envoy-config\\n  namespace: kube-system\\ndata:\\n  # Keep the key name as bootstrap-config.json to avoid breaking changes\\n  bootstrap-config.json: |\\n    {\\\"admin\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}},\\\"applicationLogConfig\\\":{\\\"logFormat\\\":{\\\"textFormat\\\":\\\"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v\\\"}},\\\"bootstrapExtensions\\\":[{\\\"name\\\":\\\"envoy.bootstrap.internal_listener\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener\\\"}}],\\\"dynamicResources\\\":{\\\"cdsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"},\\\"ldsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"}},\\\"node\\\":{\\\"cluster\\\":\\\"ingress-cluster\\\",\\\"id\\\":\\\"host~127.0.0.1~no-id~localdomain\\\"},\\\"overloadManager\\\":{\\\"resourceMonitors\\\":[{\\\"name\\\":\\\"envoy.resource_monitors.global_downstream_max_connections\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig\\\",\\\"max_active_downstream_connections\\\":\\\"50000\\\"}}]},\\\"staticResources\\\":{\\\"clusters\\\":[{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/xds.sock\\\"}}}}]}]},\\\"name\\\":\\\"xds-grpc-cilium\\\",\\\"type\\\":\\\"STATIC\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"explicitHttpConfig\\\":{\\\"http2ProtocolOptions\\\":{}}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"/envoy-admin\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}}}]}]},\\\"name\\\":\\\"/envoy-admin\\\",\\\"type\\\":\\\"STATIC\\\"}],\\\"listeners\\\":[{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"0.0.0.0\\\",\\\"portValue\\\":9964}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtualHosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/metrics\\\"},\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/stats/prometheus\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-prometheus-metrics-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-prometheus-metrics-listener\\\"},{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"127.0.0.1\\\",\\\"portValue\\\":9878}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtual_hosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"health\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/healthz\\\"},\\\"name\\\":\\\"health\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/ready\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-health-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-health-listener\\\"}]}}\\n---\\n# Source: cilium/templates/cilium-agent/clusterrole.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cilium\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - networking.k8s.io\\n  resources:\\n  - networkpolicies\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - discovery.k8s.io\\n  resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - namespaces\\n  - services\\n  - pods\\n  - endpoints\\n  - nodes\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - list\\n  - watch\\n  # This is used when validating policies in preflight. This will need to stay\\n  # until we figure out how to avoid \\\"get\\\" inside the preflight, and then\\n  # should be removed ideally.\\n  - get\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools\\n  - ciliumbgppeeringpolicies\\n  - ciliumbgpnodeconfigs\\n  - ciliumbgpadvertisements\\n  - ciliumbgppeerconfigs\\n  - ciliumclusterwideenvoyconfigs\\n  - ciliumclusterwidenetworkpolicies\\n  - ciliumegressgatewaypolicies\\n  - ciliumendpoints\\n  - ciliumendpointslices\\n  - ciliumenvoyconfigs\\n  - ciliumidentities\\n  - ciliumlocalredirectpolicies\\n  - ciliumnetworkpolicies\\n  - ciliumnodes\\n  - ciliumnodeconfigs\\n  - ciliumcidrgroups\\n  - ciliuml2announcementpolicies\\n  - ciliumpodippools\\n  verbs:\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumidentities\\n  - ciliumendpoints\\n  - ciliumnodes\\n  verbs:\\n  - create\\n- apiGroups:\\n  - cilium.io\\n  # To synchronize garbage collection of such resources\\n  resources:\\n  - ciliumidentities\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints\\n  verbs:\\n  - delete\\n  - get\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes\\n  - ciliumnodes/status\\n  verbs:\\n  - get\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints/status\\n  - ciliumendpoints\\n  - ciliuml2announcementpolicies/status\\n  - ciliumbgpnodeconfigs/status\\n  verbs:\\n  - patch\\n---\\n# Source: cilium/templates/cilium-operator/clusterrole.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cilium-operator\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - pods\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n  # to automatically delete [core|kube]dns pods so that are starting to being\\n  # managed by Cilium\\n  - delete\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - configmaps\\n  resourceNames:\\n  - cilium-config\\n  verbs:\\n   # allow patching of the configmap to set annotations\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - nodes\\n  verbs:\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # To remove node taints\\n  - nodes\\n  # To set NetworkUnavailable false on startup\\n  - nodes/status\\n  verbs:\\n  - patch\\n- apiGroups:\\n  - discovery.k8s.io\\n  resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to perform LB IP allocation for BGP\\n  - services/status\\n  verbs:\\n  - update\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to check apiserver connectivity\\n  - namespaces\\n  - secrets\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to perform the translation of a CNP that contains `ToGroup` to its endpoints\\n  - services\\n  - endpoints\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnetworkpolicies\\n  - ciliumclusterwidenetworkpolicies\\n  verbs:\\n  # Create auto-generated CNPs and CCNPs from Policies that have 'toGroups'\\n  - create\\n  - update\\n  - deletecollection\\n  # To update the status of the CNPs and CCNPs\\n  - patch\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnetworkpolicies/status\\n  - ciliumclusterwidenetworkpolicies/status\\n  verbs:\\n  # Update the auto-generated CNPs and CCNPs status.\\n  - patch\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints\\n  - ciliumidentities\\n  verbs:\\n  # To perform garbage collection of such resources\\n  - delete\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumidentities\\n  verbs:\\n  # To synchronize garbage collection of such resources\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes\\n  verbs:\\n  - create\\n  - update\\n  - get\\n  - list\\n  - watch\\n    # To perform CiliumNode garbage collector\\n  - delete\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpointslices\\n  - ciliumenvoyconfigs\\n  - ciliumbgppeerconfigs\\n  - ciliumbgpadvertisements\\n  - ciliumbgpnodeconfigs\\n  verbs:\\n  - create\\n  - update\\n  - get\\n  - list\\n  - watch\\n  - delete\\n  - patch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumbgpclusterconfigs/status\\n  - ciliumbgppeerconfigs/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - create\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - update\\n  resourceNames:\\n  - ciliumloadbalancerippools.cilium.io\\n  - ciliumbgppeeringpolicies.cilium.io\\n  - ciliumbgpclusterconfigs.cilium.io\\n  - ciliumbgppeerconfigs.cilium.io\\n  - ciliumbgpadvertisements.cilium.io\\n  - ciliumbgpnodeconfigs.cilium.io\\n  - ciliumbgpnodeconfigoverrides.cilium.io\\n  - ciliumclusterwideenvoyconfigs.cilium.io\\n  - ciliumclusterwidenetworkpolicies.cilium.io\\n  - ciliumegressgatewaypolicies.cilium.io\\n  - ciliumendpoints.cilium.io\\n  - ciliumendpointslices.cilium.io\\n  - ciliumenvoyconfigs.cilium.io\\n  - ciliumidentities.cilium.io\\n  - ciliumlocalredirectpolicies.cilium.io\\n  - ciliumnetworkpolicies.cilium.io\\n  - ciliumnodes.cilium.io\\n  - ciliumnodeconfigs.cilium.io\\n  - ciliumcidrgroups.cilium.io\\n  - ciliuml2announcementpolicies.cilium.io\\n  - ciliumpodippools.cilium.io\\n  - ciliumgatewayclassconfigs.cilium.io\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools\\n  - ciliumpodippools\\n  - ciliumbgppeeringpolicies\\n  - ciliumbgpclusterconfigs\\n  - ciliumbgpnodeconfigoverrides\\n  - ciliumbgppeerconfigs\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n    - cilium.io\\n  resources:\\n    - ciliumpodippools\\n  verbs:\\n    - create\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools/status\\n  verbs:\\n  - patch\\n# For cilium-operator running in HA mode.\\n#\\n# Cilium operator running in HA mode requires the use of ResourceLock for Leader Election\\n# between multiple running instances.\\n# The preferred way of doing this is to use LeasesResourceLock as edits to Leases are less\\n# common and fewer objects in the cluster watch \\\"all Leases\\\".\\n- apiGroups:\\n  - coordination.k8s.io\\n  resources:\\n  - leases\\n  verbs:\\n  - create\\n  - get\\n  - update\\n---\\n# Source: cilium/templates/cilium-agent/clusterrolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cilium\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/clusterrolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium-operator\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cilium-operator\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n\\n\"\n        - name: hcloud-ccm\n          contents: \"---\\n# Source: hcloud-cloud-controller-manager/templates/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n  namespace: kube-system\\n---\\n# Source: hcloud-cloud-controller-manager/templates/clusterrolebinding.yaml\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: \\\"system:hcloud-cloud-controller-manager\\\"\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cluster-admin\\nsubjects:\\n  - kind: ServiceAccount\\n    name: hcloud-cloud-controller-manager\\n    namespace: kube-system\\n---\\n# Source: hcloud-cloud-controller-manager/templates/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n  namespace: kube-system\\nspec:\\n  revisionHistoryLimit: 2\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n      app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n        app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\\n    spec:\\n      serviceAccountName: hcloud-cloud-controller-manager\\n      dnsPolicy: Default\\n      tolerations:\\n        # Allow HCCM itself to schedule on nodes that have not yet been initialized by HCCM.\\n        - key: \\\"node.cloudprovider.kubernetes.io/uninitialized\\\"\\n          value: \\\"true\\\"\\n          effect: \\\"NoSchedule\\\"\\n        - key: \\\"CriticalAddonsOnly\\\"\\n          operator: \\\"Exists\\\"\\n\\n        # Allow HCCM to schedule on control plane nodes.\\n        - key: \\\"node-role.kubernetes.io/master\\\"\\n          effect: NoSchedule\\n          operator: Exists\\n        - key: \\\"node-role.kubernetes.io/control-plane\\\"\\n          effect: NoSchedule\\n          operator: Exists\\n\\n        - key: \\\"node.kubernetes.io/not-ready\\\"\\n          effect: \\\"NoExecute\\\"\\n      nodeSelector:\\n        \\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      hostNetwork: true\\n      containers:\\n        - name: hcloud-cloud-controller-manager\\n          command:\\n            - \\\"/bin/hcloud-cloud-controller-manager\\\"\\n            - \\\"--allow-untagged-cloud\\\"\\n            - \\\"--cloud-provider=hcloud\\\"\\n            - \\\"--route-reconciliation-period=30s\\\"\\n            - \\\"--webhook-secure-port=0\\\"\\n            - \\\"--allocate-node-cidrs=true\\\"\\n            - \\\"--cluster-cidr=10.0.128.0/17\\\"\\n          env:\\n            - name: HCLOUD_LOAD_BALANCERS_DISABLE_PRIVATE_INGRESS\\n              value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_ENABLED\\n              value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_LOCATION\\n              value: hel1\\n            - name: HCLOUD_LOAD_BALANCERS_USE_PRIVATE_IP\\n              value: \\\"true\\\"\\n            - name: HCLOUD_NETWORK_ROUTES_ENABLED\\n              value: \\\"true\\\"\\n            - name: HCLOUD_TOKEN\\n              valueFrom:\\n                secretKeyRef:\\n                  key: token\\n                  name: hcloud\\n            - name: ROBOT_PASSWORD\\n              valueFrom:\\n                secretKeyRef:\\n                  key: robot-password\\n                  name: hcloud\\n                  optional: true\\n            - name: ROBOT_USER\\n              valueFrom:\\n                secretKeyRef:\\n                  key: robot-user\\n                  name: hcloud\\n                  optional: true\\n            - name: HCLOUD_NETWORK\\n              valueFrom:\\n                secretKeyRef:\\n                  key: network\\n                  name: hcloud\\n          image: docker.io/hetznercloud/hcloud-cloud-controller-manager:v1.26.0 # x-releaser-pleaser-version\\n          ports:\\n            - name: metrics\\n              containerPort: 8233\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 50Mi\\n      priorityClassName: system-cluster-critical\\n\"\n        - name: hcloud-csi\n          contents: \"\\\"apiVersion\\\": \\\"v1\\\"\\n\\\"data\\\":\\n  \\\"encryption-passphrase\\\": \\\"aXlJaUllVnpKY1l0cFFyWCZIQURkQ1lzWnd2RnRTTXB0dUgmRG51Q2VxJldic3Fl\\\"\\n\\\"kind\\\": \\\"Secret\\\"\\n\\\"metadata\\\":\\n  \\\"name\\\": \\\"hcloud-csi-secret\\\"\\n  \\\"namespace\\\": \\\"kube-system\\\"\\n\\\"type\\\": \\\"Opaque\\\"\\n\\n---\\n---\\n# Source: hcloud-csi/templates/controller/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nautomountServiceAccountToken: true\\n---\\n# Source: hcloud-csi/templates/core/storageclass.yaml\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: hcloud-volumes-encrypted-xfs\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \\\"true\\\"\\nprovisioner: csi.hetzner.cloud\\nvolumeBindingMode: WaitForFirstConsumer\\nallowVolumeExpansion: true\\nreclaimPolicy: \\\"Retain\\\"\\nparameters:\\n  csi.storage.k8s.io/fstype: xfs\\n  csi.storage.k8s.io/node-publish-secret-name: hcloud-csi-secret\\n  csi.storage.k8s.io/node-publish-secret-namespace: kube-system\\n  fsFormatOption: -i nrext64=1\\n---\\n# Source: hcloud-csi/templates/controller/clusterrole.yaml\\nkind: ClusterRole\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: hcloud-csi-controller\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nrules:\\n  # attacher\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [nodes]\\n    verbs: [get, list, watch]\\n  - apiGroups: [csi.storage.k8s.io]\\n    resources: [csinodeinfos]\\n    verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [csinodes]\\n    verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [volumeattachments]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [volumeattachments/status]\\n    verbs: [patch]\\n  # provisioner\\n  - apiGroups: [\\\"\\\"]\\n    resources: [secrets]\\n    verbs: [get, list]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n    verbs: [get, list, watch, create, delete, patch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumeclaims, persistentvolumeclaims/status]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [storageclasses]\\n    verbs: [get, list, watch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [events]\\n    verbs: [list, watch, create, update, patch]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n    resources: [volumesnapshots]\\n    verbs: [get, list]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n    resources: [volumesnapshotcontents]\\n    verbs: [get, list]\\n  # resizer\\n  - apiGroups: [\\\"\\\"]\\n    resources: [pods]\\n    verbs: [get, list, watch]\\n  # node\\n  - apiGroups: [\\\"\\\"]\\n    resources: [events]\\n    verbs: [get, list, watch, create, update, patch]\\n---\\n# Source: hcloud-csi/templates/controller/clusterrolebinding.yaml\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: hcloud-csi-controller\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: hcloud-csi-controller\\nsubjects:\\n  - kind: ServiceAccount\\n    name: hcloud-csi-controller\\n    namespace: \\\"kube-system\\\"\\n---\\n# Source: hcloud-csi/templates/node/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: hcloud-csi-node\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: node\\n    app: hcloud-csi\\nspec:\\n  updateStrategy:\\n    type: RollingUpdate\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance: hcloud-csi\\n      app.kubernetes.io/component: node\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart: hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by: Helm\\n        app.kubernetes.io/component: node\\n    spec:\\n      \\n      affinity:\\n        nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            nodeSelectorTerms:\\n            - matchExpressions:\\n              - key: instance.hetzner.cloud/is-root-server\\n                operator: NotIn\\n                values:\\n                - \\\"true\\\"\\n              - key: instance.hetzner.cloud/provided-by\\n                operator: NotIn\\n                values:\\n                - robot\\n      tolerations:\\n        - effect: NoExecute\\n          operator: Exists\\n        - effect: NoSchedule\\n          operator: Exists\\n        - key: CriticalAddonsOnly\\n          operator: Exists\\n      securityContext:\\n        fsGroup: 1001\\n      initContainers:\\n      containers:\\n        - name: csi-node-driver-registrar\\n          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.14.0\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.hetzner.cloud/socket\\n          volumeMounts:\\n            - name: plugin-dir\\n              mountPath: /run/csi\\n            - name: registration-dir\\n              mountPath: /registration\\n          resources:\\n            limits: {}\\n            requests: {}\\n        - name: liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n          imagePullPolicy: IfNotPresent\\n          volumeMounts:\\n          - mountPath: /run/csi\\n            name: plugin-dir\\n          resources:\\n            limits: {}\\n            requests: {}\\n        - name: hcloud-csi-driver\\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - -node\\n          volumeMounts:\\n            - name: kubelet-dir\\n              mountPath: /var/lib/kubelet\\n              mountPropagation: \\\"Bidirectional\\\"\\n            - name: plugin-dir\\n              mountPath: /run/csi\\n            - name: device-dir\\n              mountPath: /dev\\n          securityContext:\\n            privileged: true\\n          env:\\n            - name: CSI_ENDPOINT\\n              value: unix:///run/csi/socket\\n            - name: ENABLE_METRICS\\n              value: \\\"false\\\"\\n          ports:\\n            - name: healthz\\n              protocol: TCP\\n              containerPort: 9808\\n          resources:\\n            limits: {}\\n            requests: {}\\n          livenessProbe:\\n            failureThreshold: 5\\n            initialDelaySeconds: 10\\n            periodSeconds: 2\\n            successThreshold: 1\\n            timeoutSeconds: 3\\n            httpGet:\\n              path: /healthz\\n              port: healthz\\n      volumes:\\n        - name: kubelet-dir\\n          hostPath:\\n            path: /var/lib/kubelet\\n            type: Directory\\n        - name: plugin-dir\\n          hostPath:\\n            path: /var/lib/kubelet/plugins/csi.hetzner.cloud/\\n            type: DirectoryOrCreate\\n        - name: registration-dir\\n          hostPath:\\n            path: /var/lib/kubelet/plugins_registry/\\n            type: Directory\\n        - name: device-dir\\n          hostPath:\\n            path: /dev\\n            type: Directory\\n---\\n# Source: hcloud-csi/templates/controller/deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\n    app: hcloud-csi-controller\\nspec:\\n  replicas: 1\\n  strategy:\\n    type: RollingUpdate\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance: hcloud-csi\\n      app.kubernetes.io/component: controller\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart: hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by: Helm\\n        app.kubernetes.io/component: controller\\n    spec:\\n      serviceAccountName: hcloud-csi-controller\\n      \\n      affinity:\\n        nodeAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n          - preference:\\n              matchExpressions:\\n              - key: instance.hetzner.cloud/provided-by\\n                operator: In\\n                values:\\n                - cloud\\n            weight: 1\\n      nodeSelector:\\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      tolerations:\\n        - effect: NoSchedule\\n          key: node-role.kubernetes.io/control-plane\\n          operator: Exists\\n      topologySpreadConstraints:\\n        - labelSelector:\\n            matchLabels:\\n              app.kubernetes.io/component: controller\\n              app.kubernetes.io/instance: hcloud-csi\\n              app.kubernetes.io/name: hcloud-csi\\n          maxSkew: 1\\n          topologyKey: kubernetes.io/hostname\\n          whenUnsatisfiable: ScheduleAnyway\\n      securityContext:\\n        fsGroup: 1001\\n      initContainers:\\n      containers:\\n        - name: csi-attacher\\n          image: registry.k8s.io/sig-storage/csi-attacher:v4.9.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          args:\\n            - --default-fstype=ext4\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: csi-resizer\\n          image: registry.k8s.io/sig-storage/csi-resizer:v1.14.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: csi-provisioner\\n          image: registry.k8s.io/sig-storage/csi-provisioner:v5.3.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          args:\\n            - --feature-gates=Topology=true\\n            - --default-fstype=ext4\\n            - --extra-create-metadata\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          volumeMounts:\\n          - mountPath: /run/csi\\n            name: socket-dir\\n\\n        - name: hcloud-csi-driver\\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - -controller\\n          env:\\n            - name: CSI_ENDPOINT\\n              value: unix:///run/csi/socket\\n            - name: ENABLE_METRICS\\n              value: \\\"false\\\"\\n            - name: KUBE_NODE_NAME\\n              valueFrom:\\n                fieldRef:\\n                  apiVersion: v1\\n                  fieldPath: spec.nodeName\\n            - name: HCLOUD_TOKEN\\n              valueFrom:\\n                secretKeyRef:\\n                  name: hcloud\\n                  key: token\\n          resources:\\n            limits: {}\\n            requests: {}\\n          ports:\\n            - name: healthz\\n              protocol: TCP\\n              containerPort: 9808\\n          livenessProbe:\\n            failureThreshold: 5\\n            initialDelaySeconds: 10\\n            periodSeconds: 2\\n            successThreshold: 1\\n            timeoutSeconds: 3\\n            httpGet:\\n              path: /healthz\\n              port: healthz\\n          volumeMounts:\\n            - name: socket-dir\\n              mountPath: /run/csi\\n\\n      volumes:\\n        - name: socket-dir\\n          emptyDir: {}\\n---\\n# Source: hcloud-csi/templates/core/csidriver.yaml\\napiVersion: storage.k8s.io/v1\\nkind: CSIDriver\\nmetadata:\\n  name: csi.hetzner.cloud\\nspec:\\n  attachRequired: true\\n  fsGroupPolicy: File\\n  podInfoOnMount: true\\n  seLinuxMount: true\\n  volumeLifecycleModes:\\n  - Persistent\\n\\n\"\n        - name: talos-backup\n          contents: |+\n            \"apiVersion\": \"talos.dev/v1alpha1\"\n            \"kind\": \"ServiceAccount\"\n            \"metadata\":\n              \"name\": \"talos-backup-secrets\"\n              \"namespace\": \"kube-system\"\n            \"spec\":\n              \"roles\":\n              - \"os:etcd:backup\"\n\n            ---\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"access_key\": \"YzYwNDY2MDQ4ODdjMDgzZTk2OWUyMWJjNTVhMjQxNzQ=\"\n              \"secret_key\": \"MzI2NTEzNjY0ZWU4N2RiMTVmMWNjOGViZTcyMDBhYTEwMGI5YTFlNzRhZTM1NzZkNWJhMDg4MGQxMjg2YTlmNg==\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"talos-backup-s3-secrets\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n\n            ---\n            \"apiVersion\": \"batch/v1\"\n            \"kind\": \"CronJob\"\n            \"metadata\":\n              \"name\": \"talos-backup\"\n              \"namespace\": \"kube-system\"\n            \"spec\":\n              \"concurrencyPolicy\": \"Forbid\"\n              \"jobTemplate\":\n                \"spec\":\n                  \"template\":\n                    \"spec\":\n                      \"containers\":\n                      - \"env\":\n                        - \"name\": \"AWS_ACCESS_KEY_ID\"\n                          \"valueFrom\":\n                            \"secretKeyRef\":\n                              \"key\": \"access_key\"\n                              \"name\": \"talos-backup-s3-secrets\"\n                        - \"name\": \"AWS_SECRET_ACCESS_KEY\"\n                          \"valueFrom\":\n                            \"secretKeyRef\":\n                              \"key\": \"secret_key\"\n                              \"name\": \"talos-backup-s3-secrets\"\n                        - \"name\": \"AGE_X25519_PUBLIC_KEY\"\n                          \"value\": null\n                        - \"name\": \"DISABLE_ENCRYPTION\"\n                          \"value\": \"true\"\n                        - \"name\": \"AWS_REGION\"\n                          \"value\": \"auto\"\n                        - \"name\": \"CUSTOM_S3_ENDPOINT\"\n                          \"value\": \"https://a694d529ab7d7176bcac8585f8bafdf4.r2.cloudflarestorage.com\"\n                        - \"name\": \"BUCKET\"\n                          \"value\": \"etcd\"\n                        - \"name\": \"CLUSTER_NAME\"\n                          \"value\": \"goingdark\"\n                        - \"name\": \"S3_PREFIX\"\n                          \"value\": null\n                        - \"name\": \"USE_PATH_STYLE\"\n                          \"value\": \"true\"\n                        \"image\": \"ghcr.io/siderolabs/talos-backup:v0.1.0-beta.2-1-g9ccc125\"\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                        \"name\": \"talos-backup\"\n                        \"resources\":\n                          \"limits\":\n                            \"cpu\": \"500m\"\n                            \"memory\": \"256Mi\"\n                          \"requests\":\n                            \"cpu\": \"250m\"\n                            \"memory\": \"128Mi\"\n                        \"securityContext\":\n                          \"allowPrivilegeEscalation\": false\n                          \"capabilities\":\n                            \"drop\":\n                            - \"ALL\"\n                          \"runAsGroup\": 1000\n                          \"runAsNonRoot\": true\n                          \"runAsUser\": 1000\n                          \"seccompProfile\":\n                            \"type\": \"RuntimeDefault\"\n                        \"volumeMounts\":\n                        - \"mountPath\": \"/tmp\"\n                          \"name\": \"tmp\"\n                        - \"mountPath\": \"/var/run/secrets/talos.dev\"\n                          \"name\": \"talos-secrets\"\n                        \"workingDir\": \"/tmp\"\n                      \"restartPolicy\": \"OnFailure\"\n                      \"tolerations\":\n                      - \"effect\": \"NoSchedule\"\n                        \"key\": \"node-role.kubernetes.io/control-plane\"\n                        \"operator\": \"Exists\"\n                      \"volumes\":\n                      - \"emptyDir\": {}\n                        \"name\": \"tmp\"\n                      - \"name\": \"talos-secrets\"\n                        \"secret\":\n                          \"secretName\": \"talos-backup-secrets\"\n              \"schedule\": \"0 * * * *\"\n              \"suspend\": false\n\n        - name: metrics-server\n          contents: |\n            ---\n            # Source: metrics-server/templates/pdb.yaml\n            apiVersion: policy/v1\n            kind: PodDisruptionBudget\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              minAvailable: 1\n\n              selector:\n                matchLabels:\n                  app.kubernetes.io/name: metrics-server\n                  app.kubernetes.io/instance: metrics-server\n            ---\n            # Source: metrics-server/templates/serviceaccount.yaml\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            ---\n            # Source: metrics-server/templates/clusterrole-aggregated-reader.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              name: system:metrics-server-aggregated-reader\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n                rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n                rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n                rbac.authorization.k8s.io/aggregate-to-view: \"true\"\n            rules:\n              - apiGroups:\n                  - metrics.k8s.io\n                resources:\n                  - pods\n                  - nodes\n                verbs:\n                  - get\n                  - list\n                  - watch\n            ---\n            # Source: metrics-server/templates/clusterrole.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              name: system:metrics-server\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            rules:\n              - apiGroups:\n                - \"\"\n                resources:\n                - nodes/metrics\n                verbs:\n                - get\n              - apiGroups:\n                - \"\"\n                resources:\n                  - pods\n                  - nodes\n                  - namespaces\n                  - configmaps\n                verbs:\n                  - get\n                  - list\n                  - watch\n            ---\n            # Source: metrics-server/templates/clusterrolebinding-auth-delegator.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              name: metrics-server:system:auth-delegator\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: system:auth-delegator\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/clusterrolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              name: system:metrics-server\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: system:metrics-server\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/rolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: RoleBinding\n            metadata:\n              name: metrics-server-auth-reader\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: Role\n              name: extension-apiserver-authentication-reader\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/service.yaml\n            apiVersion: v1\n            kind: Service\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              type: ClusterIP\n              ports:\n                - name: https\n                  port: 443\n                  protocol: TCP\n                  targetPort: https\n                  appProtocol: https\n              selector:\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n            ---\n            # Source: metrics-server/templates/deployment.yaml\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              replicas: 2\n              selector:\n                matchLabels:\n                  app.kubernetes.io/name: metrics-server\n                  app.kubernetes.io/instance: metrics-server\n              template:\n                metadata:\n                  labels:\n                    app.kubernetes.io/name: metrics-server\n                    app.kubernetes.io/instance: metrics-server\n                spec:\n                  serviceAccountName: metrics-server\n                  priorityClassName: \"system-cluster-critical\"\n                  containers:\n                    - name: metrics-server\n                      securityContext:\n                        allowPrivilegeEscalation: false\n                        capabilities:\n                          drop:\n                          - ALL\n                        readOnlyRootFilesystem: true\n                        runAsNonRoot: true\n                        runAsUser: 1000\n                        seccompProfile:\n                          type: RuntimeDefault\n                      image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n                      imagePullPolicy: IfNotPresent\n                      args:\n                        - --secure-port=10250\n                        - --cert-dir=/tmp\n                        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n                        - --kubelet-use-node-status-port\n                        - --metric-resolution=15s\n                      ports:\n                      - name: https\n                        protocol: TCP\n                        containerPort: 10250\n                      livenessProbe:\n                        failureThreshold: 3\n                        httpGet:\n                          path: /livez\n                          port: https\n                          scheme: HTTPS\n                        initialDelaySeconds: 0\n                        periodSeconds: 10\n                      readinessProbe:\n                        failureThreshold: 3\n                        httpGet:\n                          path: /readyz\n                          port: https\n                          scheme: HTTPS\n                        initialDelaySeconds: 20\n                        periodSeconds: 10\n                      volumeMounts:\n                        - name: tmp\n                          mountPath: /tmp\n                      resources:\n                        requests:\n                          cpu: 100m\n                          memory: 200Mi\n                  volumes:\n                    - name: tmp\n                      emptyDir: {}\n                  topologySpreadConstraints:\n                    - labelSelector:\n                        matchLabels:\n                          app.kubernetes.io/instance: metrics-server\n                          app.kubernetes.io/name: metrics-server\n                      maxSkew: 1\n                      topologyKey: kubernetes.io/hostname\n                      whenUnsatisfiable: ScheduleAnyway\n            ---\n            # Source: metrics-server/templates/apiservice.yaml\n            apiVersion: apiregistration.k8s.io/v1\n            kind: APIService\n            metadata:\n              name: v1beta1.metrics.k8s.io\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n              annotations:\n            spec:\n              group: metrics.k8s.io\n              groupPriorityMinimum: 100\n              insecureSkipTLSVerify: true\n              service:\n                name: metrics-server\n                namespace: kube-system\n                port: 443\n              version: v1beta1\n              versionPriority: 100\n        - name: cluster-autoscaler\n          contents: |+\n            ---\n            # Source: cluster-autoscaler/templates/pdb.yaml\n            apiVersion: policy/v1\n            kind: PodDisruptionBudget\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              selector:\n                matchLabels:\n                  app.kubernetes.io/instance: \"cluster-autoscaler\"\n                  app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n            ---\n            # Source: cluster-autoscaler/templates/serviceaccount.yaml\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            automountServiceAccountToken: true\n            ---\n            # Source: cluster-autoscaler/templates/clusterrole.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            rules:\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - events\n                  - endpoints\n                verbs:\n                  - create\n                  - patch\n              - apiGroups:\n                - \"\"\n                resources:\n                - pods/eviction\n                verbs:\n                - create\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - pods/status\n                verbs:\n                  - update\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - endpoints\n                resourceNames:\n                  - cluster-autoscaler\n                verbs:\n                  - get\n                  - update\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - nodes\n                verbs:\n                - watch\n                - list\n                - create\n                - delete\n                - get\n                - update\n              - apiGroups:\n                - \"\"\n                resources:\n                  - namespaces\n                  - pods\n                  - services\n                  - replicationcontrollers\n                  - persistentvolumeclaims\n                  - persistentvolumes\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                - batch\n                resources:\n                  - jobs\n                  - cronjobs\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                - batch\n                - extensions\n                resources:\n                - jobs\n                verbs:\n                - get\n                - list\n                - patch\n                - watch\n              - apiGroups:\n                  - extensions\n                resources:\n                  - replicasets\n                  - daemonsets\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                  - policy\n                resources:\n                  - poddisruptionbudgets\n                verbs:\n                  - watch\n                  - list\n              - apiGroups:\n                - apps\n                resources:\n                - daemonsets\n                - replicasets\n                - statefulsets\n                verbs:\n                - watch\n                - list\n                - get\n              - apiGroups:\n                - storage.k8s.io\n                resources:\n                - storageclasses\n                - csinodes\n                - csidrivers\n                - csistoragecapacities\n                - volumeattachments\n                verbs:\n                - watch\n                - list\n                - get\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                verbs:\n                  - list\n                  - watch\n                  - get\n              - apiGroups:\n                - coordination.k8s.io\n                resources:\n                - leases\n                verbs:\n                - create\n              - apiGroups:\n                - coordination.k8s.io\n                resourceNames:\n                - cluster-autoscaler\n                resources:\n                - leases\n                verbs:\n                - get\n                - update\n            ---\n            # Source: cluster-autoscaler/templates/clusterrolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            subjects:\n              - kind: ServiceAccount\n                name: cluster-autoscaler-hetzner-cluster-autoscaler\n                namespace: kube-system\n            ---\n            # Source: cluster-autoscaler/templates/role.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: Role\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            rules:\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                verbs:\n                  - create\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                resourceNames:\n                  - cluster-autoscaler-status\n                verbs:\n                  - delete\n                  - get\n                  - update\n            ---\n            # Source: cluster-autoscaler/templates/rolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: RoleBinding\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: Role\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            subjects:\n              - kind: ServiceAccount\n                name: cluster-autoscaler-hetzner-cluster-autoscaler\n                namespace: kube-system\n            ---\n            # Source: cluster-autoscaler/templates/service.yaml\n            apiVersion: v1\n            kind: Service\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              ports:\n                - port: 8085\n                  protocol: TCP\n                  targetPort: 8085\n                  name: http\n              selector:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n              type: \"ClusterIP\"\n            ---\n            # Source: cluster-autoscaler/templates/deployment.yaml\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              annotations:\n                {}\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              replicas: 1\n              revisionHistoryLimit: 10\n              selector:\n                matchLabels:\n                  app.kubernetes.io/instance: \"cluster-autoscaler\"\n                  app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n              template:\n                metadata:\n                  labels:\n                    app.kubernetes.io/instance: \"cluster-autoscaler\"\n                    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                spec:\n                  priorityClassName: \"system-cluster-critical\"\n                  dnsPolicy: \"ClusterFirst\"\n                  containers:\n                    - name: hetzner-cluster-autoscaler\n                      image: \"registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0\"\n                      imagePullPolicy: \"IfNotPresent\"\n                      command:\n                        - ./cluster-autoscaler\n                        - --cloud-provider=hetzner\n                        - --namespace=kube-system\n                        - --nodes=0:2:cx43:hel1:goingdark-autoscaler\n                        - --balance-similar-node-groups=true\n                        - --expander=least-waste\n                        - --logtostderr=true\n                        - --scale-down-delay-after-add=10m\n                        - --scale-down-delay-after-delete=10m\n                        - --scale-down-unneeded-time=8m\n                        - --scale-down-utilization-threshold=0.75\n                        - --stderrthreshold=info\n                        - --v=4\n                      env:\n                        - name: POD_NAMESPACE\n                          valueFrom:\n                            fieldRef:\n                              fieldPath: metadata.namespace\n                        - name: SERVICE_ACCOUNT\n                          valueFrom:\n                            fieldRef:\n                              fieldPath: spec.serviceAccountName\n                        - name: HCLOUD_CLUSTER_CONFIG_FILE\n                          value: \"/config/cluster-config\"\n                        - name: HCLOUD_FIREWALL\n                          value: \"2379867\"\n                        - name: HCLOUD_NETWORK\n                          value: \"11368893\"\n                        - name: HCLOUD_PUBLIC_IPV4\n                          value: \"true\"\n                        - name: HCLOUD_PUBLIC_IPV6\n                          value: \"false\"\n                        - name: HCLOUD_SERVER_CREATION_TIMEOUT\n                          value: \"10\"\n                        - name: HCLOUD_SSH_KEY\n                          value: \"101120850\"\n                        - name: HCLOUD_TOKEN\n                          valueFrom:\n                            secretKeyRef:\n                              name: hcloud\n                              key: token\n                      livenessProbe:\n                        httpGet:\n                          path: /health-check\n                          port: 8085\n                      ports:\n                        - containerPort: 8085\n                      resources:\n                        {}\n                      volumeMounts:\n                        - name: cluster-autoscaler-hetzner-config\n                          mountPath: /config\n                          readOnly: true\n                  nodeSelector:\n                    node-role.kubernetes.io/control-plane: \"\"\n                  serviceAccountName: cluster-autoscaler-hetzner-cluster-autoscaler\n                  tolerations:\n                    - effect: NoSchedule\n                      key: node-role.kubernetes.io/control-plane\n                      operator: Exists\n                  topologySpreadConstraints:\n                    - labelSelector:\n                        matchLabels:\n                          app.kubernetes.io/instance: cluster-autoscaler\n                          app.kubernetes.io/name: hetzner-cluster-autoscaler\n                      maxSkew: 1\n                      topologyKey: kubernetes.io/hostname\n                      whenUnsatisfiable: ScheduleAnyway\n                  volumes:\n                    - name: cluster-autoscaler-hetzner-config\n                      secret:\n                        secretName: cluster-autoscaler-hetzner-config\n\n            ---\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"cluster-config\": \"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\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"cluster-autoscaler-hetzner-config\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n\n        - name: cilium-settings\n          contents: \"apiVersion: cilium.io/v2alpha1\\nkind: CiliumL2AnnouncementPolicy\\nmetadata:\\n  name: default-l2-announcement-policy\\n  namespace: kube-system\\nspec:\\n  externalIPs: true\\n  loadBalancerIPs: true\\n\\n---\\napiVersion: cilium.io/v2alpha1\\nkind: CiliumLoadBalancerIPPool\\nmetadata:\\n  name: service-pool\\nspec:\\n  blocks:\\n    - start: 10.0.96.240\\n      stop: 10.0.96.250\\n\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-helm-values\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/name: cilium-helm-values\\n    app.kubernetes.io/managed-by: talos-inline\\n  data:\\n    values.yaml: |-\\n# https://github.com/cilium/cilium/blob/main/install/kubernetes/cilium/values.yaml\\n      cluster:\\n        name: talos\\n        id: 1\\n      \\n      # Correct boolean, not a nested map\\n      kubeProxyReplacement: true\\n      \\n        # Talos specific\\n      k8sServiceHost: localhost\\n      k8sServicePort: 7445\\n      securityContext:\\n        capabilities:\\n          ciliumAgent:\\n            [CHOWN, KILL, NET_ADMIN, NET_RAW, IPC_LOCK, SYS_ADMIN, SYS_RESOURCE, DAC_OVERRIDE, FOWNER, SETGID, SETUID]\\n          cleanCiliumState: [NET_ADMIN, SYS_ADMIN, SYS_RESOURCE]\\n      \\n      cgroup:\\n        autoMount:\\n          enabled: false\\n        hostRoot: /sys/fs/cgroup\\n      \\n      # https://www.talos.dev/latest/talos-guides/network/host-dns/#forwarding-kube-dns-to-host-dns\\n      # https://docs.cilium.io/en/stable/operations/performance/tuning/#ebpf-host-routing\\n      bpf:\\n        hostLegacyRouting: true\\n      \\n      # https://docs.cilium.io/en/stable/network/concepts/ipam/\\n      ipam:\\n        mode: kubernetes\\n        multiPoolPreAllocation: ''\\n      \\n      enableMulticast: false\\n      multicast:\\n        enabled: false\\n      \\n      \\n      operator:\\n        rollOutPods: true\\n        resources:\\n          requests:\\n            cpu: 50m\\n            memory: 128Mi\\n          limits:\\n            memory: 256Mi\\n      \\n      # Roll out cilium agent pods automatically when ConfigMap is updated.\\n      rollOutCiliumPods: true\\n      agent: true\\n      agentConfig:\\n        tolerations:\\n          - key: gpu\\n            operator: Equal\\n            value: \\\"true\\\"\\n            effect: NoSchedule\\n        resources:\\n          requests:\\n            cpu: 100m      # Reduced from 500m - now Burstable QoS\\n            memory: 128Mi   # Reduced from 512Mi\\n          limits:\\n            memory: 384Mi   # Remove CPU limit to prevent throttling\\n      \\n      \\n      \\n      #debug:\\n      #  enabled: true\\n      \\n      # Increase rate limit when doing L2 announcements\\n      k8sClientRateLimit:\\n        qps: 20\\n        burst: 100\\n      \\n      l2announcements:\\n        enabled: true\\n      \\n      externalIPs:\\n        enabled: true\\n      \\n      #enableCiliumEndpointSlice: true\\n      \\n      loadBalancer:\\n        # https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#maglev-consistent-hashing\\n        algorithm: maglev\\n      \\n      gatewayAPI:\\n        enabled: true\\n        gatewayClass:\\n          create: \\\"true\\\"\\n        enableAlpn: true\\n        xdsServer:\\n          enabled: true\\n      envoy:\\n        securityContext:\\n          capabilities:\\n            keepCapNetBindService: true\\n            envoy: [NET_BIND_SERVICE, NET_ADMIN, PERFMON, BPF]\\n      \\n      hubble:\\n        peerService:\\n          clusterDomain: cluster.local\\n        enabled: true\\n        relay:\\n          enabled: true\\n          rollOutPods: true\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 128Mi\\n            limits:\\n              cpu: 200m\\n              memory: 256Mi\\n        ui:\\n          enabled: true\\n          rollOutPods: true\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 128Mi\\n            limits:\\n              cpu: 200m\\n              memory: 256Mi\\n      \\n      ingressController:\\n        enabled: false\\n        default: true\\n        loadbalancerMode: shared\\n        service:\\n          annotations:\\n            io.cilium/lb-ipam-ips: 10.0.96.243\\n      \\n      # mTLS\\n      authentication:\\n        enabled: false\\n        mutual:\\n          spire:\\n            enabled: false\\n            install:\\n              server:\\n                dataStorage:\\n                  storageClass: cilium-spire-sc\\n\"\n    adminKubeconfig:\n        certLifetime: 87600h0m0s\n    allowSchedulingOnControlPlanes: false\n","machine_secrets":{"certs":{"etcd":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_aggregator":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_serviceaccount":{"key":"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"},"os":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"}},"cluster":{"id":"PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=","secret":"z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU="},"secrets":{"aescbc_encryption_secret":null,"bootstrap_token":"qztdq7.ktojonbxqnrcsz3q","secretbox_encryption_secret":"vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE="},"trustdinfo":{"token":"7or9kx.41p55ltb292998sw"}},"machine_type":"controlplane","talos_version":"v1.11.1"},"sensitive_attributes":[[{"type":"get_attr","value":"config_patches"},{"type":"index","value":{"value":0,"type":"number"}}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"cluster"},{"type":"get_attr","value":"secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"aescbc_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"bootstrap_token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"secretbox_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"trustdinfo"},{"type":"get_attr","value":"token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_aggregator"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_serviceaccount"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"os"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"etcd"},{"type":"get_attr","value":"key"}]]}]},{"module":"module.kubernetes","mode":"data","type":"talos_machine_configuration","name":"worker","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"index_key":"goingdark-worker-1","schema_version":0,"attributes":{"cluster_endpoint":"https://10.0.64.126:6443","cluster_name":"goingdark","config_patches":["\"cluster\":\n  \"discovery\":\n    \"enabled\": true\n    \"registries\":\n      \"kubernetes\":\n        \"disabled\": true\n      \"service\":\n        \"disabled\": false\n  \"network\":\n    \"cni\":\n      \"name\": \"none\"\n    \"dnsDomain\": \"cluster.local\"\n    \"podSubnets\":\n    - \"10.0.128.0/17\"\n    \"serviceSubnets\":\n    - \"10.0.96.0/19\"\n  \"proxy\":\n    \"disabled\": true\n\"machine\":\n  \"certSANs\":\n  - \"10.0.64.1\"\n  - \"10.0.64.126\"\n  - \"10.0.64.254\"\n  - \"127.0.0.1\"\n  - \"46.62.164.172\"\n  - \"::1\"\n  - \"localhost\"\n  \"features\":\n    \"hostDNS\":\n      \"enabled\": true\n      \"forwardKubeDNSToHost\": false\n      \"resolveMemberNames\": true\n  \"install\":\n    \"extraKernelArgs\": []\n    \"image\": \"factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\"\n  \"kernel\":\n    \"modules\": null\n  \"kubelet\":\n    \"extraArgs\":\n      \"cloud-provider\": \"external\"\n      \"rotate-server-certificates\": true\n    \"extraConfig\":\n      \"kubeReserved\":\n        \"cpu\": \"100m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"350Mi\"\n      \"registerWithTaints\": []\n      \"shutdownGracePeriod\": \"90s\"\n      \"shutdownGracePeriodCriticalPods\": \"15s\"\n      \"systemReserved\":\n        \"cpu\": \"100m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"300Mi\"\n    \"extraMounts\": []\n    \"nodeIP\":\n      \"validSubnets\":\n      - \"10.0.64.0/19\"\n  \"logging\":\n    \"destinations\": []\n  \"network\":\n    \"extraHostEntries\": []\n    \"hostname\": \"goingdark-worker-1\"\n    \"interfaces\":\n    - \"dhcp\": true\n      \"dhcpOptions\":\n        \"ipv4\": true\n        \"ipv6\": false\n      \"interface\": \"eth0\"\n    - \"dhcp\": true\n      \"interface\": \"eth1\"\n      \"routes\": []\n    \"nameservers\":\n    - \"185.12.64.1\"\n    - \"185.12.64.2\"\n    - \"2a01:4ff:ff00::add:1\"\n    - \"2a01:4ff:ff00::add:2\"\n  \"nodeAnnotations\": {}\n  \"nodeLabels\":\n    \"nodepool\": \"worker\"\n  \"registries\": null\n  \"sysctls\":\n    \"net.core.netdev_max_backlog\": \"4096\"\n    \"net.core.somaxconn\": \"65535\"\n    \"net.ipv6.conf.all.disable_ipv6\": 0\n    \"net.ipv6.conf.default.disable_ipv6\": 0\n  \"systemDiskEncryption\":\n    \"ephemeral\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n    \"state\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n  \"time\":\n    \"servers\":\n    - \"ntp1.hetzner.de\"\n    - \"ntp2.hetzner.com\"\n    - \"ntp3.hetzner.net\"\n","\"machine\":\n  \"sysctls\":\n    \"vm.max_map_count\": \"262144\"\n"],"docs":false,"examples":false,"id":"goingdark","kubernetes_version":"v1.33.4","machine_configuration":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: worker\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints: []\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-worker-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n        vm.max_map_count: \"262144\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        nodepool: worker\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    proxy:\n        disabled: true\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n","machine_secrets":{"certs":{"etcd":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_aggregator":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_serviceaccount":{"key":"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"},"os":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"}},"cluster":{"id":"PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=","secret":"z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU="},"secrets":{"aescbc_encryption_secret":null,"bootstrap_token":"qztdq7.ktojonbxqnrcsz3q","secretbox_encryption_secret":"vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE="},"trustdinfo":{"token":"7or9kx.41p55ltb292998sw"}},"machine_type":"worker","talos_version":"v1.11.1"},"sensitive_attributes":[[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"secretbox_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"aescbc_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"bootstrap_token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"trustdinfo"},{"type":"get_attr","value":"token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_aggregator"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_serviceaccount"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"os"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"etcd"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"cluster"},{"type":"get_attr","value":"secret"}]]},{"index_key":"goingdark-worker-secondary-1","schema_version":0,"attributes":{"cluster_endpoint":"https://10.0.64.126:6443","cluster_name":"goingdark","config_patches":["\"cluster\":\n  \"discovery\":\n    \"enabled\": true\n    \"registries\":\n      \"kubernetes\":\n        \"disabled\": true\n      \"service\":\n        \"disabled\": false\n  \"network\":\n    \"cni\":\n      \"name\": \"none\"\n    \"dnsDomain\": \"cluster.local\"\n    \"podSubnets\":\n    - \"10.0.128.0/17\"\n    \"serviceSubnets\":\n    - \"10.0.96.0/19\"\n  \"proxy\":\n    \"disabled\": true\n\"machine\":\n  \"certSANs\":\n  - \"10.0.64.1\"\n  - \"10.0.64.126\"\n  - \"10.0.64.254\"\n  - \"127.0.0.1\"\n  - \"46.62.164.172\"\n  - \"::1\"\n  - \"localhost\"\n  \"features\":\n    \"hostDNS\":\n      \"enabled\": true\n      \"forwardKubeDNSToHost\": false\n      \"resolveMemberNames\": true\n  \"install\":\n    \"extraKernelArgs\": []\n    \"image\": \"factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\"\n  \"kernel\":\n    \"modules\": null\n  \"kubelet\":\n    \"extraArgs\":\n      \"cloud-provider\": \"external\"\n      \"rotate-server-certificates\": true\n    \"extraConfig\":\n      \"kubeReserved\":\n        \"cpu\": \"100m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"350Mi\"\n      \"registerWithTaints\": []\n      \"shutdownGracePeriod\": \"90s\"\n      \"shutdownGracePeriodCriticalPods\": \"15s\"\n      \"systemReserved\":\n        \"cpu\": \"100m\"\n        \"ephemeral-storage\": \"1Gi\"\n        \"memory\": \"300Mi\"\n    \"extraMounts\": []\n    \"nodeIP\":\n      \"validSubnets\":\n      - \"10.0.64.0/19\"\n  \"logging\":\n    \"destinations\": []\n  \"network\":\n    \"extraHostEntries\": []\n    \"hostname\": \"goingdark-worker-secondary-1\"\n    \"interfaces\":\n    - \"dhcp\": true\n      \"dhcpOptions\":\n        \"ipv4\": true\n        \"ipv6\": false\n      \"interface\": \"eth0\"\n    - \"dhcp\": true\n      \"interface\": \"eth1\"\n      \"routes\": []\n    \"nameservers\":\n    - \"185.12.64.1\"\n    - \"185.12.64.2\"\n    - \"2a01:4ff:ff00::add:1\"\n    - \"2a01:4ff:ff00::add:2\"\n  \"nodeAnnotations\": {}\n  \"nodeLabels\":\n    \"nodepool\": \"worker-secondary\"\n  \"registries\": null\n  \"sysctls\":\n    \"net.core.netdev_max_backlog\": \"4096\"\n    \"net.core.somaxconn\": \"65535\"\n    \"net.ipv6.conf.all.disable_ipv6\": 0\n    \"net.ipv6.conf.default.disable_ipv6\": 0\n  \"systemDiskEncryption\":\n    \"ephemeral\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n    \"state\":\n      \"keys\":\n      - \"nodeID\": {}\n        \"slot\": 0\n      \"options\":\n      - \"no_read_workqueue\"\n      - \"no_write_workqueue\"\n      \"provider\": \"luks2\"\n  \"time\":\n    \"servers\":\n    - \"ntp1.hetzner.de\"\n    - \"ntp2.hetzner.com\"\n    - \"ntp3.hetzner.net\"\n","\"machine\":\n  \"sysctls\":\n    \"vm.max_map_count\": \"262144\"\n"],"docs":false,"examples":false,"id":"goingdark","kubernetes_version":"v1.33.4","machine_configuration":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: worker\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints: []\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-worker-secondary-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n        vm.max_map_count: \"262144\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        nodepool: worker-secondary\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    proxy:\n        disabled: true\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n","machine_secrets":{"certs":{"etcd":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_aggregator":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_serviceaccount":{"key":"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"},"os":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"}},"cluster":{"id":"PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=","secret":"z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU="},"secrets":{"aescbc_encryption_secret":null,"bootstrap_token":"qztdq7.ktojonbxqnrcsz3q","secretbox_encryption_secret":"vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE="},"trustdinfo":{"token":"7or9kx.41p55ltb292998sw"}},"machine_type":"worker","talos_version":"v1.11.1"},"sensitive_attributes":[[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"trustdinfo"},{"type":"get_attr","value":"token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"etcd"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_aggregator"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_serviceaccount"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"os"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"cluster"},{"type":"get_attr","value":"secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"bootstrap_token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"secretbox_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"aescbc_encryption_secret"}]]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_firewall","name":"this","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":0,"attributes":{"apply_to":[{"label_selector":"","server":107257777},{"label_selector":"","server":107380026},{"label_selector":"","server":112565838},{"label_selector":"","server":115519273}],"id":"2379867","labels":{"cluster":"goingdark"},"name":"goingdark","rule":[{"description":"Allow IMAP (TCP 143)","destination_ips":[],"direction":"in","port":"143","protocol":"tcp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow IMAPS (TCP 993)","destination_ips":[],"direction":"in","port":"993","protocol":"tcp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow Incoming Requests to Kube API","destination_ips":[],"direction":"in","port":"6443","protocol":"tcp","source_ips":["10.0.0.0/16","10.0.64.254/32","170.62.100.224/32","95.155.200.56/32","95.217.173.34/32"]},{"description":"Allow Incoming Requests to Talos API","destination_ips":[],"direction":"in","port":"50000","protocol":"tcp","source_ips":["10.0.0.0/16","10.0.64.254/32","170.62.100.224/32","95.155.200.56/32","95.217.173.34/32"]},{"description":"Allow POP3 (TCP 110)","destination_ips":[],"direction":"in","port":"110","protocol":"tcp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow POP3S (TCP 995)","destination_ips":[],"direction":"in","port":"995","protocol":"tcp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow SMTP (TCP 25)","destination_ips":[],"direction":"in","port":"25","protocol":"tcp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow SMTP Submission (TCP 587)","destination_ips":[],"direction":"in","port":"587","protocol":"tcp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow SMTPS (TCP 465)","destination_ips":[],"direction":"in","port":"465","protocol":"tcp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow UDP to Cloudflare on port 8443","destination_ips":[],"direction":"in","port":"8443","protocol":"udp","source_ips":["0.0.0.0/0","::/0"]},{"description":"Allow inbound established UDP responses","destination_ips":[],"direction":"in","port":"32768-65535","protocol":"udp","source_ips":["0.0.0.0/0","::/0"]}]},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_network","name":"this","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"delete_protection":false,"expose_routes_to_vswitch":false,"id":"11368893","ip_range":"10.0.0.0/16","labels":{"cluster":"goingdark"},"name":"goingdark"},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["module.kubernetes.data.hcloud_network.this"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_network_subnet","name":"autoscaler","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":0,"attributes":{"gateway":"10.0.0.1","id":"11368893-10.0.95.128/25","ip_range":"10.0.95.128/25","network_id":11368893,"network_zone":"eu-central","type":"cloud","vswitch_id":null},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["module.kubernetes.data.hcloud_network.this","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_network_subnet","name":"control_plane","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":0,"attributes":{"gateway":"10.0.0.1","id":"11368893-10.0.64.0/25","ip_range":"10.0.64.0/25","network_id":11368893,"network_zone":"eu-central","type":"cloud","vswitch_id":null},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["module.kubernetes.data.hcloud_network.this","module.kubernetes.hcloud_network.this"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_network_subnet","name":"load_balancer","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":0,"attributes":{"gateway":"10.0.0.1","id":"11368893-10.0.64.128/25","ip_range":"10.0.64.128/25","network_id":11368893,"network_zone":"eu-central","type":"cloud","vswitch_id":null},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["module.kubernetes.data.hcloud_network.this","module.kubernetes.hcloud_network.this"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_network_subnet","name":"worker","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"index_key":"worker","schema_version":0,"attributes":{"gateway":"10.0.0.1","id":"11368893-10.0.65.0/25","ip_range":"10.0.65.0/25","network_id":11368893,"network_zone":"eu-central","type":"cloud","vswitch_id":null},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["module.kubernetes.data.hcloud_network.this","module.kubernetes.hcloud_network.this"]},{"index_key":"worker-secondary","schema_version":0,"attributes":{"gateway":"10.0.0.1","id":"11368893-10.0.65.128/25","ip_range":"10.0.65.128/25","network_id":11368893,"network_zone":"eu-central","type":"cloud","vswitch_id":null},"sensitive_attributes":[],"private":"bnVsbA==","dependencies":["module.kubernetes.data.hcloud_network.this","module.kubernetes.hcloud_network.this"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_placement_group","name":"control_plane","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":0,"attributes":{"id":"1123923","labels":{"cluster":"goingdark","role":"control-plane"},"name":"goingdark-control-plane-pg","servers":[107257777],"type":"spread"},"sensitive_attributes":[],"private":"bnVsbA=="}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_placement_group","name":"worker","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"index_key":"goingdark-worker-pg-1","schema_version":0,"attributes":{"id":"1128028","labels":{"cluster":"goingdark","nodepool":"worker","role":"worker"},"name":"goingdark-worker-pg-1","servers":[107380026],"type":"spread"},"sensitive_attributes":[],"private":"bnVsbA=="},{"index_key":"goingdark-worker-secondary-pg-1","schema_version":0,"attributes":{"id":"1265533","labels":{"cluster":"goingdark","nodepool":"worker-secondary","role":"worker"},"name":"goingdark-worker-secondary-pg-1","servers":[112565838],"type":"spread"},"sensitive_attributes":[],"private":"bnVsbA=="}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_server","name":"control_plane","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"index_key":"goingdark-control-1","schema_version":0,"attributes":{"allow_deprecated_images":false,"backup_window":"","backups":false,"datacenter":"hel1-dc2","delete_protection":false,"firewall_ids":[2379867],"id":"107257777","ignore_remote_firewall_ids":false,"image":"311396083","ipv4_address":"46.62.164.172","ipv6_address":"","ipv6_network":"\u003cnil\u003e","iso":null,"keep_disk":false,"labels":{"cluster":"goingdark","nodepool":"control","role":"control-plane"},"location":"hel1","name":"goingdark-control-1","network":[{"alias_ips":["10.0.64.126"],"ip":"10.0.64.1","mac_address":"86:00:00:a9:14:9d","network_id":11368893}],"placement_group_id":1123923,"primary_disk_size":80,"public_net":[{"ipv4":0,"ipv4_enabled":true,"ipv6":0,"ipv6_enabled":false}],"rebuild_protection":false,"rescue":null,"server_type":"cx33","shutdown_before_deletion":true,"ssh_keys":["101120850"],"status":"running","timeouts":null,"user_data":null},"sensitive_attributes":[],"private":"eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo1NDAwMDAwMDAwMDAwfX0=","dependencies":["module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_server","name":"worker","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"index_key":"goingdark-worker-1","schema_version":0,"attributes":{"allow_deprecated_images":false,"backup_window":"","backups":false,"datacenter":"hel1-dc2","delete_protection":false,"firewall_ids":[2379867],"id":"107380026","ignore_remote_firewall_ids":false,"image":"312133846","ipv4_address":"46.62.203.120","ipv6_address":"","ipv6_network":"\u003cnil\u003e","iso":null,"keep_disk":false,"labels":{"cluster":"goingdark","nodepool":"worker","role":"worker"},"location":"hel1","name":"goingdark-worker-1","network":[{"alias_ips":[],"ip":"10.0.65.1","mac_address":"86:00:00:a8:98:03","network_id":11368893}],"placement_group_id":1128028,"primary_disk_size":80,"public_net":[{"ipv4":0,"ipv4_enabled":true,"ipv6":0,"ipv6_enabled":false}],"rebuild_protection":false,"rescue":null,"server_type":"cx43","shutdown_before_deletion":true,"ssh_keys":["101120850"],"status":"running","timeouts":null,"user_data":null},"sensitive_attributes":[],"private":"eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo1NDAwMDAwMDAwMDAwfX0=","dependencies":["module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.tls_private_key.ssh_key"]},{"index_key":"goingdark-worker-secondary-1","schema_version":0,"attributes":{"allow_deprecated_images":false,"backup_window":"","backups":false,"datacenter":"hel1-dc2","delete_protection":false,"firewall_ids":[2379867],"id":"112565838","ignore_remote_firewall_ids":false,"image":"317978117","ipv4_address":"135.181.92.222","ipv6_address":"","ipv6_network":"\u003cnil\u003e","iso":null,"keep_disk":false,"labels":{"cluster":"goingdark","nodepool":"worker-secondary","role":"worker"},"location":"hel1","name":"goingdark-worker-secondary-1","network":[{"alias_ips":[],"ip":"10.0.65.129","mac_address":"86:00:00:94:fb:71","network_id":11368893}],"placement_group_id":1265533,"primary_disk_size":80,"public_net":[{"ipv4":0,"ipv4_enabled":true,"ipv6":0,"ipv6_enabled":false}],"rebuild_protection":false,"rescue":null,"server_type":"cx33","shutdown_before_deletion":true,"ssh_keys":["101120850"],"status":"running","timeouts":null,"user_data":null},"sensitive_attributes":[],"private":"eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo1NDAwMDAwMDAwMDAwfX0=","dependencies":["module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_ssh_key","name":"this","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":0,"attributes":{"fingerprint":"67:a0:23:3e:0b:5d:ad:90:ef:33:9c:ec:32:cd:84:9b","id":"101120850","labels":{"cluster":"goingdark"},"name":"goingdark-default","public_key":"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIMU5Oi3/CXsea0nKIY0CLTj1Nvx2HdCQC3aNGHkmzaee\n"},"sensitive_attributes":[],"dependencies":["module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"hcloud_uploaded_certificate","name":"state","provider":"module.kubernetes.provider[\"registry.opentofu.org/hetznercloud/hcloud\"]","instances":[{"schema_version":1,"attributes":{"certificate":"-----BEGIN CERTIFICATE-----\nMIIC2jCCAcKgAwIBAgIRAPnXgpn+6biaAieROzh+VDYwDQYJKoZIhvcNAQELBQAw\nFDESMBAGA1UEAxMJZ29pbmdkYXJrMCAXDTI1MDgyMzA5MTI0N1oYDzIxMjUwODI0\nMDkxMjQ3WjAUMRIwEAYDVQQDEwlnb2luZ2RhcmswggEiMA0GCSqGSIb3DQEBAQUA\nA4IBDwAwggEKAoIBAQDRgrI1Ty5TlbY65wrgR0ErWAdUlu8R7+qk22e8hKg7mViK\neCLyhCFmpZp7s0LNTf6hbHDIL67Goz/UbYegIHWNGj7In4ZP5hjgRaOTTxoV7ZCm\ndIV1mAp5B4naU3fjyPG1Tc3sVUc0hIjRdp9H2OKTlQGp4WmrV+Z5Dve29tObTMld\nqMIkDwPG4jeAwUndAtpBdX1TAVKix8+Pqg1L/I/NeSOa2oVKlo1EtlDANIOKjhCl\nQk19ISuGqzIzLS0bu22L7tSUj7KOhPJx7Qm0jc/YQERiZr5nrSNwcNwIxNC5jSZZ\nDgj3WgMyZQVScXtmalNad79FvdV9ujIwRPzgbgdnAgMBAAGjJTAjMBMGA1UdJQQM\nMAoGCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwDQYJKoZIhvcNAQELBQADggEBAJNk\n7E8QpmF7KRvRO6rorysv82xT7Q+KSuFBdaxgu0V0LmHCpn3ZXZqJwAB1GLiKv1Th\n7cvhr+jsr5KdYHFbIEenOH0BsQnqODj5GJtRRXzvQLcL2awzb+NAN8n1wnwdC6cZ\nbwWFO3jg0DThGF80vEjNME3EGC3nfz8RIhH5zxSUjt3OX9s2xi2kttlgcw/HTP3C\np+mqLI6ZHlwsDM1+n+fBjJhPCWR/n+/OyDeyLbWkadgLjIU41hI7e9vH698ZHQNw\nToP/ckQD7X2FSpdBp/z+NQA63WzQZE30YEVxGXnonUu9Z1X55E2U6DkH2SFv44TL\n2qcGS3CP9LYspObP994=\n-----END CERTIFICATE-----\n","created":"2025-08-23T09:16:39Z","domain_names":["goingdark"],"fingerprint":"9E:FB:96:BA:3E:6C:F2:08:D1:82:52:B8:10:4E:F4:49:2B:9B:1E:C8:E9:58:4E:7A:30:E3:83:B2:0B:FD:42:A7","id":"1487604","labels":{"cluster":"goingdark","state":"initialized"},"name":"goingdark-state","not_valid_after":"2125-08-24T09:12:47Z","not_valid_before":"2025-08-23T09:12:47Z","private_key":"-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA0YKyNU8uU5W2OucK4EdBK1gHVJbvEe/qpNtnvISoO5lYingi\n8oQhZqWae7NCzU3+oWxwyC+uxqM/1G2HoCB1jRo+yJ+GT+YY4EWjk08aFe2QpnSF\ndZgKeQeJ2lN348jxtU3N7FVHNISI0XafR9jik5UBqeFpq1fmeQ73tvbTm0zJXajC\nJA8DxuI3gMFJ3QLaQXV9UwFSosfPj6oNS/yPzXkjmtqFSpaNRLZQwDSDio4QpUJN\nfSErhqsyMy0tG7tti+7UlI+yjoTyce0JtI3P2EBEYma+Z60jcHDcCMTQuY0mWQ4I\n91oDMmUFUnF7ZmpTWne/Rb3VfboyMET84G4HZwIDAQABAoIBABFHhhAnLCsg+Asf\n9Bcu+UfXC5+6j77Qepnx2I/v9voXZHnjJ6q4eBLBtMiKrodFQqt980f8xXLvkuZK\ndFBMwl/gBW6W2EUttT6GBSmInB5T3hrEax9u6eo1KMr0ww4VNs4yZmSdXJvkacOw\n1Lk1wOuAHhzXO60HOQ8h9g9FMzWlzOngLWrjYB0SqtRxd9RoHEVzwIXh/KPlrvDn\nUeT06wgbLw5ps6z1ndGfsQUt0vnm3btwBfat0GXzd9a5+i3d7chgvvfrl3bWBScb\nk5C3jvg/HtTjOdfHUAs4eZxJLYROu+mgEwtrBge/a9YRdMcbetNnMANyAMneW9Kg\n8mn6xlUCgYEA5FAACadLKju/TWcrpcHcGNLwYnQu7p1tsaJc5mtxQGjqqNes2zah\nPzrgJvSljfX9W8xIvqfwy/UazBKxtD+s7yep+VinShaUPHFD0r07viyBmS/121CQ\nUt+ylHA7Xlnqq/Oik0GpaFIf0p8x2Tw4kpemFiRHT+aYxP1FY6pExbUCgYEA6ur8\nVx+jJWuGUPkajMEslKHR41L2C53+G6AzBoQF+Hls1jza6ylgMBoi9D4ecXwSG+67\nx12aMT5rGG+Z/FQPJE1mTSHI6O0DIJ/XjudSdnd17sI4voLAVzrkBPwH78Pxj/fn\nR6qOeaOBzUBHbouG68uGbAsJRypRLJ0busNgyisCgYB0vi92wDNgEhL82j9oT5ti\nmiHOxgflfVxE03fbXp8XR06OLVI2+VU0Rr+tS7WHxHvbv7aGvssTD263YOu69kBQ\nPaDdXiaUbL05ttTuYlK1KH7QPMTjcuuJZNckrcq9sQ6dBERiNdYRAMVC5qIPqYpn\n8uE7Oyu3wHrnZ3ZewwbNbQKBgBHMDaP4FJ8QH1PhG17Qf7Ue8Uy8i0FkDc0//L2g\noOcrI9CzOY6ZSt06Da3dSuckFCcjlfxLnhPe+7QVOGBCagdBCeAMEDF9S0Bce7Mq\n2Td6Y2fcNPpR44p4PrAV++/xM0rJ6C6owDR866eGGCNK1MTmXV+wElre3Nl5aZFj\nneCTAoGBAJWfDOmSsjpjt76zx7THcWRi56Y5kA1bsLIl6ZiKHne59cpleOY/D6WY\ns+DGTYw96sGGYkE+uessQfXwh4bKa8JRcM4VUZngHrqlWkqzu9Qk9BvjpoxXjFc6\n/IbEsCCeP/gQmk2I2X4mpdtoOPsDCPBVVkBkRcEzxE6xPALYg026\n-----END RSA PRIVATE KEY-----\n","type":"uploaded"},"sensitive_attributes":[[{"type":"get_attr","value":"private_key"}]],"private":"eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==","dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_bootstrap.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_configuration_apply.worker","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.synchronize_manifests","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key","module.kubernetes.tls_private_key.state","module.kubernetes.tls_self_signed_cert.state"]}]},{"module":"module.kubernetes","mode":"managed","type":"random_bytes","name":"hcloud_csi_encryption_key","provider":"provider[\"registry.opentofu.org/hashicorp/random\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"base64":"fCenL/jRIZG9xtXiFzXlrfzj10GoZsm5BCaC2YZXiFs=","hex":"7c27a72ff8d12191bdc6d5e21735e5adfce3d741a866c9b9042682d98657885b","keepers":null,"length":32},"sensitive_attributes":[[{"type":"get_attr","value":"base64"}],[{"type":"get_attr","value":"hex"}]]}]},{"module":"module.kubernetes","mode":"managed","type":"talos_cluster_kubeconfig","name":"this","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"schema_version":1,"attributes":{"certificate_renewal_duration":"720h","client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","client_key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"},"endpoint":"46.62.164.172","id":"goingdark","kubeconfig_raw":"apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: https://10.0.64.126:6443\n  name: goingdark\ncontexts:\n- context:\n    cluster: goingdark\n    namespace: default\n    user: admin@goingdark\n  name: admin@goingdark\ncurrent-context: admin@goingdark\nkind: Config\npreferences: {}\nusers:\n- name: admin@goingdark\n  user:\n    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJnekNDQVNxZ0F3SUJBZ0lRWU5wQWN6TVAvMFJ4U24yamlkZmRuREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1UWXdOMW9YRFRNMU1EZ3lNVEE1TVRZeApOMW93S1RFWE1CVUdBMVVFQ2hNT2MzbHpkR1Z0T20xaGMzUmxjbk14RGpBTUJnTlZCQU1UQldGa2JXbHVNRmt3CkV3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFZWpuRGhaUGJRbG00dUtHNzE5a1Q1aVZHbmMvTkVuYmQKTlExc3FOTXhRUUJ0aDJ2VzBzcE5SWmZDSmQyUHBuSUVlQUlSazkzbXRRMFF1NHludWFnM2xxTklNRVl3RGdZRApWUjBQQVFIL0JBUURBZ1dnTUJNR0ExVWRKUVFNTUFvR0NDc0dBUVVGQndNQ01COEdBMVVkSXdRWU1CYUFGR051CnJEUHl4aTB6dHg1NWVhbU45R0FCcVRjeE1Bb0dDQ3FHU000OUJBTUNBMGNBTUVRQ0lDdTMxVlp6ZW5NZGp5RkgKMk9DTTJzQ2U5Ym1RMzlNK1pxdkVUOWVjYU5ncEFpQkl5c0RYVUVVR2pHdFJka25yQTVhZHNablYvVnVsVVVTTQprME91N2dBZm1RPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\n    client-key-data: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUhielRPckgvL3Ixdm9OREJEbk5rSVBDL3RqSEtmak14TGlieTNCNnJNZExvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZWpuRGhaUGJRbG00dUtHNzE5a1Q1aVZHbmMvTkVuYmROUTFzcU5NeFFRQnRoMnZXMHNwTgpSWmZDSmQyUHBuSUVlQUlSazkzbXRRMFF1NHludWFnM2xnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n","kubernetes_client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJnekNDQVNxZ0F3SUJBZ0lRWU5wQWN6TVAvMFJ4U24yamlkZmRuREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1UWXdOMW9YRFRNMU1EZ3lNVEE1TVRZeApOMW93S1RFWE1CVUdBMVVFQ2hNT2MzbHpkR1Z0T20xaGMzUmxjbk14RGpBTUJnTlZCQU1UQldGa2JXbHVNRmt3CkV3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFZWpuRGhaUGJRbG00dUtHNzE5a1Q1aVZHbmMvTkVuYmQKTlExc3FOTXhRUUJ0aDJ2VzBzcE5SWmZDSmQyUHBuSUVlQUlSazkzbXRRMFF1NHludWFnM2xxTklNRVl3RGdZRApWUjBQQVFIL0JBUURBZ1dnTUJNR0ExVWRKUVFNTUFvR0NDc0dBUVVGQndNQ01COEdBMVVkSXdRWU1CYUFGR051CnJEUHl4aTB6dHg1NWVhbU45R0FCcVRjeE1Bb0dDQ3FHU000OUJBTUNBMGNBTUVRQ0lDdTMxVlp6ZW5NZGp5RkgKMk9DTTJzQ2U5Ym1RMzlNK1pxdkVUOWVjYU5ncEFpQkl5c0RYVUVVR2pHdFJka25yQTVhZHNablYvVnVsVVVTTQprME91N2dBZm1RPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=","client_key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUhielRPckgvL3Ixdm9OREJEbk5rSVBDL3RqSEtmak14TGlieTNCNnJNZExvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZWpuRGhaUGJRbG00dUtHNzE5a1Q1aVZHbmMvTkVuYmROUTFzcU5NeFFRQnRoMnZXMHNwTgpSWmZDSmQyUHBuSUVlQUlSazkzbXRRMFF1NHludWFnM2xnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=","host":"https://10.0.64.126:6443"},"node":"46.62.164.172","timeouts":null},"sensitive_attributes":[[{"type":"get_attr","value":"client_configuration"},{"type":"get_attr","value":"client_key"}],[{"type":"get_attr","value":"kubeconfig_raw"}],[{"type":"get_attr","value":"kubernetes_client_configuration"},{"type":"get_attr","value":"client_key"}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"talos_image_factory_schematic","name":"this","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"id":"ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515","schematic":"\"customization\":\n  \"extraKernelArgs\": []\n  \"systemExtensions\":\n    \"officialExtensions\":\n    - \"siderolabs/qemu-guest-agent\"\n"},"sensitive_attributes":[],"dependencies":["module.kubernetes.data.talos_image_factory_extensions_versions.this"]}]},{"module":"module.kubernetes","mode":"managed","type":"talos_machine_bootstrap","name":"this","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"schema_version":1,"attributes":{"client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","client_key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"},"endpoint":"46.62.164.172","id":"machine_bootstrap","node":"10.0.64.1","timeouts":null},"sensitive_attributes":[[{"type":"get_attr","value":"client_configuration"},{"type":"get_attr","value":"client_key"}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_configuration_apply.worker","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"talos_machine_configuration_apply","name":"control_plane","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"index_key":"goingdark-control-1","schema_version":1,"attributes":{"apply_mode":"auto","client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","client_key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"},"config_patches":null,"endpoint":"46.62.164.172","id":"machine_configuration_apply","machine_configuration":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: controlplane\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 250m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints:\n                - effect: NoSchedule\n                  key: node-role.kubernetes.io/control-plane\n                  value: \"\"\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 250m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-control-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n              vip:\n                ip: 10.0.64.126\n                hcloud:\n                    apiToken: lkbVSBpKQf0XMxYjXcxBLSUl9DYHEoWFnquZ2r0taZTUY1llFe9raH1hQXqZ9ZfQ\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        kubernetesTalosAPIAccess:\n            enabled: true\n            allowedRoles:\n                - os:reader\n                - os:etcd:backup\n            allowedKubernetesNamespaces:\n                - kube-system\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        node.kubernetes.io/exclude-from-external-load-balancers: \"\"\n        nodepool: control\n    nodeTaints:\n        node-role.kubernetes.io/control-plane: :NoSchedule\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    secretboxEncryptionSecret: vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE=\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n    aggregatorCA:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n        key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n    serviceAccount:\n        key: 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\n    apiServer:\n        image: registry.k8s.io/kube-apiserver:v1.33.4\n        extraArgs:\n            enable-aggregator-routing: \"true\"\n        certSANs:\n            - 10.0.64.126\n            - 10.0.64.1\n            - 10.0.64.126\n            - 10.0.64.254\n            - 127.0.0.1\n            - 46.62.164.172\n            - ::1\n            - localhost\n        disablePodSecurityPolicy: true\n        admissionControl:\n            - name: PodSecurity\n              configuration:\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n                defaults:\n                    audit: restricted\n                    audit-version: latest\n                    enforce: baseline\n                    enforce-version: latest\n                    warn: restricted\n                    warn-version: latest\n                exemptions:\n                    namespaces:\n                        - kube-system\n                    runtimeClasses: []\n                    usernames: []\n                kind: PodSecurityConfiguration\n        auditPolicy:\n            apiVersion: audit.k8s.io/v1\n            kind: Policy\n            rules:\n                - level: Metadata\n    controllerManager:\n        image: registry.k8s.io/kube-controller-manager:v1.33.4\n        extraArgs:\n            bind-address: 0.0.0.0\n            cloud-provider: external\n    proxy:\n        disabled: true\n        image: registry.k8s.io/kube-proxy:v1.33.4\n    scheduler:\n        image: registry.k8s.io/kube-scheduler:v1.33.4\n        extraArgs:\n            bind-address: 0.0.0.0\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n    etcd:\n        ca:\n            crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n            key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n        extraArgs:\n            listen-metrics-urls: http://0.0.0.0:2381\n        advertisedSubnets:\n            - 10.0.64.0/25\n    coreDNS:\n        disabled: false\n    externalCloudProvider:\n        enabled: true\n        manifests:\n            - https://raw.githubusercontent.com/siderolabs/talos-cloud-controller-manager/v1.10.1/docs/deploy/cloud-controller-manager-daemonset.yml\n            - https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.85.0/stripped-down-crds.yaml\n            - https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.3.0/standard-install.yaml\n    inlineManifests:\n        - name: hcloud-secret\n          contents: |\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"network\": \"MTEzNjg4OTM=\"\n              \"token\": \"bGtiVlNCcEtRZjBYTXhZalhjeEJMU1VsOURZSEVvV0ZucXVaMnIwdGFaVFVZMWxsRmU5cmFIMWhRWHFaOVpmUQ==\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"hcloud\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n        - name: cilium\n          contents: \"null\\n...\\n\\n---\\n---\\n# Source: cilium/templates/cilium-secrets-namespace.yaml\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\n  annotations:\\n---\\n# Source: cilium/templates/cilium-agent/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-agent/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-config-agent\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - configmaps\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n---\\n# Source: cilium/templates/cilium-agent/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n---\\n# Source: cilium/templates/cilium-operator/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - create\\n  - delete\\n  - update\\n  - patch\\n---\\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-config-agent\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-config-agent\\nsubjects:\\n  - kind: ServiceAccount\\n    name: \\\"cilium\\\"\\n    namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-tlsinterception-secrets\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-operator-tlsinterception-secrets\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-envoy/service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: cilium-envoy\\n  namespace: kube-system\\n  annotations:\\n    prometheus.io/scrape: \\\"true\\\"\\n    prometheus.io/port: \\\"9964\\\"\\n  labels:\\n    k8s-app: cilium-envoy\\n    app.kubernetes.io/name: cilium-envoy\\n    app.kubernetes.io/part-of: cilium\\n    io.cilium/app: proxy\\nspec:\\n  clusterIP: None\\n  type: ClusterIP\\n  selector:\\n    k8s-app: cilium-envoy\\n  ports:\\n  - name: envoy-metrics\\n    port: 9964\\n    protocol: TCP\\n    targetPort: envoy-metrics\\n---\\n# Source: cilium/templates/cilium-agent/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: cilium\\n  namespace: kube-system\\n  labels:\\n    k8s-app: cilium\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-agent\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cilium\\n  updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n        prometheus.io/port: \\\"9962\\\"\\n        prometheus.io/scrape: \\\"true\\\"\\n        kubectl.kubernetes.io/default-container: cilium-agent\\n      labels:\\n        k8s-app: cilium\\n        app.kubernetes.io/name: cilium-agent\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n        appArmorProfile:\\n          type: Unconfined\\n        seccompProfile:\\n          type: Unconfined\\n      containers:\\n      - name: cilium-agent\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-agent\\n        args:\\n        - --config-dir=/tmp/cilium/config-map\\n        startupProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n          failureThreshold: 300\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds: 5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n            - name: \\\"require-k8s-connectivity\\\"\\n              value: \\\"false\\\"\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_CLUSTERMESH_CONFIG\\n          value: /var/lib/cilium/clustermesh/\\n        - name: GOMEMLIMIT\\n          valueFrom:\\n            resourceFieldRef:\\n              resource: limits.memory\\n              divisor: '1'\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        - name: KUBE_CLIENT_BACKOFF_BASE\\n          value: \\\"1\\\"\\n        - name: KUBE_CLIENT_BACKOFF_DURATION\\n          value: \\\"120\\\"\\n        lifecycle:\\n          postStart:\\n            exec:\\n              command:\\n              - \\\"bash\\\"\\n              - \\\"-c\\\"\\n              - |\\n                    set -o errexit\\n                    set -o pipefail\\n                    set -o nounset\\n                    \\n                    # When running in AWS ENI mode, it's likely that 'aws-node' has\\n                    # had a chance to install SNAT iptables rules. These can result\\n                    # in dropped traffic, so we should attempt to remove them.\\n                    # We do it using a 'postStart' hook since this may need to run\\n                    # for nodes which might have already been init'ed but may still\\n                    # have dangling rules. This is safe because there are no\\n                    # dependencies on anything that is part of the startup script\\n                    # itself, and can be safely run multiple times per node (e.g. in\\n                    # case of a restart).\\n                    if [[ \\\"$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n                    then\\n                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'\\n                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore\\n                    fi\\n                    echo 'Done!'\\n                    \\n          preStop:\\n            exec:\\n              command:\\n              - /cni-uninstall.sh\\n        ports:\\n        - name: peer-service\\n          containerPort: 4244\\n          hostPort: 4244\\n          protocol: TCP\\n        - name: prometheus\\n          containerPort: 9962\\n          hostPort: 9962\\n          protocol: TCP\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - CHOWN\\n              - KILL\\n              - NET_ADMIN\\n              - NET_RAW\\n              - IPC_LOCK\\n              - SYS_ADMIN\\n              - SYS_RESOURCE\\n              - DAC_OVERRIDE\\n              - FOWNER\\n              - SETGID\\n              - SETUID\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n          mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n        # Unprivileged containers need to mount /proc/sys/net from the host\\n        # to have write access\\n        - mountPath: /host/proc/sys/net\\n          name: host-proc-sys-net\\n        # Unprivileged containers need to mount /proc/sys/kernel from the host\\n        # to have write access\\n        - mountPath: /host/proc/sys/kernel\\n          name: host-proc-sys-kernel\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          # Unprivileged containers can't set mount propagation to bidirectional\\n          # in this case we will mount the bpf fs from an init container that\\n          # is privileged and set the mount propagation from host to container\\n          # in Cilium.\\n          mountPropagation: HostToContainer\\n        # Check for duplicate mounts before mounting\\n        - name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n        - name: cilium-run\\n          mountPath: /var/run/cilium\\n        - name: cilium-netns\\n          mountPath: /var/run/cilium/netns\\n          mountPropagation: HostToContainer\\n        - name: etc-cni-netd\\n          mountPath: /host/etc/cni/net.d\\n        - name: clustermesh-secrets\\n          mountPath: /var/lib/cilium/clustermesh\\n          readOnly: true\\n          # Needed to be able to load kernel modules\\n        - name: lib-modules\\n          mountPath: /lib/modules\\n          readOnly: true\\n        - name: xtables-lock\\n          mountPath: /run/xtables.lock\\n        - name: tmp\\n          mountPath: /tmp\\n        \\n      initContainers:\\n      - name: config\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-dbg\\n        - build-config\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        volumeMounts:\\n        - name: tmp\\n          mountPath: /tmp\\n        terminationMessagePolicy: FallbackToLogsOnError\\n      - name: apply-sysctl-overwrites\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        env:\\n        - name: BIN_PATH\\n          value: /opt/cni/bin\\n        command:\\n        - sh\\n        - -ec\\n        # The statically linked Go program binary is invoked to avoid any\\n        # dependency on utilities like sh that can be missing on certain\\n        # distros installed on the underlying host. Copy the binary to the\\n        # same directory where we install cilium cni plugin so that exec permissions\\n        # are available.\\n        - |\\n          cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\\n          nsenter --mount=/hostproc/1/ns/mnt \\\"${BIN_PATH}/cilium-sysctlfix\\\";\\n          rm /hostbin/cilium-sysctlfix\\n        volumeMounts:\\n        - name: hostproc\\n          mountPath: /hostproc\\n        - name: cni-path\\n          mountPath: /hostbin\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - SYS_ADMIN\\n              - SYS_CHROOT\\n              - SYS_PTRACE\\n            drop:\\n              - ALL\\n      # Mount the bpf fs if it is not mounted. We will perform this task\\n      # from a privileged container because the mount propagation bidirectional\\n      # only works from privileged containers.\\n      - name: mount-bpf-fs\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        args:\\n        - 'mount | grep \\\"/sys/fs/bpf type bpf\\\" || mount -t bpf bpf /sys/fs/bpf'\\n        command:\\n        - /bin/bash\\n        - -c\\n        - --\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          privileged: true\\n        volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          mountPropagation: Bidirectional\\n      - name: clean-cilium-state\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - /init-container.sh\\n        env:\\n        - name: CILIUM_ALL_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: clean-cilium-state\\n              optional: true\\n        - name: CILIUM_BPF_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: clean-cilium-bpf-state\\n              optional: true\\n        - name: WRITE_CNI_CONF_WHEN_READY\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: write-cni-conf-when-ready\\n              optional: true\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - NET_ADMIN\\n              - SYS_ADMIN\\n              - SYS_RESOURCE\\n            drop:\\n              - ALL\\n        volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          # Required to mount cgroup filesystem from the host to cilium agent pod\\n        - name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n          mountPropagation: HostToContainer\\n        - name: cilium-run\\n          mountPath: /var/run/cilium # wait-for-kube-proxy\\n      # Install the CNI binaries in an InitContainer so we don't have a writable host mount in the agent\\n      - name: install-cni-binaries\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n          - \\\"/install-plugin.sh\\\"\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 10Mi\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n          - name: cni-path\\n            mountPath: /host/opt/cni/bin # .Values.cni.install\\n      restartPolicy: Always\\n      priorityClassName: system-node-critical\\n      serviceAccountName: \\\"cilium\\\"\\n      automountServiceAccountToken: true\\n      terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      tolerations:\\n        - operator: Exists\\n      volumes:\\n        # For sharing configuration between the \\\"config\\\" initContainer and the agent\\n      - name: tmp\\n        emptyDir: {}\\n        # To keep state between restarts / upgrades\\n      - name: cilium-run\\n        hostPath:\\n          path: /var/run/cilium\\n          type: DirectoryOrCreate\\n        # To exec into pod network namespaces\\n      - name: cilium-netns\\n        hostPath:\\n          path: /var/run/netns\\n          type: DirectoryOrCreate\\n        # To keep state between restarts / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n          path: /sys/fs/bpf\\n          type: DirectoryOrCreate\\n      # To mount cgroup2 filesystem on the host or apply sysctlfix\\n      - name: hostproc\\n        hostPath:\\n          path: /proc\\n          type: Directory\\n      # To keep state between restarts / upgrades for cgroup2 filesystem\\n      - name: cilium-cgroup\\n        hostPath:\\n          path: /sys/fs/cgroup\\n          type: DirectoryOrCreate\\n      # To install cilium cni plugin in the host\\n      - name: cni-path\\n        hostPath:\\n          path:  /opt/cni/bin\\n          type: DirectoryOrCreate\\n        # To install cilium cni configuration in the host\\n      - name: etc-cni-netd\\n        hostPath:\\n          path: /etc/cni/net.d\\n          type: DirectoryOrCreate\\n        # To be able to load kernel modules\\n      - name: lib-modules\\n        hostPath:\\n          path: /lib/modules\\n        # To access iptables concurrently with other processes (e.g. kube-proxy)\\n      - name: xtables-lock\\n        hostPath:\\n          path: /run/xtables.lock\\n          type: FileOrCreate\\n      # Sharing socket with Cilium Envoy on the same node by using a host path\\n      - name: envoy-sockets\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/sockets\\\"\\n          type: DirectoryOrCreate\\n        # To read the clustermesh configuration\\n      - name: clustermesh-secrets\\n        projected:\\n          # note: the leading zero means this number is in octal representation: do not remove it\\n          defaultMode: 0400\\n          sources:\\n          - secret:\\n              name: cilium-clustermesh\\n              optional: true\\n              # note: items are not explicitly listed here, since the entries of this secret\\n              # depend on the peers configured, and that would cause a restart of all agents\\n              # at every addition/removal. Leaving the field empty makes each secret entry\\n              # to be automatically projected into the volume as a file whose name is the key.\\n          - secret:\\n              name: clustermesh-apiserver-remote-cert\\n              optional: true\\n              items:\\n              - key: tls.key\\n                path: common-etcd-client.key\\n              - key: tls.crt\\n                path: common-etcd-client.crt\\n              - key: ca.crt\\n                path: common-etcd-client-ca.crt\\n          # note: we configure the volume for the kvstoremesh-specific certificate\\n          # regardless of whether KVStoreMesh is enabled or not, so that it can be\\n          # automatically mounted in case KVStoreMesh gets subsequently enabled,\\n          # without requiring an agent restart.\\n          - secret:\\n              name: clustermesh-apiserver-local-cert\\n              optional: true\\n              items:\\n              - key: tls.key\\n                path: local-etcd-client.key\\n              - key: tls.crt\\n                path: local-etcd-client.crt\\n              - key: ca.crt\\n                path: local-etcd-client-ca.crt\\n      - name: host-proc-sys-net\\n        hostPath:\\n          path: /proc/sys/net\\n          type: Directory\\n      - name: host-proc-sys-kernel\\n        hostPath:\\n          path: /proc/sys/kernel\\n          type: Directory\\n---\\n# Source: cilium/templates/cilium-envoy/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: cilium-envoy\\n  namespace: kube-system\\n  labels:\\n    k8s-app: cilium-envoy\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-envoy\\n    name: cilium-envoy\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cilium-envoy\\n  updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n      labels:\\n        k8s-app: cilium-envoy\\n        name: cilium-envoy\\n        app.kubernetes.io/name: cilium-envoy\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n        appArmorProfile:\\n          type: Unconfined\\n      containers:\\n      - name: cilium-envoy\\n        image: \\\"quay.io/cilium/cilium-envoy:v1.34.4-1754895458-68cffdfa568b6b226d70a7ef81fc65dda3b890bf@sha256:247e908700012f7ef56f75908f8c965215c26a27762f296068645eb55450bda2\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - /usr/bin/cilium-envoy-starter\\n        args:\\n        - '--'\\n        - '-c /var/run/cilium/envoy/bootstrap-config.json'\\n        - '--base-id 0'\\n        - '--log-level info'\\n        startupProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          failureThreshold: 105\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds: 5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        ports:\\n        - name: envoy-metrics\\n          containerPort: 9964\\n          hostPort: 9964\\n          protocol: TCP\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - NET_ADMIN\\n              - SYS_ADMIN\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n          mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n        - name: envoy-artifacts\\n          mountPath: /var/run/cilium/envoy/artifacts\\n          readOnly: true\\n        - name: envoy-config\\n          mountPath: /var/run/cilium/envoy/\\n          readOnly: true\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          mountPropagation: HostToContainer\\n      restartPolicy: Always\\n      priorityClassName: system-node-critical\\n      serviceAccountName: \\\"cilium-envoy\\\"\\n      automountServiceAccountToken: true\\n      terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n        nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            nodeSelectorTerms:\\n            - matchExpressions:\\n              - key: cilium.io/no-schedule\\n                operator: NotIn\\n                values:\\n                - \\\"true\\\"\\n        podAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium\\n            topologyKey: kubernetes.io/hostname\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium-envoy\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      tolerations:\\n        - operator: Exists\\n      volumes:\\n      - name: envoy-sockets\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/sockets\\\"\\n          type: DirectoryOrCreate\\n      - name: envoy-artifacts\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/artifacts\\\"\\n          type: DirectoryOrCreate\\n      - name: envoy-config\\n        configMap:\\n          name: \\\"cilium-envoy-config\\\"\\n          # note: the leading zero means this number is in octal representation: do not remove it\\n          defaultMode: 0400\\n          items:\\n            - key: bootstrap-config.json\\n              path: bootstrap-config.json\\n        # To keep state between restarts / upgrades\\n        # To keep state between restarts / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n          path: /sys/fs/bpf\\n          type: DirectoryOrCreate\\n---\\n# Source: cilium/templates/cilium-operator/deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: cilium-operator\\n  namespace: kube-system\\n  labels:\\n    io.cilium/app: operator\\n    name: cilium-operator\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-operator\\nspec:\\n  # See docs on ServerCapabilities.LeasesResourceLock in file pkg/k8s/version/version.go\\n  # for more details.\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      io.cilium/app: operator\\n      name: cilium-operator\\n  # ensure operator update on single node k8s clusters, by using rolling update with maxUnavailable=100% in case\\n  # of one replica and no user configured Recreate strategy.\\n  # otherwise an update might get stuck due to the default maxUnavailable=50% in combination with the\\n  # podAntiAffinity which prevents deployments of multiple operator replicas on the same node.\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 25%\\n      maxUnavailable: 100%\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n        prometheus.io/port: \\\"9963\\\"\\n        prometheus.io/scrape: \\\"true\\\"\\n      labels:\\n        io.cilium/app: operator\\n        name: cilium-operator\\n        app.kubernetes.io/part-of: cilium\\n        app.kubernetes.io/name: cilium-operator\\n    spec:\\n      securityContext:\\n        seccompProfile:\\n          type: RuntimeDefault\\n      containers:\\n      - name: cilium-operator\\n        image: \\\"quay.io/cilium/operator-generic:v1.18.1@sha256:97f4553afa443465bdfbc1cc4927c93f16ac5d78e4dd2706736e7395382201bc\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-operator-generic\\n        args:\\n        - --config-dir=/tmp/cilium/config-map\\n        - --debug=$(CILIUM_DEBUG)\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_DEBUG\\n          valueFrom:\\n            configMapKeyRef:\\n              key: debug\\n              name: cilium-config\\n              optional: true\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        ports:\\n        - name: prometheus\\n          containerPort: 9963\\n          hostPort: 9963\\n          protocol: TCP\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds: 60\\n          periodSeconds: 10\\n          timeoutSeconds: 3\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds: 0\\n          periodSeconds: 5\\n          timeoutSeconds: 3\\n          failureThreshold: 5\\n        volumeMounts:\\n        - name: cilium-config-path\\n          mountPath: /tmp/cilium/config-map\\n          readOnly: true\\n        securityContext:\\n          allowPrivilegeEscalation: false\\n          capabilities:\\n            drop:\\n            - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n      hostNetwork: true\\n      restartPolicy: Always\\n      priorityClassName: system-cluster-critical\\n      serviceAccountName: \\\"cilium-operator\\\"\\n      automountServiceAccountToken: true\\n      # In HA mode, cilium-operator pods must not be scheduled on the same\\n      # node as they will clash with each other.\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                io.cilium/app: operator\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      tolerations:\\n        - operator: Exists\\n        - key: node.cilium.io/agent-not-ready\\n          operator: Exists\\n      \\n      volumes:\\n        # To read the configuration from the config map\\n      - name: cilium-config-path\\n        configMap:\\n          name: cilium-config\\n---\\n# Source: cilium/templates/cilium-envoy/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-envoy\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-configmap.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-config\\n  namespace: kube-system\\ndata:\\n\\n  # Identity allocation mode selects how identities are shared between cilium\\n  # nodes by setting how they are stored. The options are \\\"crd\\\", \\\"kvstore\\\" or\\n  # \\\"doublewrite-readkvstore\\\" / \\\"doublewrite-readcrd\\\".\\n  # - \\\"crd\\\" stores identities in kubernetes as CRDs (custom resource definition).\\n  #   These can be queried with:\\n  #     kubectl get ciliumid\\n  # - \\\"kvstore\\\" stores identities in an etcd kvstore, that is\\n  #   configured below. Cilium versions before 1.6 supported only the kvstore\\n  #   backend. Upgrades from these older cilium versions should continue using\\n  #   the kvstore by commenting out the identity-allocation-mode below, or\\n  #   setting it to \\\"kvstore\\\".\\n  # - \\\"doublewrite\\\" modes store identities in both the kvstore and CRDs. This is useful\\n  #   for seamless migrations from the kvstore mode to the crd mode. Consult the\\n  #   documentation for more information on how to perform the migration.\\n  identity-allocation-mode: crd\\n\\n  identity-heartbeat-timeout: \\\"30m0s\\\"\\n  identity-gc-interval: \\\"15m0s\\\"\\n  cilium-endpoint-gc-interval: \\\"5m0s\\\"\\n  nodes-gc-interval: \\\"5m0s\\\"\\n\\n  # If you want to run cilium in debug mode change this value to true\\n  debug: \\\"false\\\"\\n  debug-verbose: \\\"\\\"\\n  metrics-sampling-interval: \\\"5m\\\"\\n  # The agent can be put into the following three policy enforcement modes\\n  # default, always and never.\\n  # https://docs.cilium.io/en/latest/security/policy/intro/#policy-enforcement-modes\\n  enable-policy: \\\"default\\\"\\n  policy-cidr-match-mode: \\\"\\\"\\n  # If you want metrics enabled in all of your Cilium agents, set the port for\\n  # which the Cilium agents will have their metrics exposed.\\n  # This option deprecates the \\\"prometheus-serve-addr\\\" in the\\n  # \\\"cilium-metrics-config\\\" ConfigMap\\n  # NOTE that this will open the port on ALL nodes where Cilium pods are\\n  # scheduled.\\n  prometheus-serve-addr: \\\":9962\\\"\\n  # A space-separated list of controller groups for which to enable metrics.\\n  # The special values of \\\"all\\\" and \\\"none\\\" are supported.\\n  controller-group-metrics:\\n    write-cni-file\\n    sync-host-ips\\n    sync-lb-maps-with-k8s-services\\n  # If you want metrics enabled in cilium-operator, set the port for\\n  # which the Cilium Operator will have their metrics exposed.\\n  # NOTE that this will open the port on the nodes where Cilium operator pod\\n  # is scheduled.\\n  operator-prometheus-serve-addr: \\\":9963\\\"\\n  enable-metrics: \\\"true\\\"\\n  enable-policy-secrets-sync: \\\"true\\\"\\n  policy-secrets-only-from-secrets-namespace: \\\"true\\\"\\n  policy-secrets-namespace: \\\"cilium-secrets\\\"\\n\\n  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4\\n  # address.\\n  enable-ipv4: \\\"true\\\"\\n\\n  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6\\n  # address.\\n  enable-ipv6: \\\"false\\\"\\n  # Users who wish to specify their own custom CNI configuration file must set\\n  # custom-cni-conf to \\\"true\\\", otherwise Cilium may overwrite the configuration.\\n  custom-cni-conf: \\\"false\\\"\\n  enable-bpf-clock-probe: \\\"false\\\"\\n  # If you want cilium monitor to aggregate tracing for packets, set this level\\n  # to \\\"low\\\", \\\"medium\\\", or \\\"maximum\\\". The higher the level, the less packets\\n  # that will be seen in monitor output.\\n  monitor-aggregation: medium\\n\\n  # The monitor aggregation interval governs the typical time between monitor\\n  # notification events for each allowed connection.\\n  #\\n  # Only effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n  monitor-aggregation-interval: \\\"5s\\\"\\n\\n  # The monitor aggregation flags determine which TCP flags which, upon the\\n  # first observation, cause monitor notifications to be generated.\\n  #\\n  # Only effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n  monitor-aggregation-flags: all\\n  # Specifies the ratio (0.0-1.0] of total system memory to use for dynamic\\n  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.\\n  bpf-map-dynamic-size-ratio: \\\"0.0025\\\"\\n  enable-host-legacy-routing: \\\"false\\\"\\n  # bpf-policy-map-max specifies the maximum number of entries in endpoint\\n  # policy map (per endpoint)\\n  bpf-policy-map-max: \\\"16384\\\"\\n  # bpf-policy-stats-map-max specifies the maximum number of entries in global\\n  # policy stats map\\n  bpf-policy-stats-map-max: \\\"65536\\\"\\n  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,\\n  # backend and affinity maps.\\n  bpf-lb-map-max: \\\"65536\\\"\\n  bpf-lb-external-clusterip: \\\"false\\\"\\n  bpf-lb-source-range-all-types: \\\"false\\\"\\n  bpf-lb-algorithm-annotation: \\\"false\\\"\\n  bpf-lb-mode-annotation: \\\"false\\\"\\n\\n  bpf-distributed-lru: \\\"false\\\"\\n  bpf-events-drop-enabled: \\\"true\\\"\\n  bpf-events-policy-verdict-enabled: \\\"true\\\"\\n  bpf-events-trace-enabled: \\\"true\\\"\\n\\n  # Pre-allocation of map entries allows per-packet latency to be reduced, at\\n  # the expense of up-front memory allocation for the entries in the maps. The\\n  # default value below will minimize memory usage in the default installation;\\n  # users who are sensitive to latency may consider setting this to \\\"true\\\".\\n  #\\n  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore\\n  # this option and behave as though it is set to \\\"true\\\".\\n  #\\n  # If this value is modified, then during the next Cilium startup the restore\\n  # of existing endpoints and tracking of ongoing connections may be disrupted.\\n  # As a result, reply packets may be dropped and the load-balancing decisions\\n  # for established connections may change.\\n  #\\n  # If this option is set to \\\"false\\\" during an upgrade from 1.3 or earlier to\\n  # 1.4 or later, then it may cause one-time disruptions during the upgrade.\\n  preallocate-bpf-maps: \\\"false\\\"\\n\\n  # Name of the cluster. Only relevant when building a mesh of clusters.\\n  cluster-name: \\\"default\\\"\\n  # Unique ID of the cluster. Must be unique across all conneted clusters and\\n  # in the range of 1 and 255. Only relevant when building a mesh of clusters.\\n  cluster-id: \\\"0\\\"\\n\\n  # Encapsulation mode for communication between nodes\\n  # Possible values:\\n  #   - disabled\\n  #   - vxlan (default)\\n  #   - geneve\\n\\n  routing-mode: \\\"native\\\"\\n  tunnel-protocol: \\\"vxlan\\\"\\n  tunnel-source-port-range: \\\"0-0\\\"\\n  service-no-backend-response: \\\"reject\\\"\\n\\n\\n  # Enables L7 proxy for L7 policy enforcement and visibility\\n  enable-l7-proxy: \\\"true\\\"\\n  enable-ipv4-masquerade: \\\"true\\\"\\n  enable-ipv4-big-tcp: \\\"false\\\"\\n  enable-ipv6-big-tcp: \\\"false\\\"\\n  enable-ipv6-masquerade: \\\"true\\\"\\n  enable-tcx: \\\"true\\\"\\n  datapath-mode: \\\"veth\\\"\\n  enable-bpf-masquerade: \\\"true\\\"\\n  enable-masquerade-to-route-source: \\\"false\\\"\\n  enable-wireguard: \\\"true\\\"\\n  wireguard-persistent-keepalive: \\\"0s\\\"\\n\\n  enable-xt-socket-fallback: \\\"true\\\"\\n  install-no-conntrack-iptables-rules: \\\"true\\\"\\n  iptables-random-fully: \\\"false\\\"\\n\\n  auto-direct-node-routes: \\\"false\\\"\\n  direct-routing-skip-unreachable: \\\"false\\\"\\n\\n\\n  ipv4-native-routing-cidr: 10.0.0.0/16\\n\\n  kube-proxy-replacement: \\\"true\\\"\\n  kube-proxy-replacement-healthz-bind-address: \\\"0.0.0.0:10256\\\"\\n  bpf-lb-sock: \\\"false\\\"\\n  nodeport-addresses: \\\"\\\"\\n  enable-health-check-nodeport: \\\"true\\\"\\n  enable-health-check-loadbalancer-ip: \\\"false\\\"\\n  node-port-bind-protection: \\\"true\\\"\\n  enable-auto-protect-node-port-range: \\\"true\\\"\\n  bpf-lb-acceleration: \\\"native\\\"\\n  enable-svc-source-range-check: \\\"true\\\"\\n  enable-l2-neigh-discovery: \\\"false\\\"\\n  k8s-require-ipv4-pod-cidr: \\\"true\\\"\\n  k8s-require-ipv6-pod-cidr: \\\"false\\\"\\n  enable-k8s-networkpolicy: \\\"true\\\"\\n  enable-endpoint-lockdown-on-policy-overflow: \\\"false\\\"\\n  # Tell the agent to generate and write a CNI configuration file\\n  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist\\n  cni-exclusive: \\\"true\\\"\\n  cni-log-file: \\\"/var/run/cilium/cilium-cni.log\\\"\\n  enable-endpoint-health-checking: \\\"true\\\"\\n  enable-health-checking: \\\"true\\\"\\n  health-check-icmp-failure-threshold: \\\"3\\\"\\n  enable-well-known-identities: \\\"false\\\"\\n  enable-node-selector-labels: \\\"false\\\"\\n  synchronize-k8s-nodes: \\\"true\\\"\\n  operator-api-serve-addr: \\\"127.0.0.1:9234\\\"\\n\\n  enable-hubble: \\\"false\\\"\\n  ipam: \\\"kubernetes\\\"\\n  ipam-cilium-node-update-rate: \\\"15s\\\"\\n\\n  default-lb-service-ipam: \\\"lbipam\\\"\\n  egress-gateway-reconciliation-trigger-interval: \\\"1s\\\"\\n  enable-vtep: \\\"false\\\"\\n  vtep-endpoint: \\\"\\\"\\n  vtep-cidr: \\\"\\\"\\n  vtep-mask: \\\"\\\"\\n  vtep-mac: \\\"\\\"\\n  procfs: \\\"/host/proc\\\"\\n  bpf-root: \\\"/sys/fs/bpf\\\"\\n  cgroup-root: \\\"/sys/fs/cgroup\\\"\\n\\n  identity-management-mode: \\\"agent\\\"\\n  enable-sctp: \\\"false\\\"\\n  remove-cilium-node-taints: \\\"true\\\"\\n  set-cilium-node-taints: \\\"true\\\"\\n  set-cilium-is-up-condition: \\\"true\\\"\\n  unmanaged-pod-watcher-interval: \\\"15\\\"\\n  # explicit setting gets precedence\\n  dnsproxy-enable-transparent-mode: \\\"true\\\"\\n  dnsproxy-socket-linger-timeout: \\\"10\\\"\\n  tofqdns-dns-reject-response-code: \\\"refused\\\"\\n  tofqdns-enable-dns-compression: \\\"true\\\"\\n  tofqdns-endpoint-max-ip-per-hostname: \\\"1000\\\"\\n  tofqdns-idle-connection-grace-period: \\\"0s\\\"\\n  tofqdns-max-deferred-connection-deletes: \\\"10000\\\"\\n  tofqdns-proxy-response-max-delay: \\\"100ms\\\"\\n  tofqdns-preallocate-identities:  \\\"true\\\"\\n  agent-not-ready-taint-key: \\\"node.cilium.io/agent-not-ready\\\"\\n\\n  mesh-auth-enabled: \\\"true\\\"\\n  mesh-auth-queue-size: \\\"1024\\\"\\n  mesh-auth-rotated-identities-queue-size: \\\"1024\\\"\\n  mesh-auth-gc-interval: \\\"5m0s\\\"\\n\\n  proxy-xff-num-trusted-hops-ingress: \\\"0\\\"\\n  proxy-xff-num-trusted-hops-egress: \\\"0\\\"\\n  proxy-connect-timeout: \\\"2\\\"\\n  proxy-initial-fetch-timeout: \\\"30\\\"\\n  proxy-max-requests-per-connection: \\\"0\\\"\\n  proxy-max-connection-duration-seconds: \\\"0\\\"\\n  proxy-idle-timeout-seconds: \\\"60\\\"\\n  proxy-max-concurrent-retries: \\\"128\\\"\\n  http-retry-count: \\\"3\\\"\\n\\n  external-envoy-proxy: \\\"true\\\"\\n  envoy-base-id: \\\"0\\\"\\n  envoy-access-log-buffer-size: \\\"4096\\\"\\n  envoy-keep-cap-netbindservice: \\\"false\\\"\\n  max-connected-clusters: \\\"255\\\"\\n  clustermesh-enable-endpoint-sync: \\\"false\\\"\\n  clustermesh-enable-mcs-api: \\\"false\\\"\\n  policy-default-local-cluster: \\\"false\\\"\\n\\n  nat-map-stats-entries: \\\"32\\\"\\n  nat-map-stats-interval: \\\"30s\\\"\\n  enable-internal-traffic-policy: \\\"true\\\"\\n  enable-lb-ipam: \\\"true\\\"\\n  enable-non-default-deny-policies: \\\"true\\\"\\n  enable-source-ip-verification: \\\"true\\\"\\n\\n# Extra config allows adding arbitrary properties to the cilium config.\\n# By putting it at the end of the ConfigMap, it's also possible to override existing properties.\\n---\\n# Source: cilium/templates/cilium-envoy/configmap.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-envoy-config\\n  namespace: kube-system\\ndata:\\n  # Keep the key name as bootstrap-config.json to avoid breaking changes\\n  bootstrap-config.json: |\\n    {\\\"admin\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}},\\\"applicationLogConfig\\\":{\\\"logFormat\\\":{\\\"textFormat\\\":\\\"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v\\\"}},\\\"bootstrapExtensions\\\":[{\\\"name\\\":\\\"envoy.bootstrap.internal_listener\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener\\\"}}],\\\"dynamicResources\\\":{\\\"cdsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"},\\\"ldsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"}},\\\"node\\\":{\\\"cluster\\\":\\\"ingress-cluster\\\",\\\"id\\\":\\\"host~127.0.0.1~no-id~localdomain\\\"},\\\"overloadManager\\\":{\\\"resourceMonitors\\\":[{\\\"name\\\":\\\"envoy.resource_monitors.global_downstream_max_connections\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig\\\",\\\"max_active_downstream_connections\\\":\\\"50000\\\"}}]},\\\"staticResources\\\":{\\\"clusters\\\":[{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/xds.sock\\\"}}}}]}]},\\\"name\\\":\\\"xds-grpc-cilium\\\",\\\"type\\\":\\\"STATIC\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"explicitHttpConfig\\\":{\\\"http2ProtocolOptions\\\":{}}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"/envoy-admin\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}}}]}]},\\\"name\\\":\\\"/envoy-admin\\\",\\\"type\\\":\\\"STATIC\\\"}],\\\"listeners\\\":[{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"0.0.0.0\\\",\\\"portValue\\\":9964}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtualHosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/metrics\\\"},\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/stats/prometheus\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-prometheus-metrics-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-prometheus-metrics-listener\\\"},{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"127.0.0.1\\\",\\\"portValue\\\":9878}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtual_hosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"health\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/healthz\\\"},\\\"name\\\":\\\"health\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/ready\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-health-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-health-listener\\\"}]}}\\n---\\n# Source: cilium/templates/cilium-agent/clusterrole.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cilium\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - networking.k8s.io\\n  resources:\\n  - networkpolicies\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - discovery.k8s.io\\n  resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - namespaces\\n  - services\\n  - pods\\n  - endpoints\\n  - nodes\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - list\\n  - watch\\n  # This is used when validating policies in preflight. This will need to stay\\n  # until we figure out how to avoid \\\"get\\\" inside the preflight, and then\\n  # should be removed ideally.\\n  - get\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools\\n  - ciliumbgppeeringpolicies\\n  - ciliumbgpnodeconfigs\\n  - ciliumbgpadvertisements\\n  - ciliumbgppeerconfigs\\n  - ciliumclusterwideenvoyconfigs\\n  - ciliumclusterwidenetworkpolicies\\n  - ciliumegressgatewaypolicies\\n  - ciliumendpoints\\n  - ciliumendpointslices\\n  - ciliumenvoyconfigs\\n  - ciliumidentities\\n  - ciliumlocalredirectpolicies\\n  - ciliumnetworkpolicies\\n  - ciliumnodes\\n  - ciliumnodeconfigs\\n  - ciliumcidrgroups\\n  - ciliuml2announcementpolicies\\n  - ciliumpodippools\\n  verbs:\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumidentities\\n  - ciliumendpoints\\n  - ciliumnodes\\n  verbs:\\n  - create\\n- apiGroups:\\n  - cilium.io\\n  # To synchronize garbage collection of such resources\\n  resources:\\n  - ciliumidentities\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints\\n  verbs:\\n  - delete\\n  - get\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes\\n  - ciliumnodes/status\\n  verbs:\\n  - get\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints/status\\n  - ciliumendpoints\\n  - ciliuml2announcementpolicies/status\\n  - ciliumbgpnodeconfigs/status\\n  verbs:\\n  - patch\\n---\\n# Source: cilium/templates/cilium-operator/clusterrole.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cilium-operator\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - pods\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n  # to automatically delete [core|kube]dns pods so that are starting to being\\n  # managed by Cilium\\n  - delete\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - configmaps\\n  resourceNames:\\n  - cilium-config\\n  verbs:\\n   # allow patching of the configmap to set annotations\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - nodes\\n  verbs:\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # To remove node taints\\n  - nodes\\n  # To set NetworkUnavailable false on startup\\n  - nodes/status\\n  verbs:\\n  - patch\\n- apiGroups:\\n  - discovery.k8s.io\\n  resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to perform LB IP allocation for BGP\\n  - services/status\\n  verbs:\\n  - update\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to check apiserver connectivity\\n  - namespaces\\n  - secrets\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to perform the translation of a CNP that contains `ToGroup` to its endpoints\\n  - services\\n  - endpoints\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnetworkpolicies\\n  - ciliumclusterwidenetworkpolicies\\n  verbs:\\n  # Create auto-generated CNPs and CCNPs from Policies that have 'toGroups'\\n  - create\\n  - update\\n  - deletecollection\\n  # To update the status of the CNPs and CCNPs\\n  - patch\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnetworkpolicies/status\\n  - ciliumclusterwidenetworkpolicies/status\\n  verbs:\\n  # Update the auto-generated CNPs and CCNPs status.\\n  - patch\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints\\n  - ciliumidentities\\n  verbs:\\n  # To perform garbage collection of such resources\\n  - delete\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumidentities\\n  verbs:\\n  # To synchronize garbage collection of such resources\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes\\n  verbs:\\n  - create\\n  - update\\n  - get\\n  - list\\n  - watch\\n    # To perform CiliumNode garbage collector\\n  - delete\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpointslices\\n  - ciliumenvoyconfigs\\n  - ciliumbgppeerconfigs\\n  - ciliumbgpadvertisements\\n  - ciliumbgpnodeconfigs\\n  verbs:\\n  - create\\n  - update\\n  - get\\n  - list\\n  - watch\\n  - delete\\n  - patch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumbgpclusterconfigs/status\\n  - ciliumbgppeerconfigs/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - create\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - update\\n  resourceNames:\\n  - ciliumloadbalancerippools.cilium.io\\n  - ciliumbgppeeringpolicies.cilium.io\\n  - ciliumbgpclusterconfigs.cilium.io\\n  - ciliumbgppeerconfigs.cilium.io\\n  - ciliumbgpadvertisements.cilium.io\\n  - ciliumbgpnodeconfigs.cilium.io\\n  - ciliumbgpnodeconfigoverrides.cilium.io\\n  - ciliumclusterwideenvoyconfigs.cilium.io\\n  - ciliumclusterwidenetworkpolicies.cilium.io\\n  - ciliumegressgatewaypolicies.cilium.io\\n  - ciliumendpoints.cilium.io\\n  - ciliumendpointslices.cilium.io\\n  - ciliumenvoyconfigs.cilium.io\\n  - ciliumidentities.cilium.io\\n  - ciliumlocalredirectpolicies.cilium.io\\n  - ciliumnetworkpolicies.cilium.io\\n  - ciliumnodes.cilium.io\\n  - ciliumnodeconfigs.cilium.io\\n  - ciliumcidrgroups.cilium.io\\n  - ciliuml2announcementpolicies.cilium.io\\n  - ciliumpodippools.cilium.io\\n  - ciliumgatewayclassconfigs.cilium.io\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools\\n  - ciliumpodippools\\n  - ciliumbgppeeringpolicies\\n  - ciliumbgpclusterconfigs\\n  - ciliumbgpnodeconfigoverrides\\n  - ciliumbgppeerconfigs\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n    - cilium.io\\n  resources:\\n    - ciliumpodippools\\n  verbs:\\n    - create\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools/status\\n  verbs:\\n  - patch\\n# For cilium-operator running in HA mode.\\n#\\n# Cilium operator running in HA mode requires the use of ResourceLock for Leader Election\\n# between multiple running instances.\\n# The preferred way of doing this is to use LeasesResourceLock as edits to Leases are less\\n# common and fewer objects in the cluster watch \\\"all Leases\\\".\\n- apiGroups:\\n  - coordination.k8s.io\\n  resources:\\n  - leases\\n  verbs:\\n  - create\\n  - get\\n  - update\\n---\\n# Source: cilium/templates/cilium-agent/clusterrolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cilium\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/clusterrolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium-operator\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cilium-operator\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n\\n\"\n        - name: hcloud-ccm\n          contents: \"---\\n# Source: hcloud-cloud-controller-manager/templates/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n  namespace: kube-system\\n---\\n# Source: hcloud-cloud-controller-manager/templates/clusterrolebinding.yaml\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: \\\"system:hcloud-cloud-controller-manager\\\"\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cluster-admin\\nsubjects:\\n  - kind: ServiceAccount\\n    name: hcloud-cloud-controller-manager\\n    namespace: kube-system\\n---\\n# Source: hcloud-cloud-controller-manager/templates/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n  namespace: kube-system\\nspec:\\n  revisionHistoryLimit: 2\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n      app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n        app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\\n    spec:\\n      serviceAccountName: hcloud-cloud-controller-manager\\n      dnsPolicy: Default\\n      tolerations:\\n        # Allow HCCM itself to schedule on nodes that have not yet been initialized by HCCM.\\n        - key: \\\"node.cloudprovider.kubernetes.io/uninitialized\\\"\\n          value: \\\"true\\\"\\n          effect: \\\"NoSchedule\\\"\\n        - key: \\\"CriticalAddonsOnly\\\"\\n          operator: \\\"Exists\\\"\\n\\n        # Allow HCCM to schedule on control plane nodes.\\n        - key: \\\"node-role.kubernetes.io/master\\\"\\n          effect: NoSchedule\\n          operator: Exists\\n        - key: \\\"node-role.kubernetes.io/control-plane\\\"\\n          effect: NoSchedule\\n          operator: Exists\\n\\n        - key: \\\"node.kubernetes.io/not-ready\\\"\\n          effect: \\\"NoExecute\\\"\\n      nodeSelector:\\n        \\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      hostNetwork: true\\n      containers:\\n        - name: hcloud-cloud-controller-manager\\n          command:\\n            - \\\"/bin/hcloud-cloud-controller-manager\\\"\\n            - \\\"--allow-untagged-cloud\\\"\\n            - \\\"--cloud-provider=hcloud\\\"\\n            - \\\"--route-reconciliation-period=30s\\\"\\n            - \\\"--webhook-secure-port=0\\\"\\n            - \\\"--allocate-node-cidrs=true\\\"\\n            - \\\"--cluster-cidr=10.0.128.0/17\\\"\\n          env:\\n            - name: HCLOUD_LOAD_BALANCERS_DISABLE_PRIVATE_INGRESS\\n              value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_ENABLED\\n              value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_LOCATION\\n              value: hel1\\n            - name: HCLOUD_LOAD_BALANCERS_USE_PRIVATE_IP\\n              value: \\\"true\\\"\\n            - name: HCLOUD_NETWORK_ROUTES_ENABLED\\n              value: \\\"true\\\"\\n            - name: HCLOUD_TOKEN\\n              valueFrom:\\n                secretKeyRef:\\n                  key: token\\n                  name: hcloud\\n            - name: ROBOT_PASSWORD\\n              valueFrom:\\n                secretKeyRef:\\n                  key: robot-password\\n                  name: hcloud\\n                  optional: true\\n            - name: ROBOT_USER\\n              valueFrom:\\n                secretKeyRef:\\n                  key: robot-user\\n                  name: hcloud\\n                  optional: true\\n            - name: HCLOUD_NETWORK\\n              valueFrom:\\n                secretKeyRef:\\n                  key: network\\n                  name: hcloud\\n          image: docker.io/hetznercloud/hcloud-cloud-controller-manager:v1.26.0 # x-releaser-pleaser-version\\n          ports:\\n            - name: metrics\\n              containerPort: 8233\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 50Mi\\n      priorityClassName: system-cluster-critical\\n\"\n        - name: hcloud-csi\n          contents: \"\\\"apiVersion\\\": \\\"v1\\\"\\n\\\"data\\\":\\n  \\\"encryption-passphrase\\\": \\\"aXlJaUllVnpKY1l0cFFyWCZIQURkQ1lzWnd2RnRTTXB0dUgmRG51Q2VxJldic3Fl\\\"\\n\\\"kind\\\": \\\"Secret\\\"\\n\\\"metadata\\\":\\n  \\\"name\\\": \\\"hcloud-csi-secret\\\"\\n  \\\"namespace\\\": \\\"kube-system\\\"\\n\\\"type\\\": \\\"Opaque\\\"\\n\\n---\\n---\\n# Source: hcloud-csi/templates/controller/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nautomountServiceAccountToken: true\\n---\\n# Source: hcloud-csi/templates/core/storageclass.yaml\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: hcloud-volumes-encrypted-xfs\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \\\"true\\\"\\nprovisioner: csi.hetzner.cloud\\nvolumeBindingMode: WaitForFirstConsumer\\nallowVolumeExpansion: true\\nreclaimPolicy: \\\"Retain\\\"\\nparameters:\\n  csi.storage.k8s.io/fstype: xfs\\n  csi.storage.k8s.io/node-publish-secret-name: hcloud-csi-secret\\n  csi.storage.k8s.io/node-publish-secret-namespace: kube-system\\n  fsFormatOption: -i nrext64=1\\n---\\n# Source: hcloud-csi/templates/controller/clusterrole.yaml\\nkind: ClusterRole\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: hcloud-csi-controller\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nrules:\\n  # attacher\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [nodes]\\n    verbs: [get, list, watch]\\n  - apiGroups: [csi.storage.k8s.io]\\n    resources: [csinodeinfos]\\n    verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [csinodes]\\n    verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [volumeattachments]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [volumeattachments/status]\\n    verbs: [patch]\\n  # provisioner\\n  - apiGroups: [\\\"\\\"]\\n    resources: [secrets]\\n    verbs: [get, list]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n    verbs: [get, list, watch, create, delete, patch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumeclaims, persistentvolumeclaims/status]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [storageclasses]\\n    verbs: [get, list, watch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [events]\\n    verbs: [list, watch, create, update, patch]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n    resources: [volumesnapshots]\\n    verbs: [get, list]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n    resources: [volumesnapshotcontents]\\n    verbs: [get, list]\\n  # resizer\\n  - apiGroups: [\\\"\\\"]\\n    resources: [pods]\\n    verbs: [get, list, watch]\\n  # node\\n  - apiGroups: [\\\"\\\"]\\n    resources: [events]\\n    verbs: [get, list, watch, create, update, patch]\\n---\\n# Source: hcloud-csi/templates/controller/clusterrolebinding.yaml\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: hcloud-csi-controller\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: hcloud-csi-controller\\nsubjects:\\n  - kind: ServiceAccount\\n    name: hcloud-csi-controller\\n    namespace: \\\"kube-system\\\"\\n---\\n# Source: hcloud-csi/templates/node/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: hcloud-csi-node\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: node\\n    app: hcloud-csi\\nspec:\\n  updateStrategy:\\n    type: RollingUpdate\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance: hcloud-csi\\n      app.kubernetes.io/component: node\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart: hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by: Helm\\n        app.kubernetes.io/component: node\\n    spec:\\n      \\n      affinity:\\n        nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            nodeSelectorTerms:\\n            - matchExpressions:\\n              - key: instance.hetzner.cloud/is-root-server\\n                operator: NotIn\\n                values:\\n                - \\\"true\\\"\\n              - key: instance.hetzner.cloud/provided-by\\n                operator: NotIn\\n                values:\\n                - robot\\n      tolerations:\\n        - effect: NoExecute\\n          operator: Exists\\n        - effect: NoSchedule\\n          operator: Exists\\n        - key: CriticalAddonsOnly\\n          operator: Exists\\n      securityContext:\\n        fsGroup: 1001\\n      initContainers:\\n      containers:\\n        - name: csi-node-driver-registrar\\n          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.14.0\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.hetzner.cloud/socket\\n          volumeMounts:\\n            - name: plugin-dir\\n              mountPath: /run/csi\\n            - name: registration-dir\\n              mountPath: /registration\\n          resources:\\n            limits: {}\\n            requests: {}\\n        - name: liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n          imagePullPolicy: IfNotPresent\\n          volumeMounts:\\n          - mountPath: /run/csi\\n            name: plugin-dir\\n          resources:\\n            limits: {}\\n            requests: {}\\n        - name: hcloud-csi-driver\\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - -node\\n          volumeMounts:\\n            - name: kubelet-dir\\n              mountPath: /var/lib/kubelet\\n              mountPropagation: \\\"Bidirectional\\\"\\n            - name: plugin-dir\\n              mountPath: /run/csi\\n            - name: device-dir\\n              mountPath: /dev\\n          securityContext:\\n            privileged: true\\n          env:\\n            - name: CSI_ENDPOINT\\n              value: unix:///run/csi/socket\\n            - name: ENABLE_METRICS\\n              value: \\\"false\\\"\\n          ports:\\n            - name: healthz\\n              protocol: TCP\\n              containerPort: 9808\\n          resources:\\n            limits: {}\\n            requests: {}\\n          livenessProbe:\\n            failureThreshold: 5\\n            initialDelaySeconds: 10\\n            periodSeconds: 2\\n            successThreshold: 1\\n            timeoutSeconds: 3\\n            httpGet:\\n              path: /healthz\\n              port: healthz\\n      volumes:\\n        - name: kubelet-dir\\n          hostPath:\\n            path: /var/lib/kubelet\\n            type: Directory\\n        - name: plugin-dir\\n          hostPath:\\n            path: /var/lib/kubelet/plugins/csi.hetzner.cloud/\\n            type: DirectoryOrCreate\\n        - name: registration-dir\\n          hostPath:\\n            path: /var/lib/kubelet/plugins_registry/\\n            type: Directory\\n        - name: device-dir\\n          hostPath:\\n            path: /dev\\n            type: Directory\\n---\\n# Source: hcloud-csi/templates/controller/deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\n    app: hcloud-csi-controller\\nspec:\\n  replicas: 1\\n  strategy:\\n    type: RollingUpdate\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance: hcloud-csi\\n      app.kubernetes.io/component: controller\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart: hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by: Helm\\n        app.kubernetes.io/component: controller\\n    spec:\\n      serviceAccountName: hcloud-csi-controller\\n      \\n      affinity:\\n        nodeAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n          - preference:\\n              matchExpressions:\\n              - key: instance.hetzner.cloud/provided-by\\n                operator: In\\n                values:\\n                - cloud\\n            weight: 1\\n      nodeSelector:\\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      tolerations:\\n        - effect: NoSchedule\\n          key: node-role.kubernetes.io/control-plane\\n          operator: Exists\\n      topologySpreadConstraints:\\n        - labelSelector:\\n            matchLabels:\\n              app.kubernetes.io/component: controller\\n              app.kubernetes.io/instance: hcloud-csi\\n              app.kubernetes.io/name: hcloud-csi\\n          maxSkew: 1\\n          topologyKey: kubernetes.io/hostname\\n          whenUnsatisfiable: ScheduleAnyway\\n      securityContext:\\n        fsGroup: 1001\\n      initContainers:\\n      containers:\\n        - name: csi-attacher\\n          image: registry.k8s.io/sig-storage/csi-attacher:v4.9.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          args:\\n            - --default-fstype=ext4\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: csi-resizer\\n          image: registry.k8s.io/sig-storage/csi-resizer:v1.14.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: csi-provisioner\\n          image: registry.k8s.io/sig-storage/csi-provisioner:v5.3.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          args:\\n            - --feature-gates=Topology=true\\n            - --default-fstype=ext4\\n            - --extra-create-metadata\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          volumeMounts:\\n          - mountPath: /run/csi\\n            name: socket-dir\\n\\n        - name: hcloud-csi-driver\\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - -controller\\n          env:\\n            - name: CSI_ENDPOINT\\n              value: unix:///run/csi/socket\\n            - name: ENABLE_METRICS\\n              value: \\\"false\\\"\\n            - name: KUBE_NODE_NAME\\n              valueFrom:\\n                fieldRef:\\n                  apiVersion: v1\\n                  fieldPath: spec.nodeName\\n            - name: HCLOUD_TOKEN\\n              valueFrom:\\n                secretKeyRef:\\n                  name: hcloud\\n                  key: token\\n          resources:\\n            limits: {}\\n            requests: {}\\n          ports:\\n            - name: healthz\\n              protocol: TCP\\n              containerPort: 9808\\n          livenessProbe:\\n            failureThreshold: 5\\n            initialDelaySeconds: 10\\n            periodSeconds: 2\\n            successThreshold: 1\\n            timeoutSeconds: 3\\n            httpGet:\\n              path: /healthz\\n              port: healthz\\n          volumeMounts:\\n            - name: socket-dir\\n              mountPath: /run/csi\\n\\n      volumes:\\n        - name: socket-dir\\n          emptyDir: {}\\n---\\n# Source: hcloud-csi/templates/core/csidriver.yaml\\napiVersion: storage.k8s.io/v1\\nkind: CSIDriver\\nmetadata:\\n  name: csi.hetzner.cloud\\nspec:\\n  attachRequired: true\\n  fsGroupPolicy: File\\n  podInfoOnMount: true\\n  seLinuxMount: true\\n  volumeLifecycleModes:\\n  - Persistent\\n\\n\"\n        - name: talos-backup\n          contents: |+\n            \"apiVersion\": \"talos.dev/v1alpha1\"\n            \"kind\": \"ServiceAccount\"\n            \"metadata\":\n              \"name\": \"talos-backup-secrets\"\n              \"namespace\": \"kube-system\"\n            \"spec\":\n              \"roles\":\n              - \"os:etcd:backup\"\n\n            ---\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"access_key\": \"YzYwNDY2MDQ4ODdjMDgzZTk2OWUyMWJjNTVhMjQxNzQ=\"\n              \"secret_key\": \"MzI2NTEzNjY0ZWU4N2RiMTVmMWNjOGViZTcyMDBhYTEwMGI5YTFlNzRhZTM1NzZkNWJhMDg4MGQxMjg2YTlmNg==\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"talos-backup-s3-secrets\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n\n            ---\n            \"apiVersion\": \"batch/v1\"\n            \"kind\": \"CronJob\"\n            \"metadata\":\n              \"name\": \"talos-backup\"\n              \"namespace\": \"kube-system\"\n            \"spec\":\n              \"concurrencyPolicy\": \"Forbid\"\n              \"jobTemplate\":\n                \"spec\":\n                  \"template\":\n                    \"spec\":\n                      \"containers\":\n                      - \"env\":\n                        - \"name\": \"AWS_ACCESS_KEY_ID\"\n                          \"valueFrom\":\n                            \"secretKeyRef\":\n                              \"key\": \"access_key\"\n                              \"name\": \"talos-backup-s3-secrets\"\n                        - \"name\": \"AWS_SECRET_ACCESS_KEY\"\n                          \"valueFrom\":\n                            \"secretKeyRef\":\n                              \"key\": \"secret_key\"\n                              \"name\": \"talos-backup-s3-secrets\"\n                        - \"name\": \"AGE_X25519_PUBLIC_KEY\"\n                          \"value\": null\n                        - \"name\": \"DISABLE_ENCRYPTION\"\n                          \"value\": \"true\"\n                        - \"name\": \"AWS_REGION\"\n                          \"value\": \"auto\"\n                        - \"name\": \"CUSTOM_S3_ENDPOINT\"\n                          \"value\": \"https://a694d529ab7d7176bcac8585f8bafdf4.r2.cloudflarestorage.com\"\n                        - \"name\": \"BUCKET\"\n                          \"value\": \"etcd\"\n                        - \"name\": \"CLUSTER_NAME\"\n                          \"value\": \"goingdark\"\n                        - \"name\": \"S3_PREFIX\"\n                          \"value\": null\n                        - \"name\": \"USE_PATH_STYLE\"\n                          \"value\": \"true\"\n                        \"image\": \"ghcr.io/siderolabs/talos-backup:v0.1.0-beta.2-1-g9ccc125\"\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                        \"name\": \"talos-backup\"\n                        \"resources\":\n                          \"limits\":\n                            \"cpu\": \"500m\"\n                            \"memory\": \"256Mi\"\n                          \"requests\":\n                            \"cpu\": \"250m\"\n                            \"memory\": \"128Mi\"\n                        \"securityContext\":\n                          \"allowPrivilegeEscalation\": false\n                          \"capabilities\":\n                            \"drop\":\n                            - \"ALL\"\n                          \"runAsGroup\": 1000\n                          \"runAsNonRoot\": true\n                          \"runAsUser\": 1000\n                          \"seccompProfile\":\n                            \"type\": \"RuntimeDefault\"\n                        \"volumeMounts\":\n                        - \"mountPath\": \"/tmp\"\n                          \"name\": \"tmp\"\n                        - \"mountPath\": \"/var/run/secrets/talos.dev\"\n                          \"name\": \"talos-secrets\"\n                        \"workingDir\": \"/tmp\"\n                      \"restartPolicy\": \"OnFailure\"\n                      \"tolerations\":\n                      - \"effect\": \"NoSchedule\"\n                        \"key\": \"node-role.kubernetes.io/control-plane\"\n                        \"operator\": \"Exists\"\n                      \"volumes\":\n                      - \"emptyDir\": {}\n                        \"name\": \"tmp\"\n                      - \"name\": \"talos-secrets\"\n                        \"secret\":\n                          \"secretName\": \"talos-backup-secrets\"\n              \"schedule\": \"0 * * * *\"\n              \"suspend\": false\n\n        - name: metrics-server\n          contents: |\n            ---\n            # Source: metrics-server/templates/pdb.yaml\n            apiVersion: policy/v1\n            kind: PodDisruptionBudget\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              minAvailable: 1\n\n              selector:\n                matchLabels:\n                  app.kubernetes.io/name: metrics-server\n                  app.kubernetes.io/instance: metrics-server\n            ---\n            # Source: metrics-server/templates/serviceaccount.yaml\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            ---\n            # Source: metrics-server/templates/clusterrole-aggregated-reader.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              name: system:metrics-server-aggregated-reader\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n                rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n                rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n                rbac.authorization.k8s.io/aggregate-to-view: \"true\"\n            rules:\n              - apiGroups:\n                  - metrics.k8s.io\n                resources:\n                  - pods\n                  - nodes\n                verbs:\n                  - get\n                  - list\n                  - watch\n            ---\n            # Source: metrics-server/templates/clusterrole.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              name: system:metrics-server\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            rules:\n              - apiGroups:\n                - \"\"\n                resources:\n                - nodes/metrics\n                verbs:\n                - get\n              - apiGroups:\n                - \"\"\n                resources:\n                  - pods\n                  - nodes\n                  - namespaces\n                  - configmaps\n                verbs:\n                  - get\n                  - list\n                  - watch\n            ---\n            # Source: metrics-server/templates/clusterrolebinding-auth-delegator.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              name: metrics-server:system:auth-delegator\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: system:auth-delegator\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/clusterrolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              name: system:metrics-server\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: system:metrics-server\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/rolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: RoleBinding\n            metadata:\n              name: metrics-server-auth-reader\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: Role\n              name: extension-apiserver-authentication-reader\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/service.yaml\n            apiVersion: v1\n            kind: Service\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              type: ClusterIP\n              ports:\n                - name: https\n                  port: 443\n                  protocol: TCP\n                  targetPort: https\n                  appProtocol: https\n              selector:\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n            ---\n            # Source: metrics-server/templates/deployment.yaml\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              replicas: 2\n              selector:\n                matchLabels:\n                  app.kubernetes.io/name: metrics-server\n                  app.kubernetes.io/instance: metrics-server\n              template:\n                metadata:\n                  labels:\n                    app.kubernetes.io/name: metrics-server\n                    app.kubernetes.io/instance: metrics-server\n                spec:\n                  serviceAccountName: metrics-server\n                  priorityClassName: \"system-cluster-critical\"\n                  containers:\n                    - name: metrics-server\n                      securityContext:\n                        allowPrivilegeEscalation: false\n                        capabilities:\n                          drop:\n                          - ALL\n                        readOnlyRootFilesystem: true\n                        runAsNonRoot: true\n                        runAsUser: 1000\n                        seccompProfile:\n                          type: RuntimeDefault\n                      image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n                      imagePullPolicy: IfNotPresent\n                      args:\n                        - --secure-port=10250\n                        - --cert-dir=/tmp\n                        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n                        - --kubelet-use-node-status-port\n                        - --metric-resolution=15s\n                      ports:\n                      - name: https\n                        protocol: TCP\n                        containerPort: 10250\n                      livenessProbe:\n                        failureThreshold: 3\n                        httpGet:\n                          path: /livez\n                          port: https\n                          scheme: HTTPS\n                        initialDelaySeconds: 0\n                        periodSeconds: 10\n                      readinessProbe:\n                        failureThreshold: 3\n                        httpGet:\n                          path: /readyz\n                          port: https\n                          scheme: HTTPS\n                        initialDelaySeconds: 20\n                        periodSeconds: 10\n                      volumeMounts:\n                        - name: tmp\n                          mountPath: /tmp\n                      resources:\n                        requests:\n                          cpu: 100m\n                          memory: 200Mi\n                  volumes:\n                    - name: tmp\n                      emptyDir: {}\n                  topologySpreadConstraints:\n                    - labelSelector:\n                        matchLabels:\n                          app.kubernetes.io/instance: metrics-server\n                          app.kubernetes.io/name: metrics-server\n                      maxSkew: 1\n                      topologyKey: kubernetes.io/hostname\n                      whenUnsatisfiable: ScheduleAnyway\n            ---\n            # Source: metrics-server/templates/apiservice.yaml\n            apiVersion: apiregistration.k8s.io/v1\n            kind: APIService\n            metadata:\n              name: v1beta1.metrics.k8s.io\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n              annotations:\n            spec:\n              group: metrics.k8s.io\n              groupPriorityMinimum: 100\n              insecureSkipTLSVerify: true\n              service:\n                name: metrics-server\n                namespace: kube-system\n                port: 443\n              version: v1beta1\n              versionPriority: 100\n        - name: cluster-autoscaler\n          contents: |+\n            ---\n            # Source: cluster-autoscaler/templates/pdb.yaml\n            apiVersion: policy/v1\n            kind: PodDisruptionBudget\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              selector:\n                matchLabels:\n                  app.kubernetes.io/instance: \"cluster-autoscaler\"\n                  app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n            ---\n            # Source: cluster-autoscaler/templates/serviceaccount.yaml\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            automountServiceAccountToken: true\n            ---\n            # Source: cluster-autoscaler/templates/clusterrole.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            rules:\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - events\n                  - endpoints\n                verbs:\n                  - create\n                  - patch\n              - apiGroups:\n                - \"\"\n                resources:\n                - pods/eviction\n                verbs:\n                - create\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - pods/status\n                verbs:\n                  - update\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - endpoints\n                resourceNames:\n                  - cluster-autoscaler\n                verbs:\n                  - get\n                  - update\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - nodes\n                verbs:\n                - watch\n                - list\n                - create\n                - delete\n                - get\n                - update\n              - apiGroups:\n                - \"\"\n                resources:\n                  - namespaces\n                  - pods\n                  - services\n                  - replicationcontrollers\n                  - persistentvolumeclaims\n                  - persistentvolumes\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                - batch\n                resources:\n                  - jobs\n                  - cronjobs\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                - batch\n                - extensions\n                resources:\n                - jobs\n                verbs:\n                - get\n                - list\n                - patch\n                - watch\n              - apiGroups:\n                  - extensions\n                resources:\n                  - replicasets\n                  - daemonsets\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                  - policy\n                resources:\n                  - poddisruptionbudgets\n                verbs:\n                  - watch\n                  - list\n              - apiGroups:\n                - apps\n                resources:\n                - daemonsets\n                - replicasets\n                - statefulsets\n                verbs:\n                - watch\n                - list\n                - get\n              - apiGroups:\n                - storage.k8s.io\n                resources:\n                - storageclasses\n                - csinodes\n                - csidrivers\n                - csistoragecapacities\n                - volumeattachments\n                verbs:\n                - watch\n                - list\n                - get\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                verbs:\n                  - list\n                  - watch\n                  - get\n              - apiGroups:\n                - coordination.k8s.io\n                resources:\n                - leases\n                verbs:\n                - create\n              - apiGroups:\n                - coordination.k8s.io\n                resourceNames:\n                - cluster-autoscaler\n                resources:\n                - leases\n                verbs:\n                - get\n                - update\n            ---\n            # Source: cluster-autoscaler/templates/clusterrolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            subjects:\n              - kind: ServiceAccount\n                name: cluster-autoscaler-hetzner-cluster-autoscaler\n                namespace: kube-system\n            ---\n            # Source: cluster-autoscaler/templates/role.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: Role\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            rules:\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                verbs:\n                  - create\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                resourceNames:\n                  - cluster-autoscaler-status\n                verbs:\n                  - delete\n                  - get\n                  - update\n            ---\n            # Source: cluster-autoscaler/templates/rolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: RoleBinding\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: Role\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            subjects:\n              - kind: ServiceAccount\n                name: cluster-autoscaler-hetzner-cluster-autoscaler\n                namespace: kube-system\n            ---\n            # Source: cluster-autoscaler/templates/service.yaml\n            apiVersion: v1\n            kind: Service\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              ports:\n                - port: 8085\n                  protocol: TCP\n                  targetPort: 8085\n                  name: http\n              selector:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n              type: \"ClusterIP\"\n            ---\n            # Source: cluster-autoscaler/templates/deployment.yaml\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              annotations:\n                {}\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              replicas: 1\n              revisionHistoryLimit: 10\n              selector:\n                matchLabels:\n                  app.kubernetes.io/instance: \"cluster-autoscaler\"\n                  app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n              template:\n                metadata:\n                  labels:\n                    app.kubernetes.io/instance: \"cluster-autoscaler\"\n                    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                spec:\n                  priorityClassName: \"system-cluster-critical\"\n                  dnsPolicy: \"ClusterFirst\"\n                  containers:\n                    - name: hetzner-cluster-autoscaler\n                      image: \"registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0\"\n                      imagePullPolicy: \"IfNotPresent\"\n                      command:\n                        - ./cluster-autoscaler\n                        - --cloud-provider=hetzner\n                        - --namespace=kube-system\n                        - --nodes=0:2:cx43:hel1:goingdark-autoscaler\n                        - --balance-similar-node-groups=true\n                        - --expander=least-waste\n                        - --logtostderr=true\n                        - --scale-down-delay-after-add=10m\n                        - --scale-down-delay-after-delete=10m\n                        - --scale-down-unneeded-time=8m\n                        - --scale-down-utilization-threshold=0.75\n                        - --stderrthreshold=info\n                        - --v=4\n                      env:\n                        - name: POD_NAMESPACE\n                          valueFrom:\n                            fieldRef:\n                              fieldPath: metadata.namespace\n                        - name: SERVICE_ACCOUNT\n                          valueFrom:\n                            fieldRef:\n                              fieldPath: spec.serviceAccountName\n                        - name: HCLOUD_CLUSTER_CONFIG_FILE\n                          value: \"/config/cluster-config\"\n                        - name: HCLOUD_FIREWALL\n                          value: \"2379867\"\n                        - name: HCLOUD_NETWORK\n                          value: \"11368893\"\n                        - name: HCLOUD_PUBLIC_IPV4\n                          value: \"true\"\n                        - name: HCLOUD_PUBLIC_IPV6\n                          value: \"false\"\n                        - name: HCLOUD_SERVER_CREATION_TIMEOUT\n                          value: \"10\"\n                        - name: HCLOUD_SSH_KEY\n                          value: \"101120850\"\n                        - name: HCLOUD_TOKEN\n                          valueFrom:\n                            secretKeyRef:\n                              name: hcloud\n                              key: token\n                      livenessProbe:\n                        httpGet:\n                          path: /health-check\n                          port: 8085\n                      ports:\n                        - containerPort: 8085\n                      resources:\n                        {}\n                      volumeMounts:\n                        - name: cluster-autoscaler-hetzner-config\n                          mountPath: /config\n                          readOnly: true\n                  nodeSelector:\n                    node-role.kubernetes.io/control-plane: \"\"\n                  serviceAccountName: cluster-autoscaler-hetzner-cluster-autoscaler\n                  tolerations:\n                    - effect: NoSchedule\n                      key: node-role.kubernetes.io/control-plane\n                      operator: Exists\n                  topologySpreadConstraints:\n                    - labelSelector:\n                        matchLabels:\n                          app.kubernetes.io/instance: cluster-autoscaler\n                          app.kubernetes.io/name: hetzner-cluster-autoscaler\n                      maxSkew: 1\n                      topologyKey: kubernetes.io/hostname\n                      whenUnsatisfiable: ScheduleAnyway\n                  volumes:\n                    - name: cluster-autoscaler-hetzner-config\n                      secret:\n                        secretName: cluster-autoscaler-hetzner-config\n\n            ---\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"cluster-config\": \"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\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"cluster-autoscaler-hetzner-config\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n\n        - name: cilium-settings\n          contents: \"apiVersion: cilium.io/v2alpha1\\nkind: CiliumL2AnnouncementPolicy\\nmetadata:\\n  name: default-l2-announcement-policy\\n  namespace: kube-system\\nspec:\\n  externalIPs: true\\n  loadBalancerIPs: true\\n\\n---\\napiVersion: cilium.io/v2alpha1\\nkind: CiliumLoadBalancerIPPool\\nmetadata:\\n  name: service-pool\\nspec:\\n  blocks:\\n    - start: 10.0.96.240\\n      stop: 10.0.96.250\\n\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-helm-values\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/name: cilium-helm-values\\n    app.kubernetes.io/managed-by: talos-inline\\n  data:\\n    values.yaml: |-\\n# https://github.com/cilium/cilium/blob/main/install/kubernetes/cilium/values.yaml\\n      cluster:\\n        name: talos\\n        id: 1\\n      \\n      # Correct boolean, not a nested map\\n      kubeProxyReplacement: true\\n      \\n        # Talos specific\\n      k8sServiceHost: localhost\\n      k8sServicePort: 7445\\n      securityContext:\\n        capabilities:\\n          ciliumAgent:\\n            [CHOWN, KILL, NET_ADMIN, NET_RAW, IPC_LOCK, SYS_ADMIN, SYS_RESOURCE, DAC_OVERRIDE, FOWNER, SETGID, SETUID]\\n          cleanCiliumState: [NET_ADMIN, SYS_ADMIN, SYS_RESOURCE]\\n      \\n      cgroup:\\n        autoMount:\\n          enabled: false\\n        hostRoot: /sys/fs/cgroup\\n      \\n      # https://www.talos.dev/latest/talos-guides/network/host-dns/#forwarding-kube-dns-to-host-dns\\n      # https://docs.cilium.io/en/stable/operations/performance/tuning/#ebpf-host-routing\\n      bpf:\\n        hostLegacyRouting: true\\n      \\n      # https://docs.cilium.io/en/stable/network/concepts/ipam/\\n      ipam:\\n        mode: kubernetes\\n        multiPoolPreAllocation: ''\\n      \\n      enableMulticast: false\\n      multicast:\\n        enabled: false\\n      \\n      \\n      operator:\\n        rollOutPods: true\\n        resources:\\n          requests:\\n            cpu: 50m\\n            memory: 128Mi\\n          limits:\\n            memory: 256Mi\\n      \\n      # Roll out cilium agent pods automatically when ConfigMap is updated.\\n      rollOutCiliumPods: true\\n      agent: true\\n      agentConfig:\\n        tolerations:\\n          - key: gpu\\n            operator: Equal\\n            value: \\\"true\\\"\\n            effect: NoSchedule\\n        resources:\\n          requests:\\n            cpu: 100m      # Reduced from 500m - now Burstable QoS\\n            memory: 128Mi   # Reduced from 512Mi\\n          limits:\\n            memory: 384Mi   # Remove CPU limit to prevent throttling\\n      \\n      \\n      \\n      #debug:\\n      #  enabled: true\\n      \\n      # Increase rate limit when doing L2 announcements\\n      k8sClientRateLimit:\\n        qps: 20\\n        burst: 100\\n      \\n      l2announcements:\\n        enabled: true\\n      \\n      externalIPs:\\n        enabled: true\\n      \\n      #enableCiliumEndpointSlice: true\\n      \\n      loadBalancer:\\n        # https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#maglev-consistent-hashing\\n        algorithm: maglev\\n      \\n      gatewayAPI:\\n        enabled: true\\n        gatewayClass:\\n          create: \\\"true\\\"\\n        enableAlpn: true\\n        xdsServer:\\n          enabled: true\\n      envoy:\\n        securityContext:\\n          capabilities:\\n            keepCapNetBindService: true\\n            envoy: [NET_BIND_SERVICE, NET_ADMIN, PERFMON, BPF]\\n      \\n      hubble:\\n        peerService:\\n          clusterDomain: cluster.local\\n        enabled: true\\n        relay:\\n          enabled: true\\n          rollOutPods: true\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 128Mi\\n            limits:\\n              cpu: 200m\\n              memory: 256Mi\\n        ui:\\n          enabled: true\\n          rollOutPods: true\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 128Mi\\n            limits:\\n              cpu: 200m\\n              memory: 256Mi\\n      \\n      ingressController:\\n        enabled: false\\n        default: true\\n        loadbalancerMode: shared\\n        service:\\n          annotations:\\n            io.cilium/lb-ipam-ips: 10.0.96.243\\n      \\n      # mTLS\\n      authentication:\\n        enabled: false\\n        mutual:\\n          spire:\\n            enabled: false\\n            install:\\n              server:\\n                dataStorage:\\n                  storageClass: cilium-spire-sc\\n\"\n    adminKubeconfig:\n        certLifetime: 87600h0m0s\n    allowSchedulingOnControlPlanes: false\n","machine_configuration_input":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: controlplane\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 250m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints:\n                - effect: NoSchedule\n                  key: node-role.kubernetes.io/control-plane\n                  value: \"\"\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 250m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-control-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n              vip:\n                ip: 10.0.64.126\n                hcloud:\n                    apiToken: lkbVSBpKQf0XMxYjXcxBLSUl9DYHEoWFnquZ2r0taZTUY1llFe9raH1hQXqZ9ZfQ\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        kubernetesTalosAPIAccess:\n            enabled: true\n            allowedRoles:\n                - os:reader\n                - os:etcd:backup\n            allowedKubernetesNamespaces:\n                - kube-system\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        node.kubernetes.io/exclude-from-external-load-balancers: \"\"\n        nodepool: control\n    nodeTaints:\n        node-role.kubernetes.io/control-plane: :NoSchedule\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    secretboxEncryptionSecret: vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE=\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n    aggregatorCA:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n        key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n    serviceAccount:\n        key: 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\n    apiServer:\n        image: registry.k8s.io/kube-apiserver:v1.33.4\n        extraArgs:\n            enable-aggregator-routing: \"true\"\n        certSANs:\n            - 10.0.64.126\n            - 10.0.64.1\n            - 10.0.64.126\n            - 10.0.64.254\n            - 127.0.0.1\n            - 46.62.164.172\n            - ::1\n            - localhost\n        disablePodSecurityPolicy: true\n        admissionControl:\n            - name: PodSecurity\n              configuration:\n                apiVersion: pod-security.admission.config.k8s.io/v1alpha1\n                defaults:\n                    audit: restricted\n                    audit-version: latest\n                    enforce: baseline\n                    enforce-version: latest\n                    warn: restricted\n                    warn-version: latest\n                exemptions:\n                    namespaces:\n                        - kube-system\n                    runtimeClasses: []\n                    usernames: []\n                kind: PodSecurityConfiguration\n        auditPolicy:\n            apiVersion: audit.k8s.io/v1\n            kind: Policy\n            rules:\n                - level: Metadata\n    controllerManager:\n        image: registry.k8s.io/kube-controller-manager:v1.33.4\n        extraArgs:\n            bind-address: 0.0.0.0\n            cloud-provider: external\n    proxy:\n        disabled: true\n        image: registry.k8s.io/kube-proxy:v1.33.4\n    scheduler:\n        image: registry.k8s.io/kube-scheduler:v1.33.4\n        extraArgs:\n            bind-address: 0.0.0.0\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n    etcd:\n        ca:\n            crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n            key: LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=\n        extraArgs:\n            listen-metrics-urls: http://0.0.0.0:2381\n        advertisedSubnets:\n            - 10.0.64.0/25\n    coreDNS:\n        disabled: false\n    externalCloudProvider:\n        enabled: true\n        manifests:\n            - https://raw.githubusercontent.com/siderolabs/talos-cloud-controller-manager/v1.10.1/docs/deploy/cloud-controller-manager-daemonset.yml\n            - https://github.com/prometheus-operator/prometheus-operator/releases/download/v0.85.0/stripped-down-crds.yaml\n            - https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.3.0/standard-install.yaml\n    inlineManifests:\n        - name: hcloud-secret\n          contents: |\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"network\": \"MTEzNjg4OTM=\"\n              \"token\": \"bGtiVlNCcEtRZjBYTXhZalhjeEJMU1VsOURZSEVvV0ZucXVaMnIwdGFaVFVZMWxsRmU5cmFIMWhRWHFaOVpmUQ==\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"hcloud\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n        - name: cilium\n          contents: \"null\\n...\\n\\n---\\n---\\n# Source: cilium/templates/cilium-secrets-namespace.yaml\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\n  annotations:\\n---\\n# Source: cilium/templates/cilium-agent/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-agent/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-config-agent\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - configmaps\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n---\\n# Source: cilium/templates/cilium-agent/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n---\\n# Source: cilium/templates/cilium-operator/role.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - secrets\\n  verbs:\\n  - create\\n  - delete\\n  - update\\n  - patch\\n---\\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-config-agent\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-config-agent\\nsubjects:\\n  - kind: ServiceAccount\\n    name: \\\"cilium\\\"\\n    namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-agent/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-tlsinterception-secrets\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/rolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: cilium-operator-tlsinterception-secrets\\n  namespace: \\\"cilium-secrets\\\"\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role\\n  name: cilium-operator-tlsinterception-secrets\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-envoy/service.yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: cilium-envoy\\n  namespace: kube-system\\n  annotations:\\n    prometheus.io/scrape: \\\"true\\\"\\n    prometheus.io/port: \\\"9964\\\"\\n  labels:\\n    k8s-app: cilium-envoy\\n    app.kubernetes.io/name: cilium-envoy\\n    app.kubernetes.io/part-of: cilium\\n    io.cilium/app: proxy\\nspec:\\n  clusterIP: None\\n  type: ClusterIP\\n  selector:\\n    k8s-app: cilium-envoy\\n  ports:\\n  - name: envoy-metrics\\n    port: 9964\\n    protocol: TCP\\n    targetPort: envoy-metrics\\n---\\n# Source: cilium/templates/cilium-agent/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: cilium\\n  namespace: kube-system\\n  labels:\\n    k8s-app: cilium\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-agent\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cilium\\n  updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n        prometheus.io/port: \\\"9962\\\"\\n        prometheus.io/scrape: \\\"true\\\"\\n        kubectl.kubernetes.io/default-container: cilium-agent\\n      labels:\\n        k8s-app: cilium\\n        app.kubernetes.io/name: cilium-agent\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n        appArmorProfile:\\n          type: Unconfined\\n        seccompProfile:\\n          type: Unconfined\\n      containers:\\n      - name: cilium-agent\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-agent\\n        args:\\n        - --config-dir=/tmp/cilium/config-map\\n        startupProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n          failureThreshold: 300\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds: 5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n            - name: \\\"require-k8s-connectivity\\\"\\n              value: \\\"false\\\"\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9879\\n            scheme: HTTP\\n            httpHeaders:\\n            - name: \\\"brief\\\"\\n              value: \\\"true\\\"\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_CLUSTERMESH_CONFIG\\n          value: /var/lib/cilium/clustermesh/\\n        - name: GOMEMLIMIT\\n          valueFrom:\\n            resourceFieldRef:\\n              resource: limits.memory\\n              divisor: '1'\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        - name: KUBE_CLIENT_BACKOFF_BASE\\n          value: \\\"1\\\"\\n        - name: KUBE_CLIENT_BACKOFF_DURATION\\n          value: \\\"120\\\"\\n        lifecycle:\\n          postStart:\\n            exec:\\n              command:\\n              - \\\"bash\\\"\\n              - \\\"-c\\\"\\n              - |\\n                    set -o errexit\\n                    set -o pipefail\\n                    set -o nounset\\n                    \\n                    # When running in AWS ENI mode, it's likely that 'aws-node' has\\n                    # had a chance to install SNAT iptables rules. These can result\\n                    # in dropped traffic, so we should attempt to remove them.\\n                    # We do it using a 'postStart' hook since this may need to run\\n                    # for nodes which might have already been init'ed but may still\\n                    # have dangling rules. This is safe because there are no\\n                    # dependencies on anything that is part of the startup script\\n                    # itself, and can be safely run multiple times per node (e.g. in\\n                    # case of a restart).\\n                    if [[ \\\"$(iptables-save | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n                    then\\n                        echo 'Deleting iptables rules created by the AWS CNI VPC plugin'\\n                        iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN' | iptables-restore\\n                    fi\\n                    echo 'Done!'\\n                    \\n          preStop:\\n            exec:\\n              command:\\n              - /cni-uninstall.sh\\n        ports:\\n        - name: peer-service\\n          containerPort: 4244\\n          hostPort: 4244\\n          protocol: TCP\\n        - name: prometheus\\n          containerPort: 9962\\n          hostPort: 9962\\n          protocol: TCP\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - CHOWN\\n              - KILL\\n              - NET_ADMIN\\n              - NET_RAW\\n              - IPC_LOCK\\n              - SYS_ADMIN\\n              - SYS_RESOURCE\\n              - DAC_OVERRIDE\\n              - FOWNER\\n              - SETGID\\n              - SETUID\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n          mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n        # Unprivileged containers need to mount /proc/sys/net from the host\\n        # to have write access\\n        - mountPath: /host/proc/sys/net\\n          name: host-proc-sys-net\\n        # Unprivileged containers need to mount /proc/sys/kernel from the host\\n        # to have write access\\n        - mountPath: /host/proc/sys/kernel\\n          name: host-proc-sys-kernel\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          # Unprivileged containers can't set mount propagation to bidirectional\\n          # in this case we will mount the bpf fs from an init container that\\n          # is privileged and set the mount propagation from host to container\\n          # in Cilium.\\n          mountPropagation: HostToContainer\\n        # Check for duplicate mounts before mounting\\n        - name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n        - name: cilium-run\\n          mountPath: /var/run/cilium\\n        - name: cilium-netns\\n          mountPath: /var/run/cilium/netns\\n          mountPropagation: HostToContainer\\n        - name: etc-cni-netd\\n          mountPath: /host/etc/cni/net.d\\n        - name: clustermesh-secrets\\n          mountPath: /var/lib/cilium/clustermesh\\n          readOnly: true\\n          # Needed to be able to load kernel modules\\n        - name: lib-modules\\n          mountPath: /lib/modules\\n          readOnly: true\\n        - name: xtables-lock\\n          mountPath: /run/xtables.lock\\n        - name: tmp\\n          mountPath: /tmp\\n        \\n      initContainers:\\n      - name: config\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-dbg\\n        - build-config\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        volumeMounts:\\n        - name: tmp\\n          mountPath: /tmp\\n        terminationMessagePolicy: FallbackToLogsOnError\\n      - name: apply-sysctl-overwrites\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        env:\\n        - name: BIN_PATH\\n          value: /opt/cni/bin\\n        command:\\n        - sh\\n        - -ec\\n        # The statically linked Go program binary is invoked to avoid any\\n        # dependency on utilities like sh that can be missing on certain\\n        # distros installed on the underlying host. Copy the binary to the\\n        # same directory where we install cilium cni plugin so that exec permissions\\n        # are available.\\n        - |\\n          cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\\n          nsenter --mount=/hostproc/1/ns/mnt \\\"${BIN_PATH}/cilium-sysctlfix\\\";\\n          rm /hostbin/cilium-sysctlfix\\n        volumeMounts:\\n        - name: hostproc\\n          mountPath: /hostproc\\n        - name: cni-path\\n          mountPath: /hostbin\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - SYS_ADMIN\\n              - SYS_CHROOT\\n              - SYS_PTRACE\\n            drop:\\n              - ALL\\n      # Mount the bpf fs if it is not mounted. We will perform this task\\n      # from a privileged container because the mount propagation bidirectional\\n      # only works from privileged containers.\\n      - name: mount-bpf-fs\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        args:\\n        - 'mount | grep \\\"/sys/fs/bpf type bpf\\\" || mount -t bpf bpf /sys/fs/bpf'\\n        command:\\n        - /bin/bash\\n        - -c\\n        - --\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          privileged: true\\n        volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          mountPropagation: Bidirectional\\n      - name: clean-cilium-state\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - /init-container.sh\\n        env:\\n        - name: CILIUM_ALL_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: clean-cilium-state\\n              optional: true\\n        - name: CILIUM_BPF_STATE\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: clean-cilium-bpf-state\\n              optional: true\\n        - name: WRITE_CNI_CONF_WHEN_READY\\n          valueFrom:\\n            configMapKeyRef:\\n              name: cilium-config\\n              key: write-cni-conf-when-ready\\n              optional: true\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - NET_ADMIN\\n              - SYS_ADMIN\\n              - SYS_RESOURCE\\n            drop:\\n              - ALL\\n        volumeMounts:\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          # Required to mount cgroup filesystem from the host to cilium agent pod\\n        - name: cilium-cgroup\\n          mountPath: /sys/fs/cgroup\\n          mountPropagation: HostToContainer\\n        - name: cilium-run\\n          mountPath: /var/run/cilium # wait-for-kube-proxy\\n      # Install the CNI binaries in an InitContainer so we don't have a writable host mount in the agent\\n      - name: install-cni-binaries\\n        image: \\\"quay.io/cilium/cilium:v1.18.1@sha256:65ab17c052d8758b2ad157ce766285e04173722df59bdee1ea6d5fda7149f0e9\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n          - \\\"/install-plugin.sh\\\"\\n        resources:\\n          requests:\\n            cpu: 100m\\n            memory: 10Mi\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n          - name: cni-path\\n            mountPath: /host/opt/cni/bin # .Values.cni.install\\n      restartPolicy: Always\\n      priorityClassName: system-node-critical\\n      serviceAccountName: \\\"cilium\\\"\\n      automountServiceAccountToken: true\\n      terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      tolerations:\\n        - operator: Exists\\n      volumes:\\n        # For sharing configuration between the \\\"config\\\" initContainer and the agent\\n      - name: tmp\\n        emptyDir: {}\\n        # To keep state between restarts / upgrades\\n      - name: cilium-run\\n        hostPath:\\n          path: /var/run/cilium\\n          type: DirectoryOrCreate\\n        # To exec into pod network namespaces\\n      - name: cilium-netns\\n        hostPath:\\n          path: /var/run/netns\\n          type: DirectoryOrCreate\\n        # To keep state between restarts / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n          path: /sys/fs/bpf\\n          type: DirectoryOrCreate\\n      # To mount cgroup2 filesystem on the host or apply sysctlfix\\n      - name: hostproc\\n        hostPath:\\n          path: /proc\\n          type: Directory\\n      # To keep state between restarts / upgrades for cgroup2 filesystem\\n      - name: cilium-cgroup\\n        hostPath:\\n          path: /sys/fs/cgroup\\n          type: DirectoryOrCreate\\n      # To install cilium cni plugin in the host\\n      - name: cni-path\\n        hostPath:\\n          path:  /opt/cni/bin\\n          type: DirectoryOrCreate\\n        # To install cilium cni configuration in the host\\n      - name: etc-cni-netd\\n        hostPath:\\n          path: /etc/cni/net.d\\n          type: DirectoryOrCreate\\n        # To be able to load kernel modules\\n      - name: lib-modules\\n        hostPath:\\n          path: /lib/modules\\n        # To access iptables concurrently with other processes (e.g. kube-proxy)\\n      - name: xtables-lock\\n        hostPath:\\n          path: /run/xtables.lock\\n          type: FileOrCreate\\n      # Sharing socket with Cilium Envoy on the same node by using a host path\\n      - name: envoy-sockets\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/sockets\\\"\\n          type: DirectoryOrCreate\\n        # To read the clustermesh configuration\\n      - name: clustermesh-secrets\\n        projected:\\n          # note: the leading zero means this number is in octal representation: do not remove it\\n          defaultMode: 0400\\n          sources:\\n          - secret:\\n              name: cilium-clustermesh\\n              optional: true\\n              # note: items are not explicitly listed here, since the entries of this secret\\n              # depend on the peers configured, and that would cause a restart of all agents\\n              # at every addition/removal. Leaving the field empty makes each secret entry\\n              # to be automatically projected into the volume as a file whose name is the key.\\n          - secret:\\n              name: clustermesh-apiserver-remote-cert\\n              optional: true\\n              items:\\n              - key: tls.key\\n                path: common-etcd-client.key\\n              - key: tls.crt\\n                path: common-etcd-client.crt\\n              - key: ca.crt\\n                path: common-etcd-client-ca.crt\\n          # note: we configure the volume for the kvstoremesh-specific certificate\\n          # regardless of whether KVStoreMesh is enabled or not, so that it can be\\n          # automatically mounted in case KVStoreMesh gets subsequently enabled,\\n          # without requiring an agent restart.\\n          - secret:\\n              name: clustermesh-apiserver-local-cert\\n              optional: true\\n              items:\\n              - key: tls.key\\n                path: local-etcd-client.key\\n              - key: tls.crt\\n                path: local-etcd-client.crt\\n              - key: ca.crt\\n                path: local-etcd-client-ca.crt\\n      - name: host-proc-sys-net\\n        hostPath:\\n          path: /proc/sys/net\\n          type: Directory\\n      - name: host-proc-sys-kernel\\n        hostPath:\\n          path: /proc/sys/kernel\\n          type: Directory\\n---\\n# Source: cilium/templates/cilium-envoy/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: cilium-envoy\\n  namespace: kube-system\\n  labels:\\n    k8s-app: cilium-envoy\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-envoy\\n    name: cilium-envoy\\nspec:\\n  selector:\\n    matchLabels:\\n      k8s-app: cilium-envoy\\n  updateStrategy:\\n    rollingUpdate:\\n      maxUnavailable: 2\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n      labels:\\n        k8s-app: cilium-envoy\\n        name: cilium-envoy\\n        app.kubernetes.io/name: cilium-envoy\\n        app.kubernetes.io/part-of: cilium\\n    spec:\\n      securityContext:\\n        appArmorProfile:\\n          type: Unconfined\\n      containers:\\n      - name: cilium-envoy\\n        image: \\\"quay.io/cilium/cilium-envoy:v1.34.4-1754895458-68cffdfa568b6b226d70a7ef81fc65dda3b890bf@sha256:247e908700012f7ef56f75908f8c965215c26a27762f296068645eb55450bda2\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - /usr/bin/cilium-envoy-starter\\n        args:\\n        - '--'\\n        - '-c /var/run/cilium/envoy/bootstrap-config.json'\\n        - '--base-id 0'\\n        - '--log-level info'\\n        startupProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          failureThreshold: 105\\n          periodSeconds: 2\\n          successThreshold: 1\\n          initialDelaySeconds: 5\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 10\\n          timeoutSeconds: 5\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9878\\n            scheme: HTTP\\n          periodSeconds: 30\\n          successThreshold: 1\\n          failureThreshold: 3\\n          timeoutSeconds: 5\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        ports:\\n        - name: envoy-metrics\\n          containerPort: 9964\\n          hostPort: 9964\\n          protocol: TCP\\n        securityContext:\\n          seLinuxOptions:\\n            level: s0\\n            type: spc_t\\n          capabilities:\\n            add:\\n              - NET_ADMIN\\n              - SYS_ADMIN\\n            drop:\\n              - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n        volumeMounts:\\n        - name: envoy-sockets\\n          mountPath: /var/run/cilium/envoy/sockets\\n          readOnly: false\\n        - name: envoy-artifacts\\n          mountPath: /var/run/cilium/envoy/artifacts\\n          readOnly: true\\n        - name: envoy-config\\n          mountPath: /var/run/cilium/envoy/\\n          readOnly: true\\n        - name: bpf-maps\\n          mountPath: /sys/fs/bpf\\n          mountPropagation: HostToContainer\\n      restartPolicy: Always\\n      priorityClassName: system-node-critical\\n      serviceAccountName: \\\"cilium-envoy\\\"\\n      automountServiceAccountToken: true\\n      terminationGracePeriodSeconds: 1\\n      hostNetwork: true\\n      affinity:\\n        nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            nodeSelectorTerms:\\n            - matchExpressions:\\n              - key: cilium.io/no-schedule\\n                operator: NotIn\\n                values:\\n                - \\\"true\\\"\\n        podAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium\\n            topologyKey: kubernetes.io/hostname\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                k8s-app: cilium-envoy\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n      tolerations:\\n        - operator: Exists\\n      volumes:\\n      - name: envoy-sockets\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/sockets\\\"\\n          type: DirectoryOrCreate\\n      - name: envoy-artifacts\\n        hostPath:\\n          path: \\\"/var/run/cilium/envoy/artifacts\\\"\\n          type: DirectoryOrCreate\\n      - name: envoy-config\\n        configMap:\\n          name: \\\"cilium-envoy-config\\\"\\n          # note: the leading zero means this number is in octal representation: do not remove it\\n          defaultMode: 0400\\n          items:\\n            - key: bootstrap-config.json\\n              path: bootstrap-config.json\\n        # To keep state between restarts / upgrades\\n        # To keep state between restarts / upgrades for bpf maps\\n      - name: bpf-maps\\n        hostPath:\\n          path: /sys/fs/bpf\\n          type: DirectoryOrCreate\\n---\\n# Source: cilium/templates/cilium-operator/deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: cilium-operator\\n  namespace: kube-system\\n  labels:\\n    io.cilium/app: operator\\n    name: cilium-operator\\n    app.kubernetes.io/part-of: cilium\\n    app.kubernetes.io/name: cilium-operator\\nspec:\\n  # See docs on ServerCapabilities.LeasesResourceLock in file pkg/k8s/version/version.go\\n  # for more details.\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      io.cilium/app: operator\\n      name: cilium-operator\\n  # ensure operator update on single node k8s clusters, by using rolling update with maxUnavailable=100% in case\\n  # of one replica and no user configured Recreate strategy.\\n  # otherwise an update might get stuck due to the default maxUnavailable=50% in combination with the\\n  # podAntiAffinity which prevents deployments of multiple operator replicas on the same node.\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 25%\\n      maxUnavailable: 100%\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      annotations:\\n        prometheus.io/port: \\\"9963\\\"\\n        prometheus.io/scrape: \\\"true\\\"\\n      labels:\\n        io.cilium/app: operator\\n        name: cilium-operator\\n        app.kubernetes.io/part-of: cilium\\n        app.kubernetes.io/name: cilium-operator\\n    spec:\\n      securityContext:\\n        seccompProfile:\\n          type: RuntimeDefault\\n      containers:\\n      - name: cilium-operator\\n        image: \\\"quay.io/cilium/operator-generic:v1.18.1@sha256:97f4553afa443465bdfbc1cc4927c93f16ac5d78e4dd2706736e7395382201bc\\\"\\n        imagePullPolicy: IfNotPresent\\n        command:\\n        - cilium-operator-generic\\n        args:\\n        - --config-dir=/tmp/cilium/config-map\\n        - --debug=$(CILIUM_DEBUG)\\n        env:\\n        - name: K8S_NODE_NAME\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: spec.nodeName\\n        - name: CILIUM_K8S_NAMESPACE\\n          valueFrom:\\n            fieldRef:\\n              apiVersion: v1\\n              fieldPath: metadata.namespace\\n        - name: CILIUM_DEBUG\\n          valueFrom:\\n            configMapKeyRef:\\n              key: debug\\n              name: cilium-config\\n              optional: true\\n        - name: KUBERNETES_SERVICE_HOST\\n          value: \\\"127.0.0.1\\\"\\n        - name: KUBERNETES_SERVICE_PORT\\n          value: \\\"7445\\\"\\n        ports:\\n        - name: prometheus\\n          containerPort: 9963\\n          hostPort: 9963\\n          protocol: TCP\\n        livenessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds: 60\\n          periodSeconds: 10\\n          timeoutSeconds: 3\\n        readinessProbe:\\n          httpGet:\\n            host: \\\"127.0.0.1\\\"\\n            path: /healthz\\n            port: 9234\\n            scheme: HTTP\\n          initialDelaySeconds: 0\\n          periodSeconds: 5\\n          timeoutSeconds: 3\\n          failureThreshold: 5\\n        volumeMounts:\\n        - name: cilium-config-path\\n          mountPath: /tmp/cilium/config-map\\n          readOnly: true\\n        securityContext:\\n          allowPrivilegeEscalation: false\\n          capabilities:\\n            drop:\\n            - ALL\\n        terminationMessagePolicy: FallbackToLogsOnError\\n      hostNetwork: true\\n      restartPolicy: Always\\n      priorityClassName: system-cluster-critical\\n      serviceAccountName: \\\"cilium-operator\\\"\\n      automountServiceAccountToken: true\\n      # In HA mode, cilium-operator pods must not be scheduled on the same\\n      # node as they will clash with each other.\\n      affinity:\\n        podAntiAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n          - labelSelector:\\n              matchLabels:\\n                io.cilium/app: operator\\n            topologyKey: kubernetes.io/hostname\\n      nodeSelector:\\n        kubernetes.io/os: linux\\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      tolerations:\\n        - operator: Exists\\n        - key: node.cilium.io/agent-not-ready\\n          operator: Exists\\n      \\n      volumes:\\n        # To read the configuration from the config map\\n      - name: cilium-config-path\\n        configMap:\\n          name: cilium-config\\n---\\n# Source: cilium/templates/cilium-envoy/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-envoy\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-configmap.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-config\\n  namespace: kube-system\\ndata:\\n\\n  # Identity allocation mode selects how identities are shared between cilium\\n  # nodes by setting how they are stored. The options are \\\"crd\\\", \\\"kvstore\\\" or\\n  # \\\"doublewrite-readkvstore\\\" / \\\"doublewrite-readcrd\\\".\\n  # - \\\"crd\\\" stores identities in kubernetes as CRDs (custom resource definition).\\n  #   These can be queried with:\\n  #     kubectl get ciliumid\\n  # - \\\"kvstore\\\" stores identities in an etcd kvstore, that is\\n  #   configured below. Cilium versions before 1.6 supported only the kvstore\\n  #   backend. Upgrades from these older cilium versions should continue using\\n  #   the kvstore by commenting out the identity-allocation-mode below, or\\n  #   setting it to \\\"kvstore\\\".\\n  # - \\\"doublewrite\\\" modes store identities in both the kvstore and CRDs. This is useful\\n  #   for seamless migrations from the kvstore mode to the crd mode. Consult the\\n  #   documentation for more information on how to perform the migration.\\n  identity-allocation-mode: crd\\n\\n  identity-heartbeat-timeout: \\\"30m0s\\\"\\n  identity-gc-interval: \\\"15m0s\\\"\\n  cilium-endpoint-gc-interval: \\\"5m0s\\\"\\n  nodes-gc-interval: \\\"5m0s\\\"\\n\\n  # If you want to run cilium in debug mode change this value to true\\n  debug: \\\"false\\\"\\n  debug-verbose: \\\"\\\"\\n  metrics-sampling-interval: \\\"5m\\\"\\n  # The agent can be put into the following three policy enforcement modes\\n  # default, always and never.\\n  # https://docs.cilium.io/en/latest/security/policy/intro/#policy-enforcement-modes\\n  enable-policy: \\\"default\\\"\\n  policy-cidr-match-mode: \\\"\\\"\\n  # If you want metrics enabled in all of your Cilium agents, set the port for\\n  # which the Cilium agents will have their metrics exposed.\\n  # This option deprecates the \\\"prometheus-serve-addr\\\" in the\\n  # \\\"cilium-metrics-config\\\" ConfigMap\\n  # NOTE that this will open the port on ALL nodes where Cilium pods are\\n  # scheduled.\\n  prometheus-serve-addr: \\\":9962\\\"\\n  # A space-separated list of controller groups for which to enable metrics.\\n  # The special values of \\\"all\\\" and \\\"none\\\" are supported.\\n  controller-group-metrics:\\n    write-cni-file\\n    sync-host-ips\\n    sync-lb-maps-with-k8s-services\\n  # If you want metrics enabled in cilium-operator, set the port for\\n  # which the Cilium Operator will have their metrics exposed.\\n  # NOTE that this will open the port on the nodes where Cilium operator pod\\n  # is scheduled.\\n  operator-prometheus-serve-addr: \\\":9963\\\"\\n  enable-metrics: \\\"true\\\"\\n  enable-policy-secrets-sync: \\\"true\\\"\\n  policy-secrets-only-from-secrets-namespace: \\\"true\\\"\\n  policy-secrets-namespace: \\\"cilium-secrets\\\"\\n\\n  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4\\n  # address.\\n  enable-ipv4: \\\"true\\\"\\n\\n  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6\\n  # address.\\n  enable-ipv6: \\\"false\\\"\\n  # Users who wish to specify their own custom CNI configuration file must set\\n  # custom-cni-conf to \\\"true\\\", otherwise Cilium may overwrite the configuration.\\n  custom-cni-conf: \\\"false\\\"\\n  enable-bpf-clock-probe: \\\"false\\\"\\n  # If you want cilium monitor to aggregate tracing for packets, set this level\\n  # to \\\"low\\\", \\\"medium\\\", or \\\"maximum\\\". The higher the level, the less packets\\n  # that will be seen in monitor output.\\n  monitor-aggregation: medium\\n\\n  # The monitor aggregation interval governs the typical time between monitor\\n  # notification events for each allowed connection.\\n  #\\n  # Only effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n  monitor-aggregation-interval: \\\"5s\\\"\\n\\n  # The monitor aggregation flags determine which TCP flags which, upon the\\n  # first observation, cause monitor notifications to be generated.\\n  #\\n  # Only effective when monitor aggregation is set to \\\"medium\\\" or higher.\\n  monitor-aggregation-flags: all\\n  # Specifies the ratio (0.0-1.0] of total system memory to use for dynamic\\n  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.\\n  bpf-map-dynamic-size-ratio: \\\"0.0025\\\"\\n  enable-host-legacy-routing: \\\"false\\\"\\n  # bpf-policy-map-max specifies the maximum number of entries in endpoint\\n  # policy map (per endpoint)\\n  bpf-policy-map-max: \\\"16384\\\"\\n  # bpf-policy-stats-map-max specifies the maximum number of entries in global\\n  # policy stats map\\n  bpf-policy-stats-map-max: \\\"65536\\\"\\n  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,\\n  # backend and affinity maps.\\n  bpf-lb-map-max: \\\"65536\\\"\\n  bpf-lb-external-clusterip: \\\"false\\\"\\n  bpf-lb-source-range-all-types: \\\"false\\\"\\n  bpf-lb-algorithm-annotation: \\\"false\\\"\\n  bpf-lb-mode-annotation: \\\"false\\\"\\n\\n  bpf-distributed-lru: \\\"false\\\"\\n  bpf-events-drop-enabled: \\\"true\\\"\\n  bpf-events-policy-verdict-enabled: \\\"true\\\"\\n  bpf-events-trace-enabled: \\\"true\\\"\\n\\n  # Pre-allocation of map entries allows per-packet latency to be reduced, at\\n  # the expense of up-front memory allocation for the entries in the maps. The\\n  # default value below will minimize memory usage in the default installation;\\n  # users who are sensitive to latency may consider setting this to \\\"true\\\".\\n  #\\n  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore\\n  # this option and behave as though it is set to \\\"true\\\".\\n  #\\n  # If this value is modified, then during the next Cilium startup the restore\\n  # of existing endpoints and tracking of ongoing connections may be disrupted.\\n  # As a result, reply packets may be dropped and the load-balancing decisions\\n  # for established connections may change.\\n  #\\n  # If this option is set to \\\"false\\\" during an upgrade from 1.3 or earlier to\\n  # 1.4 or later, then it may cause one-time disruptions during the upgrade.\\n  preallocate-bpf-maps: \\\"false\\\"\\n\\n  # Name of the cluster. Only relevant when building a mesh of clusters.\\n  cluster-name: \\\"default\\\"\\n  # Unique ID of the cluster. Must be unique across all conneted clusters and\\n  # in the range of 1 and 255. Only relevant when building a mesh of clusters.\\n  cluster-id: \\\"0\\\"\\n\\n  # Encapsulation mode for communication between nodes\\n  # Possible values:\\n  #   - disabled\\n  #   - vxlan (default)\\n  #   - geneve\\n\\n  routing-mode: \\\"native\\\"\\n  tunnel-protocol: \\\"vxlan\\\"\\n  tunnel-source-port-range: \\\"0-0\\\"\\n  service-no-backend-response: \\\"reject\\\"\\n\\n\\n  # Enables L7 proxy for L7 policy enforcement and visibility\\n  enable-l7-proxy: \\\"true\\\"\\n  enable-ipv4-masquerade: \\\"true\\\"\\n  enable-ipv4-big-tcp: \\\"false\\\"\\n  enable-ipv6-big-tcp: \\\"false\\\"\\n  enable-ipv6-masquerade: \\\"true\\\"\\n  enable-tcx: \\\"true\\\"\\n  datapath-mode: \\\"veth\\\"\\n  enable-bpf-masquerade: \\\"true\\\"\\n  enable-masquerade-to-route-source: \\\"false\\\"\\n  enable-wireguard: \\\"true\\\"\\n  wireguard-persistent-keepalive: \\\"0s\\\"\\n\\n  enable-xt-socket-fallback: \\\"true\\\"\\n  install-no-conntrack-iptables-rules: \\\"true\\\"\\n  iptables-random-fully: \\\"false\\\"\\n\\n  auto-direct-node-routes: \\\"false\\\"\\n  direct-routing-skip-unreachable: \\\"false\\\"\\n\\n\\n  ipv4-native-routing-cidr: 10.0.0.0/16\\n\\n  kube-proxy-replacement: \\\"true\\\"\\n  kube-proxy-replacement-healthz-bind-address: \\\"0.0.0.0:10256\\\"\\n  bpf-lb-sock: \\\"false\\\"\\n  nodeport-addresses: \\\"\\\"\\n  enable-health-check-nodeport: \\\"true\\\"\\n  enable-health-check-loadbalancer-ip: \\\"false\\\"\\n  node-port-bind-protection: \\\"true\\\"\\n  enable-auto-protect-node-port-range: \\\"true\\\"\\n  bpf-lb-acceleration: \\\"native\\\"\\n  enable-svc-source-range-check: \\\"true\\\"\\n  enable-l2-neigh-discovery: \\\"false\\\"\\n  k8s-require-ipv4-pod-cidr: \\\"true\\\"\\n  k8s-require-ipv6-pod-cidr: \\\"false\\\"\\n  enable-k8s-networkpolicy: \\\"true\\\"\\n  enable-endpoint-lockdown-on-policy-overflow: \\\"false\\\"\\n  # Tell the agent to generate and write a CNI configuration file\\n  write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist\\n  cni-exclusive: \\\"true\\\"\\n  cni-log-file: \\\"/var/run/cilium/cilium-cni.log\\\"\\n  enable-endpoint-health-checking: \\\"true\\\"\\n  enable-health-checking: \\\"true\\\"\\n  health-check-icmp-failure-threshold: \\\"3\\\"\\n  enable-well-known-identities: \\\"false\\\"\\n  enable-node-selector-labels: \\\"false\\\"\\n  synchronize-k8s-nodes: \\\"true\\\"\\n  operator-api-serve-addr: \\\"127.0.0.1:9234\\\"\\n\\n  enable-hubble: \\\"false\\\"\\n  ipam: \\\"kubernetes\\\"\\n  ipam-cilium-node-update-rate: \\\"15s\\\"\\n\\n  default-lb-service-ipam: \\\"lbipam\\\"\\n  egress-gateway-reconciliation-trigger-interval: \\\"1s\\\"\\n  enable-vtep: \\\"false\\\"\\n  vtep-endpoint: \\\"\\\"\\n  vtep-cidr: \\\"\\\"\\n  vtep-mask: \\\"\\\"\\n  vtep-mac: \\\"\\\"\\n  procfs: \\\"/host/proc\\\"\\n  bpf-root: \\\"/sys/fs/bpf\\\"\\n  cgroup-root: \\\"/sys/fs/cgroup\\\"\\n\\n  identity-management-mode: \\\"agent\\\"\\n  enable-sctp: \\\"false\\\"\\n  remove-cilium-node-taints: \\\"true\\\"\\n  set-cilium-node-taints: \\\"true\\\"\\n  set-cilium-is-up-condition: \\\"true\\\"\\n  unmanaged-pod-watcher-interval: \\\"15\\\"\\n  # explicit setting gets precedence\\n  dnsproxy-enable-transparent-mode: \\\"true\\\"\\n  dnsproxy-socket-linger-timeout: \\\"10\\\"\\n  tofqdns-dns-reject-response-code: \\\"refused\\\"\\n  tofqdns-enable-dns-compression: \\\"true\\\"\\n  tofqdns-endpoint-max-ip-per-hostname: \\\"1000\\\"\\n  tofqdns-idle-connection-grace-period: \\\"0s\\\"\\n  tofqdns-max-deferred-connection-deletes: \\\"10000\\\"\\n  tofqdns-proxy-response-max-delay: \\\"100ms\\\"\\n  tofqdns-preallocate-identities:  \\\"true\\\"\\n  agent-not-ready-taint-key: \\\"node.cilium.io/agent-not-ready\\\"\\n\\n  mesh-auth-enabled: \\\"true\\\"\\n  mesh-auth-queue-size: \\\"1024\\\"\\n  mesh-auth-rotated-identities-queue-size: \\\"1024\\\"\\n  mesh-auth-gc-interval: \\\"5m0s\\\"\\n\\n  proxy-xff-num-trusted-hops-ingress: \\\"0\\\"\\n  proxy-xff-num-trusted-hops-egress: \\\"0\\\"\\n  proxy-connect-timeout: \\\"2\\\"\\n  proxy-initial-fetch-timeout: \\\"30\\\"\\n  proxy-max-requests-per-connection: \\\"0\\\"\\n  proxy-max-connection-duration-seconds: \\\"0\\\"\\n  proxy-idle-timeout-seconds: \\\"60\\\"\\n  proxy-max-concurrent-retries: \\\"128\\\"\\n  http-retry-count: \\\"3\\\"\\n\\n  external-envoy-proxy: \\\"true\\\"\\n  envoy-base-id: \\\"0\\\"\\n  envoy-access-log-buffer-size: \\\"4096\\\"\\n  envoy-keep-cap-netbindservice: \\\"false\\\"\\n  max-connected-clusters: \\\"255\\\"\\n  clustermesh-enable-endpoint-sync: \\\"false\\\"\\n  clustermesh-enable-mcs-api: \\\"false\\\"\\n  policy-default-local-cluster: \\\"false\\\"\\n\\n  nat-map-stats-entries: \\\"32\\\"\\n  nat-map-stats-interval: \\\"30s\\\"\\n  enable-internal-traffic-policy: \\\"true\\\"\\n  enable-lb-ipam: \\\"true\\\"\\n  enable-non-default-deny-policies: \\\"true\\\"\\n  enable-source-ip-verification: \\\"true\\\"\\n\\n# Extra config allows adding arbitrary properties to the cilium config.\\n# By putting it at the end of the ConfigMap, it's also possible to override existing properties.\\n---\\n# Source: cilium/templates/cilium-envoy/configmap.yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-envoy-config\\n  namespace: kube-system\\ndata:\\n  # Keep the key name as bootstrap-config.json to avoid breaking changes\\n  bootstrap-config.json: |\\n    {\\\"admin\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}},\\\"applicationLogConfig\\\":{\\\"logFormat\\\":{\\\"textFormat\\\":\\\"[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v\\\"}},\\\"bootstrapExtensions\\\":[{\\\"name\\\":\\\"envoy.bootstrap.internal_listener\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.bootstrap.internal_listener.v3.InternalListener\\\"}}],\\\"dynamicResources\\\":{\\\"cdsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"},\\\"ldsConfig\\\":{\\\"apiConfigSource\\\":{\\\"apiType\\\":\\\"GRPC\\\",\\\"grpcServices\\\":[{\\\"envoyGrpc\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\"}}],\\\"setNodeOnFirstMessageOnly\\\":true,\\\"transportApiVersion\\\":\\\"V3\\\"},\\\"initialFetchTimeout\\\":\\\"30s\\\",\\\"resourceApiVersion\\\":\\\"V3\\\"}},\\\"node\\\":{\\\"cluster\\\":\\\"ingress-cluster\\\",\\\"id\\\":\\\"host~127.0.0.1~no-id~localdomain\\\"},\\\"overloadManager\\\":{\\\"resourceMonitors\\\":[{\\\"name\\\":\\\"envoy.resource_monitors.global_downstream_max_connections\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.resource_monitors.downstream_connections.v3.DownstreamConnectionsConfig\\\",\\\"max_active_downstream_connections\\\":\\\"50000\\\"}}]},\\\"staticResources\\\":{\\\"clusters\\\":[{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"egress-cluster\\\",\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"circuitBreakers\\\":{\\\"thresholds\\\":[{\\\"maxRetries\\\":128}]},\\\"cleanupInterval\\\":\\\"2.500s\\\",\\\"connectTimeout\\\":\\\"2s\\\",\\\"lbPolicy\\\":\\\"CLUSTER_PROVIDED\\\",\\\"name\\\":\\\"ingress-cluster-tls\\\",\\\"transportSocket\\\":{\\\"name\\\":\\\"cilium.tls_wrapper\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/cilium.UpstreamTlsWrapperContext\\\"}},\\\"type\\\":\\\"ORIGINAL_DST\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"commonHttpProtocolOptions\\\":{\\\"idleTimeout\\\":\\\"60s\\\",\\\"maxConnectionDuration\\\":\\\"0s\\\",\\\"maxRequestsPerConnection\\\":0},\\\"upstreamHttpProtocolOptions\\\":{},\\\"useDownstreamProtocolConfig\\\":{}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"xds-grpc-cilium\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/xds.sock\\\"}}}}]}]},\\\"name\\\":\\\"xds-grpc-cilium\\\",\\\"type\\\":\\\"STATIC\\\",\\\"typedExtensionProtocolOptions\\\":{\\\"envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\\\",\\\"explicitHttpConfig\\\":{\\\"http2ProtocolOptions\\\":{}}}}},{\\\"connectTimeout\\\":\\\"2s\\\",\\\"loadAssignment\\\":{\\\"clusterName\\\":\\\"/envoy-admin\\\",\\\"endpoints\\\":[{\\\"lbEndpoints\\\":[{\\\"endpoint\\\":{\\\"address\\\":{\\\"pipe\\\":{\\\"path\\\":\\\"/var/run/cilium/envoy/sockets/admin.sock\\\"}}}}]}]},\\\"name\\\":\\\"/envoy-admin\\\",\\\"type\\\":\\\"STATIC\\\"}],\\\"listeners\\\":[{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"0.0.0.0\\\",\\\"portValue\\\":9964}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtualHosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/metrics\\\"},\\\"name\\\":\\\"prometheus_metrics_route\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/stats/prometheus\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-prometheus-metrics-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-prometheus-metrics-listener\\\"},{\\\"address\\\":{\\\"socketAddress\\\":{\\\"address\\\":\\\"127.0.0.1\\\",\\\"portValue\\\":9878}},\\\"filterChains\\\":[{\\\"filters\\\":[{\\\"name\\\":\\\"envoy.filters.network.http_connection_manager\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\\\",\\\"httpFilters\\\":[{\\\"name\\\":\\\"envoy.filters.http.router\\\",\\\"typedConfig\\\":{\\\"@type\\\":\\\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\\\"}}],\\\"internalAddressConfig\\\":{\\\"cidrRanges\\\":[{\\\"addressPrefix\\\":\\\"10.0.0.0\\\",\\\"prefixLen\\\":8},{\\\"addressPrefix\\\":\\\"172.16.0.0\\\",\\\"prefixLen\\\":12},{\\\"addressPrefix\\\":\\\"192.168.0.0\\\",\\\"prefixLen\\\":16},{\\\"addressPrefix\\\":\\\"127.0.0.1\\\",\\\"prefixLen\\\":32}]},\\\"routeConfig\\\":{\\\"virtual_hosts\\\":[{\\\"domains\\\":[\\\"*\\\"],\\\"name\\\":\\\"health\\\",\\\"routes\\\":[{\\\"match\\\":{\\\"prefix\\\":\\\"/healthz\\\"},\\\"name\\\":\\\"health\\\",\\\"route\\\":{\\\"cluster\\\":\\\"/envoy-admin\\\",\\\"prefixRewrite\\\":\\\"/ready\\\"}}]}]},\\\"statPrefix\\\":\\\"envoy-health-listener\\\",\\\"streamIdleTimeout\\\":\\\"300s\\\"}}]}],\\\"name\\\":\\\"envoy-health-listener\\\"}]}}\\n---\\n# Source: cilium/templates/cilium-agent/clusterrole.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cilium\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - networking.k8s.io\\n  resources:\\n  - networkpolicies\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - discovery.k8s.io\\n  resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - namespaces\\n  - services\\n  - pods\\n  - endpoints\\n  - nodes\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - list\\n  - watch\\n  # This is used when validating policies in preflight. This will need to stay\\n  # until we figure out how to avoid \\\"get\\\" inside the preflight, and then\\n  # should be removed ideally.\\n  - get\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools\\n  - ciliumbgppeeringpolicies\\n  - ciliumbgpnodeconfigs\\n  - ciliumbgpadvertisements\\n  - ciliumbgppeerconfigs\\n  - ciliumclusterwideenvoyconfigs\\n  - ciliumclusterwidenetworkpolicies\\n  - ciliumegressgatewaypolicies\\n  - ciliumendpoints\\n  - ciliumendpointslices\\n  - ciliumenvoyconfigs\\n  - ciliumidentities\\n  - ciliumlocalredirectpolicies\\n  - ciliumnetworkpolicies\\n  - ciliumnodes\\n  - ciliumnodeconfigs\\n  - ciliumcidrgroups\\n  - ciliuml2announcementpolicies\\n  - ciliumpodippools\\n  verbs:\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumidentities\\n  - ciliumendpoints\\n  - ciliumnodes\\n  verbs:\\n  - create\\n- apiGroups:\\n  - cilium.io\\n  # To synchronize garbage collection of such resources\\n  resources:\\n  - ciliumidentities\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints\\n  verbs:\\n  - delete\\n  - get\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes\\n  - ciliumnodes/status\\n  verbs:\\n  - get\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints/status\\n  - ciliumendpoints\\n  - ciliuml2announcementpolicies/status\\n  - ciliumbgpnodeconfigs/status\\n  verbs:\\n  - patch\\n---\\n# Source: cilium/templates/cilium-operator/clusterrole.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: cilium-operator\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nrules:\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - pods\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n  # to automatically delete [core|kube]dns pods so that are starting to being\\n  # managed by Cilium\\n  - delete\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - configmaps\\n  resourceNames:\\n  - cilium-config\\n  verbs:\\n   # allow patching of the configmap to set annotations\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  - nodes\\n  verbs:\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # To remove node taints\\n  - nodes\\n  # To set NetworkUnavailable false on startup\\n  - nodes/status\\n  verbs:\\n  - patch\\n- apiGroups:\\n  - discovery.k8s.io\\n  resources:\\n  - endpointslices\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to perform LB IP allocation for BGP\\n  - services/status\\n  verbs:\\n  - update\\n  - patch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to check apiserver connectivity\\n  - namespaces\\n  - secrets\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - \\\"\\\"\\n  resources:\\n  # to perform the translation of a CNP that contains `ToGroup` to its endpoints\\n  - services\\n  - endpoints\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnetworkpolicies\\n  - ciliumclusterwidenetworkpolicies\\n  verbs:\\n  # Create auto-generated CNPs and CCNPs from Policies that have 'toGroups'\\n  - create\\n  - update\\n  - deletecollection\\n  # To update the status of the CNPs and CCNPs\\n  - patch\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnetworkpolicies/status\\n  - ciliumclusterwidenetworkpolicies/status\\n  verbs:\\n  # Update the auto-generated CNPs and CCNPs status.\\n  - patch\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpoints\\n  - ciliumidentities\\n  verbs:\\n  # To perform garbage collection of such resources\\n  - delete\\n  - list\\n  - watch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumidentities\\n  verbs:\\n  # To synchronize garbage collection of such resources\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes\\n  verbs:\\n  - create\\n  - update\\n  - get\\n  - list\\n  - watch\\n    # To perform CiliumNode garbage collector\\n  - delete\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumnodes/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumendpointslices\\n  - ciliumenvoyconfigs\\n  - ciliumbgppeerconfigs\\n  - ciliumbgpadvertisements\\n  - ciliumbgpnodeconfigs\\n  verbs:\\n  - create\\n  - update\\n  - get\\n  - list\\n  - watch\\n  - delete\\n  - patch\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumbgpclusterconfigs/status\\n  - ciliumbgppeerconfigs/status\\n  verbs:\\n  - update\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - create\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n  - apiextensions.k8s.io\\n  resources:\\n  - customresourcedefinitions\\n  verbs:\\n  - update\\n  resourceNames:\\n  - ciliumloadbalancerippools.cilium.io\\n  - ciliumbgppeeringpolicies.cilium.io\\n  - ciliumbgpclusterconfigs.cilium.io\\n  - ciliumbgppeerconfigs.cilium.io\\n  - ciliumbgpadvertisements.cilium.io\\n  - ciliumbgpnodeconfigs.cilium.io\\n  - ciliumbgpnodeconfigoverrides.cilium.io\\n  - ciliumclusterwideenvoyconfigs.cilium.io\\n  - ciliumclusterwidenetworkpolicies.cilium.io\\n  - ciliumegressgatewaypolicies.cilium.io\\n  - ciliumendpoints.cilium.io\\n  - ciliumendpointslices.cilium.io\\n  - ciliumenvoyconfigs.cilium.io\\n  - ciliumidentities.cilium.io\\n  - ciliumlocalredirectpolicies.cilium.io\\n  - ciliumnetworkpolicies.cilium.io\\n  - ciliumnodes.cilium.io\\n  - ciliumnodeconfigs.cilium.io\\n  - ciliumcidrgroups.cilium.io\\n  - ciliuml2announcementpolicies.cilium.io\\n  - ciliumpodippools.cilium.io\\n  - ciliumgatewayclassconfigs.cilium.io\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools\\n  - ciliumpodippools\\n  - ciliumbgppeeringpolicies\\n  - ciliumbgpclusterconfigs\\n  - ciliumbgpnodeconfigoverrides\\n  - ciliumbgppeerconfigs\\n  verbs:\\n  - get\\n  - list\\n  - watch\\n- apiGroups:\\n    - cilium.io\\n  resources:\\n    - ciliumpodippools\\n  verbs:\\n    - create\\n- apiGroups:\\n  - cilium.io\\n  resources:\\n  - ciliumloadbalancerippools/status\\n  verbs:\\n  - patch\\n# For cilium-operator running in HA mode.\\n#\\n# Cilium operator running in HA mode requires the use of ResourceLock for Leader Election\\n# between multiple running instances.\\n# The preferred way of doing this is to use LeasesResourceLock as edits to Leases are less\\n# common and fewer objects in the cluster watch \\\"all Leases\\\".\\n- apiGroups:\\n  - coordination.k8s.io\\n  resources:\\n  - leases\\n  verbs:\\n  - create\\n  - get\\n  - update\\n---\\n# Source: cilium/templates/cilium-agent/clusterrolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cilium\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium\\\"\\n  namespace: kube-system\\n---\\n# Source: cilium/templates/cilium-operator/clusterrolebinding.yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: cilium-operator\\n  labels:\\n    app.kubernetes.io/part-of: cilium\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cilium-operator\\nsubjects:\\n- kind: ServiceAccount\\n  name: \\\"cilium-operator\\\"\\n  namespace: kube-system\\n\\n\"\n        - name: hcloud-ccm\n          contents: \"---\\n# Source: hcloud-cloud-controller-manager/templates/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n  namespace: kube-system\\n---\\n# Source: hcloud-cloud-controller-manager/templates/clusterrolebinding.yaml\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: \\\"system:hcloud-cloud-controller-manager\\\"\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: cluster-admin\\nsubjects:\\n  - kind: ServiceAccount\\n    name: hcloud-cloud-controller-manager\\n    namespace: kube-system\\n---\\n# Source: hcloud-cloud-controller-manager/templates/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: hcloud-cloud-controller-manager\\n  namespace: kube-system\\nspec:\\n  revisionHistoryLimit: 2\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n      app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/instance: 'hcloud-cloud-controller-manager'\\n        app.kubernetes.io/name: 'hcloud-cloud-controller-manager'\\n    spec:\\n      serviceAccountName: hcloud-cloud-controller-manager\\n      dnsPolicy: Default\\n      tolerations:\\n        # Allow HCCM itself to schedule on nodes that have not yet been initialized by HCCM.\\n        - key: \\\"node.cloudprovider.kubernetes.io/uninitialized\\\"\\n          value: \\\"true\\\"\\n          effect: \\\"NoSchedule\\\"\\n        - key: \\\"CriticalAddonsOnly\\\"\\n          operator: \\\"Exists\\\"\\n\\n        # Allow HCCM to schedule on control plane nodes.\\n        - key: \\\"node-role.kubernetes.io/master\\\"\\n          effect: NoSchedule\\n          operator: Exists\\n        - key: \\\"node-role.kubernetes.io/control-plane\\\"\\n          effect: NoSchedule\\n          operator: Exists\\n\\n        - key: \\\"node.kubernetes.io/not-ready\\\"\\n          effect: \\\"NoExecute\\\"\\n      nodeSelector:\\n        \\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      hostNetwork: true\\n      containers:\\n        - name: hcloud-cloud-controller-manager\\n          command:\\n            - \\\"/bin/hcloud-cloud-controller-manager\\\"\\n            - \\\"--allow-untagged-cloud\\\"\\n            - \\\"--cloud-provider=hcloud\\\"\\n            - \\\"--route-reconciliation-period=30s\\\"\\n            - \\\"--webhook-secure-port=0\\\"\\n            - \\\"--allocate-node-cidrs=true\\\"\\n            - \\\"--cluster-cidr=10.0.128.0/17\\\"\\n          env:\\n            - name: HCLOUD_LOAD_BALANCERS_DISABLE_PRIVATE_INGRESS\\n              value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_ENABLED\\n              value: \\\"true\\\"\\n            - name: HCLOUD_LOAD_BALANCERS_LOCATION\\n              value: hel1\\n            - name: HCLOUD_LOAD_BALANCERS_USE_PRIVATE_IP\\n              value: \\\"true\\\"\\n            - name: HCLOUD_NETWORK_ROUTES_ENABLED\\n              value: \\\"true\\\"\\n            - name: HCLOUD_TOKEN\\n              valueFrom:\\n                secretKeyRef:\\n                  key: token\\n                  name: hcloud\\n            - name: ROBOT_PASSWORD\\n              valueFrom:\\n                secretKeyRef:\\n                  key: robot-password\\n                  name: hcloud\\n                  optional: true\\n            - name: ROBOT_USER\\n              valueFrom:\\n                secretKeyRef:\\n                  key: robot-user\\n                  name: hcloud\\n                  optional: true\\n            - name: HCLOUD_NETWORK\\n              valueFrom:\\n                secretKeyRef:\\n                  key: network\\n                  name: hcloud\\n          image: docker.io/hetznercloud/hcloud-cloud-controller-manager:v1.26.0 # x-releaser-pleaser-version\\n          ports:\\n            - name: metrics\\n              containerPort: 8233\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 50Mi\\n      priorityClassName: system-cluster-critical\\n\"\n        - name: hcloud-csi\n          contents: \"\\\"apiVersion\\\": \\\"v1\\\"\\n\\\"data\\\":\\n  \\\"encryption-passphrase\\\": \\\"aXlJaUllVnpKY1l0cFFyWCZIQURkQ1lzWnd2RnRTTXB0dUgmRG51Q2VxJldic3Fl\\\"\\n\\\"kind\\\": \\\"Secret\\\"\\n\\\"metadata\\\":\\n  \\\"name\\\": \\\"hcloud-csi-secret\\\"\\n  \\\"namespace\\\": \\\"kube-system\\\"\\n\\\"type\\\": \\\"Opaque\\\"\\n\\n---\\n---\\n# Source: hcloud-csi/templates/controller/serviceaccount.yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nautomountServiceAccountToken: true\\n---\\n# Source: hcloud-csi/templates/core/storageclass.yaml\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: hcloud-volumes-encrypted-xfs\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \\\"true\\\"\\nprovisioner: csi.hetzner.cloud\\nvolumeBindingMode: WaitForFirstConsumer\\nallowVolumeExpansion: true\\nreclaimPolicy: \\\"Retain\\\"\\nparameters:\\n  csi.storage.k8s.io/fstype: xfs\\n  csi.storage.k8s.io/node-publish-secret-name: hcloud-csi-secret\\n  csi.storage.k8s.io/node-publish-secret-namespace: kube-system\\n  fsFormatOption: -i nrext64=1\\n---\\n# Source: hcloud-csi/templates/controller/clusterrole.yaml\\nkind: ClusterRole\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: hcloud-csi-controller\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nrules:\\n  # attacher\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [nodes]\\n    verbs: [get, list, watch]\\n  - apiGroups: [csi.storage.k8s.io]\\n    resources: [csinodeinfos]\\n    verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [csinodes]\\n    verbs: [get, list, watch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [volumeattachments]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [volumeattachments/status]\\n    verbs: [patch]\\n  # provisioner\\n  - apiGroups: [\\\"\\\"]\\n    resources: [secrets]\\n    verbs: [get, list]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumes]\\n    verbs: [get, list, watch, create, delete, patch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [persistentvolumeclaims, persistentvolumeclaims/status]\\n    verbs: [get, list, watch, update, patch]\\n  - apiGroups: [storage.k8s.io]\\n    resources: [storageclasses]\\n    verbs: [get, list, watch]\\n  - apiGroups: [\\\"\\\"]\\n    resources: [events]\\n    verbs: [list, watch, create, update, patch]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n    resources: [volumesnapshots]\\n    verbs: [get, list]\\n  - apiGroups: [snapshot.storage.k8s.io]\\n    resources: [volumesnapshotcontents]\\n    verbs: [get, list]\\n  # resizer\\n  - apiGroups: [\\\"\\\"]\\n    resources: [pods]\\n    verbs: [get, list, watch]\\n  # node\\n  - apiGroups: [\\\"\\\"]\\n    resources: [events]\\n    verbs: [get, list, watch, create, update, patch]\\n---\\n# Source: hcloud-csi/templates/controller/clusterrolebinding.yaml\\nkind: ClusterRoleBinding\\napiVersion: rbac.authorization.k8s.io/v1\\nmetadata:\\n  name: hcloud-csi-controller\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: hcloud-csi-controller\\nsubjects:\\n  - kind: ServiceAccount\\n    name: hcloud-csi-controller\\n    namespace: \\\"kube-system\\\"\\n---\\n# Source: hcloud-csi/templates/node/daemonset.yaml\\napiVersion: apps/v1\\nkind: DaemonSet\\nmetadata:\\n  name: hcloud-csi-node\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: node\\n    app: hcloud-csi\\nspec:\\n  updateStrategy:\\n    type: RollingUpdate\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance: hcloud-csi\\n      app.kubernetes.io/component: node\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart: hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by: Helm\\n        app.kubernetes.io/component: node\\n    spec:\\n      \\n      affinity:\\n        nodeAffinity:\\n          requiredDuringSchedulingIgnoredDuringExecution:\\n            nodeSelectorTerms:\\n            - matchExpressions:\\n              - key: instance.hetzner.cloud/is-root-server\\n                operator: NotIn\\n                values:\\n                - \\\"true\\\"\\n              - key: instance.hetzner.cloud/provided-by\\n                operator: NotIn\\n                values:\\n                - robot\\n      tolerations:\\n        - effect: NoExecute\\n          operator: Exists\\n        - effect: NoSchedule\\n          operator: Exists\\n        - key: CriticalAddonsOnly\\n          operator: Exists\\n      securityContext:\\n        fsGroup: 1001\\n      initContainers:\\n      containers:\\n        - name: csi-node-driver-registrar\\n          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.14.0\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - --kubelet-registration-path=/var/lib/kubelet/plugins/csi.hetzner.cloud/socket\\n          volumeMounts:\\n            - name: plugin-dir\\n              mountPath: /run/csi\\n            - name: registration-dir\\n              mountPath: /registration\\n          resources:\\n            limits: {}\\n            requests: {}\\n        - name: liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n          imagePullPolicy: IfNotPresent\\n          volumeMounts:\\n          - mountPath: /run/csi\\n            name: plugin-dir\\n          resources:\\n            limits: {}\\n            requests: {}\\n        - name: hcloud-csi-driver\\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - -node\\n          volumeMounts:\\n            - name: kubelet-dir\\n              mountPath: /var/lib/kubelet\\n              mountPropagation: \\\"Bidirectional\\\"\\n            - name: plugin-dir\\n              mountPath: /run/csi\\n            - name: device-dir\\n              mountPath: /dev\\n          securityContext:\\n            privileged: true\\n          env:\\n            - name: CSI_ENDPOINT\\n              value: unix:///run/csi/socket\\n            - name: ENABLE_METRICS\\n              value: \\\"false\\\"\\n          ports:\\n            - name: healthz\\n              protocol: TCP\\n              containerPort: 9808\\n          resources:\\n            limits: {}\\n            requests: {}\\n          livenessProbe:\\n            failureThreshold: 5\\n            initialDelaySeconds: 10\\n            periodSeconds: 2\\n            successThreshold: 1\\n            timeoutSeconds: 3\\n            httpGet:\\n              path: /healthz\\n              port: healthz\\n      volumes:\\n        - name: kubelet-dir\\n          hostPath:\\n            path: /var/lib/kubelet\\n            type: Directory\\n        - name: plugin-dir\\n          hostPath:\\n            path: /var/lib/kubelet/plugins/csi.hetzner.cloud/\\n            type: DirectoryOrCreate\\n        - name: registration-dir\\n          hostPath:\\n            path: /var/lib/kubelet/plugins_registry/\\n            type: Directory\\n        - name: device-dir\\n          hostPath:\\n            path: /dev\\n            type: Directory\\n---\\n# Source: hcloud-csi/templates/controller/deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: hcloud-csi-controller\\n  namespace: \\\"kube-system\\\"\\n  labels:\\n    app.kubernetes.io/name: hcloud-csi\\n    helm.sh/chart: hcloud-csi-2.17.0\\n    app.kubernetes.io/instance: hcloud-csi\\n    app.kubernetes.io/managed-by: Helm\\n    app.kubernetes.io/component: controller\\n    app: hcloud-csi-controller\\nspec:\\n  replicas: 1\\n  strategy:\\n    type: RollingUpdate\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: hcloud-csi\\n      app.kubernetes.io/instance: hcloud-csi\\n      app.kubernetes.io/component: controller\\n  template:\\n    metadata:\\n      labels:\\n        app.kubernetes.io/name: hcloud-csi\\n        helm.sh/chart: hcloud-csi-2.17.0\\n        app.kubernetes.io/instance: hcloud-csi\\n        app.kubernetes.io/managed-by: Helm\\n        app.kubernetes.io/component: controller\\n    spec:\\n      serviceAccountName: hcloud-csi-controller\\n      \\n      affinity:\\n        nodeAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:\\n          - preference:\\n              matchExpressions:\\n              - key: instance.hetzner.cloud/provided-by\\n                operator: In\\n                values:\\n                - cloud\\n            weight: 1\\n      nodeSelector:\\n        node-role.kubernetes.io/control-plane: \\\"\\\"\\n      tolerations:\\n        - effect: NoSchedule\\n          key: node-role.kubernetes.io/control-plane\\n          operator: Exists\\n      topologySpreadConstraints:\\n        - labelSelector:\\n            matchLabels:\\n              app.kubernetes.io/component: controller\\n              app.kubernetes.io/instance: hcloud-csi\\n              app.kubernetes.io/name: hcloud-csi\\n          maxSkew: 1\\n          topologyKey: kubernetes.io/hostname\\n          whenUnsatisfiable: ScheduleAnyway\\n      securityContext:\\n        fsGroup: 1001\\n      initContainers:\\n      containers:\\n        - name: csi-attacher\\n          image: registry.k8s.io/sig-storage/csi-attacher:v4.9.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          args:\\n            - --default-fstype=ext4\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: csi-resizer\\n          image: registry.k8s.io/sig-storage/csi-resizer:v1.14.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: csi-provisioner\\n          image: registry.k8s.io/sig-storage/csi-provisioner:v5.3.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          args:\\n            - --feature-gates=Topology=true\\n            - --default-fstype=ext4\\n            - --extra-create-metadata\\n          volumeMounts:\\n          - name: socket-dir\\n            mountPath: /run/csi\\n\\n        - name: liveness-probe\\n          image: registry.k8s.io/sig-storage/livenessprobe:v2.16.0\\n          imagePullPolicy: IfNotPresent\\n          resources:\\n            limits: {}\\n            requests: {}\\n          volumeMounts:\\n          - mountPath: /run/csi\\n            name: socket-dir\\n\\n        - name: hcloud-csi-driver\\n          image: docker.io/hetznercloud/hcloud-csi-driver:v2.17.0 # x-releaser-pleaser-version\\n          imagePullPolicy: IfNotPresent\\n          args:\\n            - -controller\\n          env:\\n            - name: CSI_ENDPOINT\\n              value: unix:///run/csi/socket\\n            - name: ENABLE_METRICS\\n              value: \\\"false\\\"\\n            - name: KUBE_NODE_NAME\\n              valueFrom:\\n                fieldRef:\\n                  apiVersion: v1\\n                  fieldPath: spec.nodeName\\n            - name: HCLOUD_TOKEN\\n              valueFrom:\\n                secretKeyRef:\\n                  name: hcloud\\n                  key: token\\n          resources:\\n            limits: {}\\n            requests: {}\\n          ports:\\n            - name: healthz\\n              protocol: TCP\\n              containerPort: 9808\\n          livenessProbe:\\n            failureThreshold: 5\\n            initialDelaySeconds: 10\\n            periodSeconds: 2\\n            successThreshold: 1\\n            timeoutSeconds: 3\\n            httpGet:\\n              path: /healthz\\n              port: healthz\\n          volumeMounts:\\n            - name: socket-dir\\n              mountPath: /run/csi\\n\\n      volumes:\\n        - name: socket-dir\\n          emptyDir: {}\\n---\\n# Source: hcloud-csi/templates/core/csidriver.yaml\\napiVersion: storage.k8s.io/v1\\nkind: CSIDriver\\nmetadata:\\n  name: csi.hetzner.cloud\\nspec:\\n  attachRequired: true\\n  fsGroupPolicy: File\\n  podInfoOnMount: true\\n  seLinuxMount: true\\n  volumeLifecycleModes:\\n  - Persistent\\n\\n\"\n        - name: talos-backup\n          contents: |+\n            \"apiVersion\": \"talos.dev/v1alpha1\"\n            \"kind\": \"ServiceAccount\"\n            \"metadata\":\n              \"name\": \"talos-backup-secrets\"\n              \"namespace\": \"kube-system\"\n            \"spec\":\n              \"roles\":\n              - \"os:etcd:backup\"\n\n            ---\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"access_key\": \"YzYwNDY2MDQ4ODdjMDgzZTk2OWUyMWJjNTVhMjQxNzQ=\"\n              \"secret_key\": \"MzI2NTEzNjY0ZWU4N2RiMTVmMWNjOGViZTcyMDBhYTEwMGI5YTFlNzRhZTM1NzZkNWJhMDg4MGQxMjg2YTlmNg==\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"talos-backup-s3-secrets\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n\n            ---\n            \"apiVersion\": \"batch/v1\"\n            \"kind\": \"CronJob\"\n            \"metadata\":\n              \"name\": \"talos-backup\"\n              \"namespace\": \"kube-system\"\n            \"spec\":\n              \"concurrencyPolicy\": \"Forbid\"\n              \"jobTemplate\":\n                \"spec\":\n                  \"template\":\n                    \"spec\":\n                      \"containers\":\n                      - \"env\":\n                        - \"name\": \"AWS_ACCESS_KEY_ID\"\n                          \"valueFrom\":\n                            \"secretKeyRef\":\n                              \"key\": \"access_key\"\n                              \"name\": \"talos-backup-s3-secrets\"\n                        - \"name\": \"AWS_SECRET_ACCESS_KEY\"\n                          \"valueFrom\":\n                            \"secretKeyRef\":\n                              \"key\": \"secret_key\"\n                              \"name\": \"talos-backup-s3-secrets\"\n                        - \"name\": \"AGE_X25519_PUBLIC_KEY\"\n                          \"value\": null\n                        - \"name\": \"DISABLE_ENCRYPTION\"\n                          \"value\": \"true\"\n                        - \"name\": \"AWS_REGION\"\n                          \"value\": \"auto\"\n                        - \"name\": \"CUSTOM_S3_ENDPOINT\"\n                          \"value\": \"https://a694d529ab7d7176bcac8585f8bafdf4.r2.cloudflarestorage.com\"\n                        - \"name\": \"BUCKET\"\n                          \"value\": \"etcd\"\n                        - \"name\": \"CLUSTER_NAME\"\n                          \"value\": \"goingdark\"\n                        - \"name\": \"S3_PREFIX\"\n                          \"value\": null\n                        - \"name\": \"USE_PATH_STYLE\"\n                          \"value\": \"true\"\n                        \"image\": \"ghcr.io/siderolabs/talos-backup:v0.1.0-beta.2-1-g9ccc125\"\n                        \"imagePullPolicy\": \"IfNotPresent\"\n                        \"name\": \"talos-backup\"\n                        \"resources\":\n                          \"limits\":\n                            \"cpu\": \"500m\"\n                            \"memory\": \"256Mi\"\n                          \"requests\":\n                            \"cpu\": \"250m\"\n                            \"memory\": \"128Mi\"\n                        \"securityContext\":\n                          \"allowPrivilegeEscalation\": false\n                          \"capabilities\":\n                            \"drop\":\n                            - \"ALL\"\n                          \"runAsGroup\": 1000\n                          \"runAsNonRoot\": true\n                          \"runAsUser\": 1000\n                          \"seccompProfile\":\n                            \"type\": \"RuntimeDefault\"\n                        \"volumeMounts\":\n                        - \"mountPath\": \"/tmp\"\n                          \"name\": \"tmp\"\n                        - \"mountPath\": \"/var/run/secrets/talos.dev\"\n                          \"name\": \"talos-secrets\"\n                        \"workingDir\": \"/tmp\"\n                      \"restartPolicy\": \"OnFailure\"\n                      \"tolerations\":\n                      - \"effect\": \"NoSchedule\"\n                        \"key\": \"node-role.kubernetes.io/control-plane\"\n                        \"operator\": \"Exists\"\n                      \"volumes\":\n                      - \"emptyDir\": {}\n                        \"name\": \"tmp\"\n                      - \"name\": \"talos-secrets\"\n                        \"secret\":\n                          \"secretName\": \"talos-backup-secrets\"\n              \"schedule\": \"0 * * * *\"\n              \"suspend\": false\n\n        - name: metrics-server\n          contents: |\n            ---\n            # Source: metrics-server/templates/pdb.yaml\n            apiVersion: policy/v1\n            kind: PodDisruptionBudget\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              minAvailable: 1\n\n              selector:\n                matchLabels:\n                  app.kubernetes.io/name: metrics-server\n                  app.kubernetes.io/instance: metrics-server\n            ---\n            # Source: metrics-server/templates/serviceaccount.yaml\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            ---\n            # Source: metrics-server/templates/clusterrole-aggregated-reader.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              name: system:metrics-server-aggregated-reader\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n                rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n                rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\n                rbac.authorization.k8s.io/aggregate-to-view: \"true\"\n            rules:\n              - apiGroups:\n                  - metrics.k8s.io\n                resources:\n                  - pods\n                  - nodes\n                verbs:\n                  - get\n                  - list\n                  - watch\n            ---\n            # Source: metrics-server/templates/clusterrole.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              name: system:metrics-server\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            rules:\n              - apiGroups:\n                - \"\"\n                resources:\n                - nodes/metrics\n                verbs:\n                - get\n              - apiGroups:\n                - \"\"\n                resources:\n                  - pods\n                  - nodes\n                  - namespaces\n                  - configmaps\n                verbs:\n                  - get\n                  - list\n                  - watch\n            ---\n            # Source: metrics-server/templates/clusterrolebinding-auth-delegator.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              name: metrics-server:system:auth-delegator\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: system:auth-delegator\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/clusterrolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              name: system:metrics-server\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: system:metrics-server\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/rolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: RoleBinding\n            metadata:\n              name: metrics-server-auth-reader\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: Role\n              name: extension-apiserver-authentication-reader\n            subjects:\n              - kind: ServiceAccount\n                name: metrics-server\n                namespace: kube-system\n            ---\n            # Source: metrics-server/templates/service.yaml\n            apiVersion: v1\n            kind: Service\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              type: ClusterIP\n              ports:\n                - name: https\n                  port: 443\n                  protocol: TCP\n                  targetPort: https\n                  appProtocol: https\n              selector:\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n            ---\n            # Source: metrics-server/templates/deployment.yaml\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              name: metrics-server\n              namespace: kube-system\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n            spec:\n              replicas: 2\n              selector:\n                matchLabels:\n                  app.kubernetes.io/name: metrics-server\n                  app.kubernetes.io/instance: metrics-server\n              template:\n                metadata:\n                  labels:\n                    app.kubernetes.io/name: metrics-server\n                    app.kubernetes.io/instance: metrics-server\n                spec:\n                  serviceAccountName: metrics-server\n                  priorityClassName: \"system-cluster-critical\"\n                  containers:\n                    - name: metrics-server\n                      securityContext:\n                        allowPrivilegeEscalation: false\n                        capabilities:\n                          drop:\n                          - ALL\n                        readOnlyRootFilesystem: true\n                        runAsNonRoot: true\n                        runAsUser: 1000\n                        seccompProfile:\n                          type: RuntimeDefault\n                      image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n                      imagePullPolicy: IfNotPresent\n                      args:\n                        - --secure-port=10250\n                        - --cert-dir=/tmp\n                        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n                        - --kubelet-use-node-status-port\n                        - --metric-resolution=15s\n                      ports:\n                      - name: https\n                        protocol: TCP\n                        containerPort: 10250\n                      livenessProbe:\n                        failureThreshold: 3\n                        httpGet:\n                          path: /livez\n                          port: https\n                          scheme: HTTPS\n                        initialDelaySeconds: 0\n                        periodSeconds: 10\n                      readinessProbe:\n                        failureThreshold: 3\n                        httpGet:\n                          path: /readyz\n                          port: https\n                          scheme: HTTPS\n                        initialDelaySeconds: 20\n                        periodSeconds: 10\n                      volumeMounts:\n                        - name: tmp\n                          mountPath: /tmp\n                      resources:\n                        requests:\n                          cpu: 100m\n                          memory: 200Mi\n                  volumes:\n                    - name: tmp\n                      emptyDir: {}\n                  topologySpreadConstraints:\n                    - labelSelector:\n                        matchLabels:\n                          app.kubernetes.io/instance: metrics-server\n                          app.kubernetes.io/name: metrics-server\n                      maxSkew: 1\n                      topologyKey: kubernetes.io/hostname\n                      whenUnsatisfiable: ScheduleAnyway\n            ---\n            # Source: metrics-server/templates/apiservice.yaml\n            apiVersion: apiregistration.k8s.io/v1\n            kind: APIService\n            metadata:\n              name: v1beta1.metrics.k8s.io\n              labels:\n                helm.sh/chart: metrics-server-3.13.0\n                app.kubernetes.io/name: metrics-server\n                app.kubernetes.io/instance: metrics-server\n                app.kubernetes.io/version: \"0.8.0\"\n                app.kubernetes.io/managed-by: Helm\n              annotations:\n            spec:\n              group: metrics.k8s.io\n              groupPriorityMinimum: 100\n              insecureSkipTLSVerify: true\n              service:\n                name: metrics-server\n                namespace: kube-system\n                port: 443\n              version: v1beta1\n              versionPriority: 100\n        - name: cluster-autoscaler\n          contents: |+\n            ---\n            # Source: cluster-autoscaler/templates/pdb.yaml\n            apiVersion: policy/v1\n            kind: PodDisruptionBudget\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              selector:\n                matchLabels:\n                  app.kubernetes.io/instance: \"cluster-autoscaler\"\n                  app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n            ---\n            # Source: cluster-autoscaler/templates/serviceaccount.yaml\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            automountServiceAccountToken: true\n            ---\n            # Source: cluster-autoscaler/templates/clusterrole.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRole\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            rules:\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - events\n                  - endpoints\n                verbs:\n                  - create\n                  - patch\n              - apiGroups:\n                - \"\"\n                resources:\n                - pods/eviction\n                verbs:\n                - create\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - pods/status\n                verbs:\n                  - update\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - endpoints\n                resourceNames:\n                  - cluster-autoscaler\n                verbs:\n                  - get\n                  - update\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - nodes\n                verbs:\n                - watch\n                - list\n                - create\n                - delete\n                - get\n                - update\n              - apiGroups:\n                - \"\"\n                resources:\n                  - namespaces\n                  - pods\n                  - services\n                  - replicationcontrollers\n                  - persistentvolumeclaims\n                  - persistentvolumes\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                - batch\n                resources:\n                  - jobs\n                  - cronjobs\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                - batch\n                - extensions\n                resources:\n                - jobs\n                verbs:\n                - get\n                - list\n                - patch\n                - watch\n              - apiGroups:\n                  - extensions\n                resources:\n                  - replicasets\n                  - daemonsets\n                verbs:\n                  - watch\n                  - list\n                  - get\n              - apiGroups:\n                  - policy\n                resources:\n                  - poddisruptionbudgets\n                verbs:\n                  - watch\n                  - list\n              - apiGroups:\n                - apps\n                resources:\n                - daemonsets\n                - replicasets\n                - statefulsets\n                verbs:\n                - watch\n                - list\n                - get\n              - apiGroups:\n                - storage.k8s.io\n                resources:\n                - storageclasses\n                - csinodes\n                - csidrivers\n                - csistoragecapacities\n                - volumeattachments\n                verbs:\n                - watch\n                - list\n                - get\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                verbs:\n                  - list\n                  - watch\n                  - get\n              - apiGroups:\n                - coordination.k8s.io\n                resources:\n                - leases\n                verbs:\n                - create\n              - apiGroups:\n                - coordination.k8s.io\n                resourceNames:\n                - cluster-autoscaler\n                resources:\n                - leases\n                verbs:\n                - get\n                - update\n            ---\n            # Source: cluster-autoscaler/templates/clusterrolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            subjects:\n              - kind: ServiceAccount\n                name: cluster-autoscaler-hetzner-cluster-autoscaler\n                namespace: kube-system\n            ---\n            # Source: cluster-autoscaler/templates/role.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: Role\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            rules:\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                verbs:\n                  - create\n              - apiGroups:\n                  - \"\"\n                resources:\n                  - configmaps\n                resourceNames:\n                  - cluster-autoscaler-status\n                verbs:\n                  - delete\n                  - get\n                  - update\n            ---\n            # Source: cluster-autoscaler/templates/rolebinding.yaml\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: RoleBinding\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: Role\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n            subjects:\n              - kind: ServiceAccount\n                name: cluster-autoscaler-hetzner-cluster-autoscaler\n                namespace: kube-system\n            ---\n            # Source: cluster-autoscaler/templates/service.yaml\n            apiVersion: v1\n            kind: Service\n            metadata:\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              ports:\n                - port: 8085\n                  protocol: TCP\n                  targetPort: 8085\n                  name: http\n              selector:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n              type: \"ClusterIP\"\n            ---\n            # Source: cluster-autoscaler/templates/deployment.yaml\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              annotations:\n                {}\n              labels:\n                app.kubernetes.io/instance: \"cluster-autoscaler\"\n                app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                app.kubernetes.io/managed-by: \"Helm\"\n                helm.sh/chart: \"cluster-autoscaler-9.50.1\"\n              name: cluster-autoscaler-hetzner-cluster-autoscaler\n              namespace: kube-system\n            spec:\n              replicas: 1\n              revisionHistoryLimit: 10\n              selector:\n                matchLabels:\n                  app.kubernetes.io/instance: \"cluster-autoscaler\"\n                  app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n              template:\n                metadata:\n                  labels:\n                    app.kubernetes.io/instance: \"cluster-autoscaler\"\n                    app.kubernetes.io/name: \"hetzner-cluster-autoscaler\"\n                spec:\n                  priorityClassName: \"system-cluster-critical\"\n                  dnsPolicy: \"ClusterFirst\"\n                  containers:\n                    - name: hetzner-cluster-autoscaler\n                      image: \"registry.k8s.io/autoscaling/cluster-autoscaler:v1.33.0\"\n                      imagePullPolicy: \"IfNotPresent\"\n                      command:\n                        - ./cluster-autoscaler\n                        - --cloud-provider=hetzner\n                        - --namespace=kube-system\n                        - --nodes=0:2:cx43:hel1:goingdark-autoscaler\n                        - --balance-similar-node-groups=true\n                        - --expander=least-waste\n                        - --logtostderr=true\n                        - --scale-down-delay-after-add=10m\n                        - --scale-down-delay-after-delete=10m\n                        - --scale-down-unneeded-time=8m\n                        - --scale-down-utilization-threshold=0.75\n                        - --stderrthreshold=info\n                        - --v=4\n                      env:\n                        - name: POD_NAMESPACE\n                          valueFrom:\n                            fieldRef:\n                              fieldPath: metadata.namespace\n                        - name: SERVICE_ACCOUNT\n                          valueFrom:\n                            fieldRef:\n                              fieldPath: spec.serviceAccountName\n                        - name: HCLOUD_CLUSTER_CONFIG_FILE\n                          value: \"/config/cluster-config\"\n                        - name: HCLOUD_FIREWALL\n                          value: \"2379867\"\n                        - name: HCLOUD_NETWORK\n                          value: \"11368893\"\n                        - name: HCLOUD_PUBLIC_IPV4\n                          value: \"true\"\n                        - name: HCLOUD_PUBLIC_IPV6\n                          value: \"false\"\n                        - name: HCLOUD_SERVER_CREATION_TIMEOUT\n                          value: \"10\"\n                        - name: HCLOUD_SSH_KEY\n                          value: \"101120850\"\n                        - name: HCLOUD_TOKEN\n                          valueFrom:\n                            secretKeyRef:\n                              name: hcloud\n                              key: token\n                      livenessProbe:\n                        httpGet:\n                          path: /health-check\n                          port: 8085\n                      ports:\n                        - containerPort: 8085\n                      resources:\n                        {}\n                      volumeMounts:\n                        - name: cluster-autoscaler-hetzner-config\n                          mountPath: /config\n                          readOnly: true\n                  nodeSelector:\n                    node-role.kubernetes.io/control-plane: \"\"\n                  serviceAccountName: cluster-autoscaler-hetzner-cluster-autoscaler\n                  tolerations:\n                    - effect: NoSchedule\n                      key: node-role.kubernetes.io/control-plane\n                      operator: Exists\n                  topologySpreadConstraints:\n                    - labelSelector:\n                        matchLabels:\n                          app.kubernetes.io/instance: cluster-autoscaler\n                          app.kubernetes.io/name: hetzner-cluster-autoscaler\n                      maxSkew: 1\n                      topologyKey: kubernetes.io/hostname\n                      whenUnsatisfiable: ScheduleAnyway\n                  volumes:\n                    - name: cluster-autoscaler-hetzner-config\n                      secret:\n                        secretName: cluster-autoscaler-hetzner-config\n\n            ---\n            \"apiVersion\": \"v1\"\n            \"data\":\n              \"cluster-config\": \"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\"\n            \"kind\": \"Secret\"\n            \"metadata\":\n              \"name\": \"cluster-autoscaler-hetzner-config\"\n              \"namespace\": \"kube-system\"\n            \"type\": \"Opaque\"\n\n        - name: cilium-settings\n          contents: \"apiVersion: cilium.io/v2alpha1\\nkind: CiliumL2AnnouncementPolicy\\nmetadata:\\n  name: default-l2-announcement-policy\\n  namespace: kube-system\\nspec:\\n  externalIPs: true\\n  loadBalancerIPs: true\\n\\n---\\napiVersion: cilium.io/v2alpha1\\nkind: CiliumLoadBalancerIPPool\\nmetadata:\\n  name: service-pool\\nspec:\\n  blocks:\\n    - start: 10.0.96.240\\n      stop: 10.0.96.250\\n\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: cilium-helm-values\\n  namespace: kube-system\\n  labels:\\n    app.kubernetes.io/name: cilium-helm-values\\n    app.kubernetes.io/managed-by: talos-inline\\n  data:\\n    values.yaml: |-\\n# https://github.com/cilium/cilium/blob/main/install/kubernetes/cilium/values.yaml\\n      cluster:\\n        name: talos\\n        id: 1\\n      \\n      # Correct boolean, not a nested map\\n      kubeProxyReplacement: true\\n      \\n        # Talos specific\\n      k8sServiceHost: localhost\\n      k8sServicePort: 7445\\n      securityContext:\\n        capabilities:\\n          ciliumAgent:\\n            [CHOWN, KILL, NET_ADMIN, NET_RAW, IPC_LOCK, SYS_ADMIN, SYS_RESOURCE, DAC_OVERRIDE, FOWNER, SETGID, SETUID]\\n          cleanCiliumState: [NET_ADMIN, SYS_ADMIN, SYS_RESOURCE]\\n      \\n      cgroup:\\n        autoMount:\\n          enabled: false\\n        hostRoot: /sys/fs/cgroup\\n      \\n      # https://www.talos.dev/latest/talos-guides/network/host-dns/#forwarding-kube-dns-to-host-dns\\n      # https://docs.cilium.io/en/stable/operations/performance/tuning/#ebpf-host-routing\\n      bpf:\\n        hostLegacyRouting: true\\n      \\n      # https://docs.cilium.io/en/stable/network/concepts/ipam/\\n      ipam:\\n        mode: kubernetes\\n        multiPoolPreAllocation: ''\\n      \\n      enableMulticast: false\\n      multicast:\\n        enabled: false\\n      \\n      \\n      operator:\\n        rollOutPods: true\\n        resources:\\n          requests:\\n            cpu: 50m\\n            memory: 128Mi\\n          limits:\\n            memory: 256Mi\\n      \\n      # Roll out cilium agent pods automatically when ConfigMap is updated.\\n      rollOutCiliumPods: true\\n      agent: true\\n      agentConfig:\\n        tolerations:\\n          - key: gpu\\n            operator: Equal\\n            value: \\\"true\\\"\\n            effect: NoSchedule\\n        resources:\\n          requests:\\n            cpu: 100m      # Reduced from 500m - now Burstable QoS\\n            memory: 128Mi   # Reduced from 512Mi\\n          limits:\\n            memory: 384Mi   # Remove CPU limit to prevent throttling\\n      \\n      \\n      \\n      #debug:\\n      #  enabled: true\\n      \\n      # Increase rate limit when doing L2 announcements\\n      k8sClientRateLimit:\\n        qps: 20\\n        burst: 100\\n      \\n      l2announcements:\\n        enabled: true\\n      \\n      externalIPs:\\n        enabled: true\\n      \\n      #enableCiliumEndpointSlice: true\\n      \\n      loadBalancer:\\n        # https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#maglev-consistent-hashing\\n        algorithm: maglev\\n      \\n      gatewayAPI:\\n        enabled: true\\n        gatewayClass:\\n          create: \\\"true\\\"\\n        enableAlpn: true\\n        xdsServer:\\n          enabled: true\\n      envoy:\\n        securityContext:\\n          capabilities:\\n            keepCapNetBindService: true\\n            envoy: [NET_BIND_SERVICE, NET_ADMIN, PERFMON, BPF]\\n      \\n      hubble:\\n        peerService:\\n          clusterDomain: cluster.local\\n        enabled: true\\n        relay:\\n          enabled: true\\n          rollOutPods: true\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 128Mi\\n            limits:\\n              cpu: 200m\\n              memory: 256Mi\\n        ui:\\n          enabled: true\\n          rollOutPods: true\\n          resources:\\n            requests:\\n              cpu: 100m\\n              memory: 128Mi\\n            limits:\\n              cpu: 200m\\n              memory: 256Mi\\n      \\n      ingressController:\\n        enabled: false\\n        default: true\\n        loadbalancerMode: shared\\n        service:\\n          annotations:\\n            io.cilium/lb-ipam-ips: 10.0.96.243\\n      \\n      # mTLS\\n      authentication:\\n        enabled: false\\n        mutual:\\n          spire:\\n            enabled: false\\n            install:\\n              server:\\n                dataStorage:\\n                  storageClass: cilium-spire-sc\\n\"\n    adminKubeconfig:\n        certLifetime: 87600h0m0s\n    allowSchedulingOnControlPlanes: false\n","node":"10.0.64.1","on_destroy":{"graceful":true,"reboot":false,"reset":true},"timeouts":null},"sensitive_attributes":[[{"type":"get_attr","value":"machine_configuration"}],[{"type":"get_attr","value":"client_configuration"},{"type":"get_attr","value":"client_key"}],[{"type":"get_attr","value":"machine_configuration_input"}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"talos_machine_configuration_apply","name":"worker","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"index_key":"goingdark-worker-1","schema_version":1,"attributes":{"apply_mode":"auto","client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","client_key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"},"config_patches":null,"endpoint":"46.62.203.120","id":"machine_configuration_apply","machine_configuration":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: worker\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints: []\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-worker-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n        vm.max_map_count: \"262144\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        nodepool: worker\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    proxy:\n        disabled: true\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n","machine_configuration_input":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: worker\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints: []\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-worker-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n        vm.max_map_count: \"262144\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        nodepool: worker\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    proxy:\n        disabled: true\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n","node":"10.0.65.1","on_destroy":{"graceful":true,"reboot":false,"reset":true},"timeouts":null},"sensitive_attributes":[[{"type":"get_attr","value":"client_configuration"},{"type":"get_attr","value":"client_key"}],[{"type":"get_attr","value":"machine_configuration_input"}],[{"type":"get_attr","value":"machine_configuration"}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]},{"index_key":"goingdark-worker-secondary-1","schema_version":1,"attributes":{"apply_mode":"auto","client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","client_key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"},"config_patches":null,"endpoint":"135.181.92.222","id":"machine_configuration_apply","machine_configuration":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: worker\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints: []\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-worker-secondary-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n        vm.max_map_count: \"262144\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        nodepool: worker-secondary\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    proxy:\n        disabled: true\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n","machine_configuration_input":"version: v1alpha1\ndebug: false\npersist: true\nmachine:\n    type: worker\n    token: 7or9kx.41p55ltb292998sw\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    certSANs:\n        - 10.0.64.1\n        - 10.0.64.126\n        - 10.0.64.254\n        - 127.0.0.1\n        - 46.62.164.172\n        - ::1\n        - localhost\n    kubelet:\n        image: ghcr.io/siderolabs/kubelet:v1.33.4\n        extraArgs:\n            cloud-provider: external\n            rotate-server-certificates: \"true\"\n        extraConfig:\n            kubeReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 350Mi\n            registerWithTaints: []\n            shutdownGracePeriod: 90s\n            shutdownGracePeriodCriticalPods: 15s\n            systemReserved:\n                cpu: 100m\n                ephemeral-storage: 1Gi\n                memory: 300Mi\n        defaultRuntimeSeccompProfileEnabled: true\n        nodeIP:\n            validSubnets:\n                - 10.0.64.0/19\n        disableManifestsDirectory: true\n    network:\n        hostname: goingdark-worker-secondary-1\n        interfaces:\n            - interface: eth0\n              dhcp: true\n              dhcpOptions:\n                routeMetric: 0\n                ipv4: true\n                ipv6: false\n            - interface: eth1\n              dhcp: true\n        nameservers:\n            - 185.12.64.1\n            - 185.12.64.2\n            - 2a01:4ff:ff00::add:1\n            - 2a01:4ff:ff00::add:2\n    install:\n        disk: /dev/sda\n        image: factory.talos.dev/hcloud-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.11.1\n        wipe: false\n    time:\n        servers:\n            - ntp1.hetzner.de\n            - ntp2.hetzner.com\n            - ntp3.hetzner.net\n    sysctls:\n        net.core.netdev_max_backlog: \"4096\"\n        net.core.somaxconn: \"65535\"\n        net.ipv6.conf.all.disable_ipv6: \"0\"\n        net.ipv6.conf.default.disable_ipv6: \"0\"\n        vm.max_map_count: \"262144\"\n    systemDiskEncryption:\n        state:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n        ephemeral:\n            provider: luks2\n            keys:\n                - nodeID: {}\n                  slot: 0\n            options:\n                - no_read_workqueue\n                - no_write_workqueue\n    features:\n        rbac: true\n        stableHostname: true\n        apidCheckExtKeyUsage: true\n        diskQuotaSupport: true\n        kubePrism:\n            enabled: true\n            port: 7445\n        hostDNS:\n            enabled: true\n            forwardKubeDNSToHost: false\n            resolveMemberNames: true\n    logging:\n        destinations: []\n    kernel: {}\n    nodeLabels:\n        nodepool: worker-secondary\ncluster:\n    id: PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=\n    secret: z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU=\n    controlPlane:\n        endpoint: https://10.0.64.126:6443\n    clusterName: goingdark\n    network:\n        cni:\n            name: none\n        dnsDomain: cluster.local\n        podSubnets:\n            - 10.0.128.0/17\n        serviceSubnets:\n            - 10.0.96.0/19\n    token: qztdq7.ktojonbxqnrcsz3q\n    ca:\n        crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n        key: \"\"\n    proxy:\n        disabled: true\n    discovery:\n        enabled: true\n        registries:\n            kubernetes:\n                disabled: true\n            service:\n                disabled: false\n","node":"10.0.65.129","on_destroy":{"graceful":true,"reboot":false,"reset":true},"timeouts":null},"sensitive_attributes":[[{"type":"get_attr","value":"client_configuration"},{"type":"get_attr","value":"client_key"}],[{"type":"get_attr","value":"machine_configuration_input"}],[{"type":"get_attr","value":"machine_configuration"}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"talos_machine_secrets","name":"this","provider":"provider[\"registry.opentofu.org/siderolabs/talos\"]","instances":[{"schema_version":1,"attributes":{"client_configuration":{"ca_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","client_certificate":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJLVENCM0tBREFnRUNBaEVBNG1hWUFKby9OUVhGZGc5bXZFTG1xREFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTWpZd09ESXpNRGt4TWpRMldqQVRNUkV3RHdZRApWUVFLRXdodmN6cGhaRzFwYmpBcU1BVUdBeXRsY0FNaEFCRWlqdzVkY0U2VkI2S1VmNElUMi8yTmFlK1h6aHliClRvVldLc1hSbUxDZ28wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0I0QXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVTUXRpQjdpUE9qN255K29XNS82YzVvSjhTdHd3QlFZREsyVndBMEVBZlpmSgpLdFR4dVBTd0FCdDhwRXZUYXFlN3VCZldIejN5aVRpU29ramEwRXFMMVR1WTRPTkxEVi8xYUVZOGN5ckV4SXN3CndNM1JVQTEyZzBVOW55cTVDUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","client_key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJSmZlYWk0LzY2MmJSei82SDZHWlRLTWt5M2hjL0dKc0pwam9BbnFpNFNLQQotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"},"id":"machine_secrets","machine_secrets":{"certs":{"etcd":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJmVENDQVNPZ0F3SUJBZ0lRVmcycituc2QzVFluYVlrSXg5MnBOekFLQmdncWhrak9QUVFEQWpBUE1RMHcKQ3dZRFZRUUtFd1JsZEdOa01CNFhEVEkxTURneU16QTVNVEkwTmxvWERUTTFNRGd5TVRBNU1USTBObG93RHpFTgpNQXNHQTFVRUNoTUVaWFJqWkRCWk1CTUdCeXFHU000OUFnRUdDQ3FHU000OUF3RUhBMElBQklMQ05QTis1SnI3ClJZOFQwNUF0TG9UUFJZSlJqM2pQckJBci9PNGdJYWRtV0MzWDBwQ2xRT0hnKzU1bUY4ZlJuK3J1RzEvR2hwVEoKZGEvWDVTcC9RbHlqWVRCZk1BNEdBMVVkRHdFQi93UUVBd0lDaERBZEJnTlZIU1VFRmpBVUJnZ3JCZ0VGQlFjRApBUVlJS3dZQkJRVUhBd0l3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVdmpZTzZSazFmTE9NCmNPSzNwdjFMdWQ2akVxa3dDZ1lJS29aSXpqMEVBd0lEU0FBd1JRSWdHcTlua1c2S0ZWMnh4YjV1bk8vL2dLOGEKRXBDbHF6NkMzYjQzWkltVzR2QUNJUUQ0d1pWSGFQak85NmQxQTNCWGVQdUVEY2JHVE9JNWY3cjFuSEROWmRVMwp0UT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBGaUNwUjR2VnF6ejVCbzFGL0IrQm15cjRIdWU0eFc0S1hPeWpQdVJWWjBvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFZ3NJMDgzN2ttdnRGanhQVGtDMHVoTTlGZ2xHUGVNK3NFQ3Y4N2lBaHAyWllMZGZTa0tWQQo0ZUQ3bm1ZWHg5R2Y2dTRiWDhhR2xNbDFyOWZsS245Q1hBPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJpVENDQVMrZ0F3SUJBZ0lRTEp1bW9XVGtXKzJseURVb2V0VUZTREFLQmdncWhrak9QUVFEQWpBVk1STXcKRVFZRFZRUUtFd3ByZFdKbGNtNWxkR1Z6TUI0WERUSTFNRGd5TXpBNU1USTBObG9YRFRNMU1EZ3lNVEE1TVRJMApObG93RlRFVE1CRUdBMVVFQ2hNS2EzVmlaWEp1WlhSbGN6QlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VICkEwSUFCSytnbU5kaFBnNXU3OFFlaEREVWhoTGs1dDAyejFYaHBBa2VFeGRmbWNBT1dKOTdFWVVjb3ArMzR3eEgKT2VpUkVZbTZxVXRGaGliSm5RNlVZcGdVWUJhallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWRCZ05WSFNVRQpGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFCkZnUVVZMjZzTS9MR0xUTzNIbmw1cVkzMFlBR3BOekV3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnWThHb3g5SSsKbDg3ZjJiZHQyUkpad0FCTW5lcENjUDJKTHVOYSszcjlIbTBDSVFDZE1UVE5QREJ4RDZJcEVGZGF2N1RmUE5pVwo5dlVCQmk3RHQrdFh3WjJxc0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSVBjRTdVTWg2MWRXa25jb2YyVStHK0VXeVoyUlRBc0R6dDZ6NjBCeDlIODJvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFcjZDWTEyRStEbTd2eEI2RU1OU0dFdVRtM1RiUFZlR2tDUjRURjErWndBNVluM3NSaFJ5aQpuN2ZqREVjNTZKRVJpYnFwUzBXR0pzbWREcFJpbUJSZ0ZnPT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_aggregator":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJYakNDQVFXZ0F3SUJBZ0lRV0xLNzhuN2pKUW1JM3Zva2pCOHhmekFLQmdncWhrak9QUVFEQWpBQU1CNFgKRFRJMU1EZ3lNekE1TVRJME5sb1hEVE0xTURneU1UQTVNVEkwTmxvd0FEQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxRwpTTTQ5QXdFSEEwSUFCTENrMVJxQW0xaldhdVE5RE5EUUV5cU8rZVV3VzdsN0dpRmZ2a2dIOGNRYTM1RCtXNU5jCjdZam5yM3o0TEhwUTNnbXNVSDFXWnVhY1QwY2k2VHU0U0hlallUQmZNQTRHQTFVZER3RUIvd1FFQXdJQ2hEQWQKQmdOVkhTVUVGakFVQmdnckJnRUZCUWNEQVFZSUt3WUJCUVVIQXdJd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZApCZ05WSFE0RUZnUVUyenRhZmpjTUdvRkVpSmFZQklOeXZrR3FsMWd3Q2dZSUtvWkl6ajBFQXdJRFJ3QXdSQUlnCkVVRzc0cG9KQUFhRGQvZDJIWUFPVTJyMVl0Y0QxazFvQ0pnUldNc0FJUUVDSUUzbEhnaFdrYmhVQWhlNzdEVlIKNmIyc0xJT1MwQm1ZK2ZpZUx1Tk1UdjdXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K","key":"LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSUdsQUp1YnlSWVJOVi91K2s1bjJsM2k1WVR6VWtaN3lKbGZHWEZBQjNYK1FvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFc0tUVkdvQ2JXTlpxNUQwTTBOQVRLbzc1NVRCYnVYc2FJVisrU0FmeHhCcmZrUDViazF6dAppT2V2ZlBnc2VsRGVDYXhRZlZabTVweFBSeUxwTzdoSWR3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo="},"k8s_serviceaccount":{"key":"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"},"os":{"cert":"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJQekNCOHFBREFnRUNBaEVBK3JVdkJGVCtlZ1FUZ2Z6RVR6OU01REFGQmdNclpYQXdFREVPTUF3R0ExVUUKQ2hNRmRHRnNiM013SGhjTk1qVXdPREl6TURreE1qUTJXaGNOTXpVd09ESXhNRGt4TWpRMldqQVFNUTR3REFZRApWUVFLRXdWMFlXeHZjekFxTUFVR0F5dGxjQU1oQUlKVWxJSnNnMGZRdFl1V1ZlbkxQWVFuckdsTjdmeEZKMGhPCnhNcG5Vc1JKbzJFd1h6QU9CZ05WSFE4QkFmOEVCQU1DQW9Rd0hRWURWUjBsQkJZd0ZBWUlLd1lCQlFVSEF3RUcKQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3SFFZRFZSME9CQllFRkVrTFlnZTRqem8rNTh2cQpGdWYrbk9hQ2ZFcmNNQVVHQXl0bGNBTkJBT1BFS3hHa2N0eUJRbW5HZWNXTllrQVFvZXY3b1FLcUVsKzAzNDlwCnlKeUd5Ly9OT2lSSjNkZWh3TEcvSVFLYS83OS9CUDlneGpsN2ZxdmU4WHNLYWdNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==","key":"LS0tLS1CRUdJTiBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0KTUM0Q0FRQXdCUVlESzJWd0JDSUVJQXhZNXllRC91SzJjK0hQWWtGK1Y5d0tNa0RGK3UxRXZmMVhrSnJZbnYzbwotLS0tLUVORCBFRDI1NTE5IFBSSVZBVEUgS0VZLS0tLS0K"}},"cluster":{"id":"PVLXhVhmj8mTQ5uNTdbW-WOjNxPDHAZttQimllQIrVo=","secret":"z5WBUfH5UY9iO68OTv2x0qv5qs6VcchYIwCNykxi3UU="},"secrets":{"aescbc_encryption_secret":null,"bootstrap_token":"qztdq7.ktojonbxqnrcsz3q","secretbox_encryption_secret":"vGp7mw/kgDN0+XjgOWs7VxQL8dYpJmC23WwQosqkKcE="},"trustdinfo":{"token":"7or9kx.41p55ltb292998sw"}},"talos_version":"v1.11.1"},"sensitive_attributes":[[{"type":"get_attr","value":"client_configuration"},{"type":"get_attr","value":"client_key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"trustdinfo"},{"type":"get_attr","value":"token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_serviceaccount"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"os"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"etcd"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"certs"},{"type":"get_attr","value":"k8s_aggregator"},{"type":"get_attr","value":"key"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"cluster"},{"type":"get_attr","value":"secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"bootstrap_token"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"secretbox_encryption_secret"}],[{"type":"get_attr","value":"machine_secrets"},{"type":"get_attr","value":"secrets"},{"type":"get_attr","value":"aescbc_encryption_secret"}]]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"amd64_image","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"id":"0eadadd0-ced3-96cf-8ffb-02dd5640d550","input":null,"output":null,"triggers_replace":{"value":["goingdark","v1.11.1","ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515"],"type":["tuple",["string","string","string"]]}},"sensitive_attributes":[],"dependencies":["module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.terraform_data.packer_init"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"create_kubeconfig","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"id":"791ff71f-88c2-2025-67ee-37fc1feb1392","input":{"value":{"cluster_kubeconfig_path":"kubeconfig"},"type":["object",{"cluster_kubeconfig_path":"string"}]},"output":{"value":{"cluster_kubeconfig_path":"kubeconfig"},"type":["object",{"cluster_kubeconfig_path":"string"}]},"triggers_replace":{"value":["70d0acce9f7f468e36f346a97a561dd40e17d12d","kubeconfig"],"type":["tuple",["string","string"]]}},"sensitive_attributes":[[{"type":"get_attr","value":"triggers_replace"},{"type":"index","value":{"value":0,"type":"number"}}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_cluster_kubeconfig.this","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"create_talosconfig","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"index_key":0,"schema_version":0,"attributes":{"id":"2e32926f-3397-7df3-5125-5e66c2fb0622","input":{"value":{"cluster_talosconfig_path":"talosconfig"},"type":["object",{"cluster_talosconfig_path":"string"}]},"output":{"value":{"cluster_talosconfig_path":"talosconfig"},"type":["object",{"cluster_talosconfig_path":"string"}]},"triggers_replace":{"value":["73fdce78b1de8cf426621a4d42013cd5067a1863","talosconfig"],"type":["tuple",["string","string"]]}},"sensitive_attributes":[[{"type":"get_attr","value":"triggers_replace"},{"type":"index","value":{"value":0,"type":"number"}}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"packer_init","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"schema_version":0,"attributes":{"id":"26974623-ad79-1f9d-2cff-c6d90a00e4e7","input":null,"output":null,"triggers_replace":{"value":["bc57eff19a69b99cb293014e82f47e0277ffb04e","goingdark","v1.11.1","ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515"],"type":["tuple",["string","string","string","string"]]}},"sensitive_attributes":[],"dependencies":["module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.talos_image_factory_schematic.this"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"synchronize_manifests","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"schema_version":0,"attributes":{"id":"82e2918c-bd22-3bda-5447-22f942147cd2","input":null,"output":null,"triggers_replace":{"value":["10a01d8f535e44f60fde84a324a97dcfc095b8e9","v1.10.1","v0.85.0"],"type":["tuple",["string","string","string"]]}},"sensitive_attributes":[[{"type":"get_attr","value":"triggers_replace"},{"type":"index","value":{"value":0,"type":"number"}}]],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_load_balancer_network.kube_api","module.kubernetes.hcloud_load_balancer_service.kube_api","module.kubernetes.hcloud_load_balancer_target.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_bootstrap.this","module.kubernetes.talos_machine_configuration_apply.control_plane","module.kubernetes.talos_machine_configuration_apply.worker","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_kubernetes","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"talos_health_data","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"schema_version":0,"attributes":{"id":"83be26d9-d9a3-cc73-78bd-78f76696eb4b","input":{"value":{"control_plane_nodes":["10.0.64.1"],"current_ip":[],"endpoints":["46.62.164.172"],"kube_api_url":"https://46.62.164.172:6443","worker_nodes":["10.0.65.1","10.0.65.129"]},"type":["object",{"control_plane_nodes":["tuple",["string"]],"current_ip":["list","string"],"endpoints":["list","string"],"kube_api_url":"string","worker_nodes":["tuple",["string","string"]]}]},"output":{"value":{"control_plane_nodes":["10.0.64.1"],"current_ip":[],"endpoints":["46.62.164.172"],"kube_api_url":"https://46.62.164.172:6443","worker_nodes":["10.0.65.1","10.0.65.129"]},"type":["object",{"control_plane_nodes":["tuple",["string"]],"current_ip":["list","string"],"endpoints":["list","string"],"kube_api_url":"string","worker_nodes":["tuple",["string","string"]]}]},"triggers_replace":null},"sensitive_attributes":[],"dependencies":["module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"upgrade_control_plane","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"schema_version":0,"attributes":{"id":"e772e4c1-22fe-8bfe-9d0f-7db86962f299","input":null,"output":null,"triggers_replace":{"value":["v1.11.1","ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515"],"type":["tuple",["string","string"]]}},"sensitive_attributes":[],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"upgrade_kubernetes","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"schema_version":0,"attributes":{"id":"32f2255a-bf5c-ce02-f785-3e4863686ce9","input":null,"output":null,"triggers_replace":{"value":["v1.33.4"],"type":["tuple",["string"]]}},"sensitive_attributes":[],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.terraform_data.upgrade_worker","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"terraform_data","name":"upgrade_worker","provider":"provider[\"terraform.io/builtin/terraform\"]","instances":[{"schema_version":0,"attributes":{"id":"96bf82d7-1a53-106f-87a8-8ecb1c3724ff","input":null,"output":null,"triggers_replace":{"value":["v1.11.1","ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515"],"type":["tuple",["string","string"]]}},"sensitive_attributes":[],"dependencies":["module.kubernetes.data.hcloud_certificates.state","module.kubernetes.data.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.data.hcloud_image.amd64","module.kubernetes.data.hcloud_image.arm64","module.kubernetes.data.hcloud_images.amd64","module.kubernetes.data.hcloud_images.arm64","module.kubernetes.data.hcloud_network.this","module.kubernetes.data.helm_template.cert_manager","module.kubernetes.data.helm_template.cilium","module.kubernetes.data.helm_template.cluster_autoscaler","module.kubernetes.data.helm_template.hcloud_ccm","module.kubernetes.data.helm_template.hcloud_csi","module.kubernetes.data.helm_template.ingress_nginx","module.kubernetes.data.helm_template.longhorn","module.kubernetes.data.helm_template.metrics_server","module.kubernetes.data.http.current_ipv4","module.kubernetes.data.http.current_ipv6","module.kubernetes.data.talos_client_configuration.this","module.kubernetes.data.talos_image_factory_extensions_versions.this","module.kubernetes.data.talos_image_factory_urls.amd64","module.kubernetes.data.talos_image_factory_urls.arm64","module.kubernetes.data.talos_machine_configuration.cluster_autoscaler","module.kubernetes.data.talos_machine_configuration.control_plane","module.kubernetes.data.talos_machine_configuration.worker","module.kubernetes.hcloud_firewall.this","module.kubernetes.hcloud_floating_ip.control_plane_ipv4","module.kubernetes.hcloud_load_balancer.ingress","module.kubernetes.hcloud_load_balancer.kube_api","module.kubernetes.hcloud_load_balancer_network.ingress","module.kubernetes.hcloud_network.this","module.kubernetes.hcloud_network_subnet.autoscaler","module.kubernetes.hcloud_network_subnet.control_plane","module.kubernetes.hcloud_network_subnet.load_balancer","module.kubernetes.hcloud_network_subnet.worker","module.kubernetes.hcloud_placement_group.control_plane","module.kubernetes.hcloud_placement_group.worker","module.kubernetes.hcloud_server.control_plane","module.kubernetes.hcloud_server.worker","module.kubernetes.hcloud_ssh_key.this","module.kubernetes.random_bytes.cilium_ipsec_key","module.kubernetes.random_bytes.hcloud_csi_encryption_key","module.kubernetes.talos_image_factory_schematic.this","module.kubernetes.talos_machine_secrets.this","module.kubernetes.terraform_data.amd64_image","module.kubernetes.terraform_data.arm64_image","module.kubernetes.terraform_data.packer_init","module.kubernetes.terraform_data.upgrade_control_plane","module.kubernetes.tls_private_key.ssh_key"]}]},{"module":"module.kubernetes","mode":"managed","type":"tls_private_key","name":"ssh_key","provider":"provider[\"registry.opentofu.org/hashicorp/tls\"]","instances":[{"schema_version":1,"attributes":{"algorithm":"ED25519","ecdsa_curve":"P224","id":"08053d34c1f41c73e0f0e29d9b792cb5d450a7d4","private_key_openssh":"-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtz\nc2gtZWQyNTUxOQAAACDFOTot/wl7HmtJyiGNAi049Tb8dh3QkAt2jRh5Js2nngAA\nAIhwox4ocKMeKAAAAAtzc2gtZWQyNTUxOQAAACDFOTot/wl7HmtJyiGNAi049Tb8\ndh3QkAt2jRh5Js2nngAAAECW7HImMu/+XrF4u0ebzNKom4M2HwOhKYboAvk69spz\nzsU5Oi3/CXsea0nKIY0CLTj1Nvx2HdCQC3aNGHkmzaeeAAAAAAECAwQF\n-----END OPENSSH PRIVATE KEY-----\n","private_key_pem":"-----BEGIN PRIVATE KEY-----\nMC4CAQAwBQYDK2VwBCIEIJbsciYy7/5esXi7R5vM0qibgzYfA6EphugC+Tr2ynPO\n-----END PRIVATE KEY-----\n","private_key_pem_pkcs8":"-----BEGIN PRIVATE KEY-----\nMC4CAQAwBQYDK2VwBCIEIJbsciYy7/5esXi7R5vM0qibgzYfA6EphugC+Tr2ynPO\n-----END PRIVATE KEY-----\n","public_key_fingerprint_md5":"67:a0:23:3e:0b:5d:ad:90:ef:33:9c:ec:32:cd:84:9b","public_key_fingerprint_sha256":"SHA256:Ulk2+RZLIvfhn/bG51etxcPRjpvzsbmWGWYEuAxm7Js","public_key_openssh":"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIMU5Oi3/CXsea0nKIY0CLTj1Nvx2HdCQC3aNGHkmzaee\n","public_key_pem":"-----BEGIN PUBLIC KEY-----\nMCowBQYDK2VwAyEAxTk6Lf8Jex5rScohjQItOPU2/HYd0JALdo0YeSbNp54=\n-----END PUBLIC KEY-----\n","rsa_bits":2048},"sensitive_attributes":[[{"type":"get_attr","value":"private_key_pem_pkcs8"}],[{"type":"get_attr","value":"private_key_pem"}],[{"type":"get_attr","value":"private_key_openssh"}]]}]},{"module":"module.kubernetes","mode":"managed","type":"tls_private_key","name":"state","provider":"provider[\"registry.opentofu.org/hashicorp/tls\"]","instances":[{"schema_version":1,"attributes":{"algorithm":"RSA","ecdsa_curve":"P224","id":"d96c7f21b902d8f1673788e7cb480f02ac254fd8","private_key_openssh":"-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABFwAAAAdz\nc2gtcnNhAAAAAwEAAQAAAQEA0YKyNU8uU5W2OucK4EdBK1gHVJbvEe/qpNtnvISo\nO5lYingi8oQhZqWae7NCzU3+oWxwyC+uxqM/1G2HoCB1jRo+yJ+GT+YY4EWjk08a\nFe2QpnSFdZgKeQeJ2lN348jxtU3N7FVHNISI0XafR9jik5UBqeFpq1fmeQ73tvbT\nm0zJXajCJA8DxuI3gMFJ3QLaQXV9UwFSosfPj6oNS/yPzXkjmtqFSpaNRLZQwDSD\nio4QpUJNfSErhqsyMy0tG7tti+7UlI+yjoTyce0JtI3P2EBEYma+Z60jcHDcCMTQ\nuY0mWQ4I91oDMmUFUnF7ZmpTWne/Rb3VfboyMET84G4HZwAAA7ghkhEDIZIRAwAA\nAAdzc2gtcnNhAAABAQDRgrI1Ty5TlbY65wrgR0ErWAdUlu8R7+qk22e8hKg7mViK\neCLyhCFmpZp7s0LNTf6hbHDIL67Goz/UbYegIHWNGj7In4ZP5hjgRaOTTxoV7ZCm\ndIV1mAp5B4naU3fjyPG1Tc3sVUc0hIjRdp9H2OKTlQGp4WmrV+Z5Dve29tObTMld\nqMIkDwPG4jeAwUndAtpBdX1TAVKix8+Pqg1L/I/NeSOa2oVKlo1EtlDANIOKjhCl\nQk19ISuGqzIzLS0bu22L7tSUj7KOhPJx7Qm0jc/YQERiZr5nrSNwcNwIxNC5jSZZ\nDgj3WgMyZQVScXtmalNad79FvdV9ujIwRPzgbgdnAAAAAwEAAQAAAQARR4YQJywr\nIPgLH/QXLvlH1wufuo++0HqZ8diP7/b6F2R54yequHgSwbTIiq6HRUKrffNH/MVy\n75LmSnRQTMJf4AVulthFLbU+hgUpiJweU94axGsfbunqNSjK9MMOFTbOMmZknVyb\n5GnDsNS5NcDrgB4c1zutBzkPIfYPRTM1pczp4C1q42AdEqrUcXfUaBxFc8CF4fyj\n5a7w51Hk9OsIGy8OabOs9Z3Rn7EFLdL55t27cAX2rdBl83fWufot3e3IYL7365d2\n1gUnG5OQt474Px7U4znXx1ALOHmcSS2ETrvpoBMLawYHv2vWEXTHG3rTZzADcgDJ\n3lvSoPJp+sZVAAAAgQCVnwzpkrI6Y7e+s8e0x3FkYuemOZANW7CyJemYih53ufXK\nZXjmPw+lmLPgxk2MPerBhmJBPrnrLEH18IeGymvCUXDOFVGZ4B66pVpKs7vUJPQb\n46aMV4xXOvyGxLAgnj/4EJpNiNl+JqXbaDj7AwjwVVZAZEXBM8ROsTwC2INNugAA\nAIEA5FAACadLKju/TWcrpcHcGNLwYnQu7p1tsaJc5mtxQGjqqNes2zahPzrgJvSl\njfX9W8xIvqfwy/UazBKxtD+s7yep+VinShaUPHFD0r07viyBmS/121CQUt+ylHA7\nXlnqq/Oik0GpaFIf0p8x2Tw4kpemFiRHT+aYxP1FY6pExbUAAACBAOrq/FcfoyVr\nhlD5GozBLJSh0eNS9gud/hugMwaEBfh5bNY82uspYDAaIvQ+HnF8Ehvuu8ddmjE+\naxhvmfxUDyRNZk0hyOjtAyCf147nUnZ3de7COL6CwFc65AT8B+/D8Y/350eqjnmj\ngc1AR26LhuvLhmwLCUcqUSydG7rDYMorAAAAAAEC\n-----END OPENSSH PRIVATE KEY-----\n","private_key_pem":"-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA0YKyNU8uU5W2OucK4EdBK1gHVJbvEe/qpNtnvISoO5lYingi\n8oQhZqWae7NCzU3+oWxwyC+uxqM/1G2HoCB1jRo+yJ+GT+YY4EWjk08aFe2QpnSF\ndZgKeQeJ2lN348jxtU3N7FVHNISI0XafR9jik5UBqeFpq1fmeQ73tvbTm0zJXajC\nJA8DxuI3gMFJ3QLaQXV9UwFSosfPj6oNS/yPzXkjmtqFSpaNRLZQwDSDio4QpUJN\nfSErhqsyMy0tG7tti+7UlI+yjoTyce0JtI3P2EBEYma+Z60jcHDcCMTQuY0mWQ4I\n91oDMmUFUnF7ZmpTWne/Rb3VfboyMET84G4HZwIDAQABAoIBABFHhhAnLCsg+Asf\n9Bcu+UfXC5+6j77Qepnx2I/v9voXZHnjJ6q4eBLBtMiKrodFQqt980f8xXLvkuZK\ndFBMwl/gBW6W2EUttT6GBSmInB5T3hrEax9u6eo1KMr0ww4VNs4yZmSdXJvkacOw\n1Lk1wOuAHhzXO60HOQ8h9g9FMzWlzOngLWrjYB0SqtRxd9RoHEVzwIXh/KPlrvDn\nUeT06wgbLw5ps6z1ndGfsQUt0vnm3btwBfat0GXzd9a5+i3d7chgvvfrl3bWBScb\nk5C3jvg/HtTjOdfHUAs4eZxJLYROu+mgEwtrBge/a9YRdMcbetNnMANyAMneW9Kg\n8mn6xlUCgYEA5FAACadLKju/TWcrpcHcGNLwYnQu7p1tsaJc5mtxQGjqqNes2zah\nPzrgJvSljfX9W8xIvqfwy/UazBKxtD+s7yep+VinShaUPHFD0r07viyBmS/121CQ\nUt+ylHA7Xlnqq/Oik0GpaFIf0p8x2Tw4kpemFiRHT+aYxP1FY6pExbUCgYEA6ur8\nVx+jJWuGUPkajMEslKHR41L2C53+G6AzBoQF+Hls1jza6ylgMBoi9D4ecXwSG+67\nx12aMT5rGG+Z/FQPJE1mTSHI6O0DIJ/XjudSdnd17sI4voLAVzrkBPwH78Pxj/fn\nR6qOeaOBzUBHbouG68uGbAsJRypRLJ0busNgyisCgYB0vi92wDNgEhL82j9oT5ti\nmiHOxgflfVxE03fbXp8XR06OLVI2+VU0Rr+tS7WHxHvbv7aGvssTD263YOu69kBQ\nPaDdXiaUbL05ttTuYlK1KH7QPMTjcuuJZNckrcq9sQ6dBERiNdYRAMVC5qIPqYpn\n8uE7Oyu3wHrnZ3ZewwbNbQKBgBHMDaP4FJ8QH1PhG17Qf7Ue8Uy8i0FkDc0//L2g\noOcrI9CzOY6ZSt06Da3dSuckFCcjlfxLnhPe+7QVOGBCagdBCeAMEDF9S0Bce7Mq\n2Td6Y2fcNPpR44p4PrAV++/xM0rJ6C6owDR866eGGCNK1MTmXV+wElre3Nl5aZFj\nneCTAoGBAJWfDOmSsjpjt76zx7THcWRi56Y5kA1bsLIl6ZiKHne59cpleOY/D6WY\ns+DGTYw96sGGYkE+uessQfXwh4bKa8JRcM4VUZngHrqlWkqzu9Qk9BvjpoxXjFc6\n/IbEsCCeP/gQmk2I2X4mpdtoOPsDCPBVVkBkRcEzxE6xPALYg026\n-----END RSA PRIVATE KEY-----\n","private_key_pem_pkcs8":"-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDRgrI1Ty5TlbY6\n5wrgR0ErWAdUlu8R7+qk22e8hKg7mViKeCLyhCFmpZp7s0LNTf6hbHDIL67Goz/U\nbYegIHWNGj7In4ZP5hjgRaOTTxoV7ZCmdIV1mAp5B4naU3fjyPG1Tc3sVUc0hIjR\ndp9H2OKTlQGp4WmrV+Z5Dve29tObTMldqMIkDwPG4jeAwUndAtpBdX1TAVKix8+P\nqg1L/I/NeSOa2oVKlo1EtlDANIOKjhClQk19ISuGqzIzLS0bu22L7tSUj7KOhPJx\n7Qm0jc/YQERiZr5nrSNwcNwIxNC5jSZZDgj3WgMyZQVScXtmalNad79FvdV9ujIw\nRPzgbgdnAgMBAAECggEAEUeGECcsKyD4Cx/0Fy75R9cLn7qPvtB6mfHYj+/2+hdk\neeMnqrh4EsG0yIquh0VCq33zR/zFcu+S5kp0UEzCX+AFbpbYRS21PoYFKYicHlPe\nGsRrH27p6jUoyvTDDhU2zjJmZJ1cm+Rpw7DUuTXA64AeHNc7rQc5DyH2D0UzNaXM\n6eAtauNgHRKq1HF31GgcRXPAheH8o+Wu8OdR5PTrCBsvDmmzrPWd0Z+xBS3S+ebd\nu3AF9q3QZfN31rn6Ld3tyGC+9+uXdtYFJxuTkLeO+D8e1OM518dQCzh5nEkthE67\n6aATC2sGB79r1hF0xxt602cwA3IAyd5b0qDyafrGVQKBgQDkUAAJp0sqO79NZyul\nwdwY0vBidC7unW2xolzma3FAaOqo16zbNqE/OuAm9KWN9f1bzEi+p/DL9RrMErG0\nP6zvJ6n5WKdKFpQ8cUPSvTu+LIGZL/XbUJBS37KUcDteWeqr86KTQaloUh/SnzHZ\nPDiSl6YWJEdP5pjE/UVjqkTFtQKBgQDq6vxXH6Mla4ZQ+RqMwSyUodHjUvYLnf4b\noDMGhAX4eWzWPNrrKWAwGiL0Ph5xfBIb7rvHXZoxPmsYb5n8VA8kTWZNIcjo7QMg\nn9eO51J2d3Xuwji+gsBXOuQE/Afvw/GP9+dHqo55o4HNQEdui4bry4ZsCwlHKlEs\nnRu6w2DKKwKBgHS+L3bAM2ASEvzaP2hPm2KaIc7GB+V9XETTd9tenxdHTo4tUjb5\nVTRGv61LtYfEe9u/toa+yxMPbrdg67r2QFA9oN1eJpRsvTm21O5iUrUoftA8xONy\n64lk1yStyr2xDp0ERGI11hEAxULmog+pimfy4Ts7K7fAeudndl7DBs1tAoGAEcwN\no/gUnxAfU+EbXtB/tR7xTLyLQWQNzT/8vaCg5ysj0LM5jplK3ToNrd1K5yQUJyOV\n/EueE977tBU4YEJqB0EJ4AwQMX1LQFx7syrZN3pjZ9w0+lHjing+sBX77/EzSsno\nLqjANHzrp4YYI0rUxOZdX7ASWt7c2XlpkWOd4JMCgYEAlZ8M6ZKyOmO3vrPHtMdx\nZGLnpjmQDVuwsiXpmIoed7n1ymV45j8PpZiz4MZNjD3qwYZiQT656yxB9fCHhspr\nwlFwzhVRmeAeuqVaSrO71CT0G+OmjFeMVzr8hsSwIJ4/+BCaTYjZfial22g4+wMI\n8FVWQGRFwTPETrE8AtiDTbo=\n-----END PRIVATE KEY-----\n","public_key_fingerprint_md5":"84:6e:ee:10:45:60:8c:c3:9e:40:b4:2f:78:c7:f6:80","public_key_fingerprint_sha256":"SHA256:DP7AY/245CMjy+5UTxhNfEA1qUHd+z0M1eyUw6EqpSc","public_key_openssh":"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDRgrI1Ty5TlbY65wrgR0ErWAdUlu8R7+qk22e8hKg7mViKeCLyhCFmpZp7s0LNTf6hbHDIL67Goz/UbYegIHWNGj7In4ZP5hjgRaOTTxoV7ZCmdIV1mAp5B4naU3fjyPG1Tc3sVUc0hIjRdp9H2OKTlQGp4WmrV+Z5Dve29tObTMldqMIkDwPG4jeAwUndAtpBdX1TAVKix8+Pqg1L/I/NeSOa2oVKlo1EtlDANIOKjhClQk19ISuGqzIzLS0bu22L7tSUj7KOhPJx7Qm0jc/YQERiZr5nrSNwcNwIxNC5jSZZDgj3WgMyZQVScXtmalNad79FvdV9ujIwRPzgbgdn\n","public_key_pem":"-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA0YKyNU8uU5W2OucK4EdB\nK1gHVJbvEe/qpNtnvISoO5lYingi8oQhZqWae7NCzU3+oWxwyC+uxqM/1G2HoCB1\njRo+yJ+GT+YY4EWjk08aFe2QpnSFdZgKeQeJ2lN348jxtU3N7FVHNISI0XafR9ji\nk5UBqeFpq1fmeQ73tvbTm0zJXajCJA8DxuI3gMFJ3QLaQXV9UwFSosfPj6oNS/yP\nzXkjmtqFSpaNRLZQwDSDio4QpUJNfSErhqsyMy0tG7tti+7UlI+yjoTyce0JtI3P\n2EBEYma+Z60jcHDcCMTQuY0mWQ4I91oDMmUFUnF7ZmpTWne/Rb3VfboyMET84G4H\nZwIDAQAB\n-----END PUBLIC KEY-----\n","rsa_bits":2048},"sensitive_attributes":[[{"type":"get_attr","value":"private_key_openssh"}],[{"type":"get_attr","value":"private_key_pem_pkcs8"}],[{"type":"get_attr","value":"private_key_pem"}]]}]},{"module":"module.kubernetes","mode":"managed","type":"tls_self_signed_cert","name":"state","provider":"provider[\"registry.opentofu.org/hashicorp/tls\"]","instances":[{"schema_version":0,"attributes":{"allowed_uses":["server_auth"],"cert_pem":"-----BEGIN CERTIFICATE-----\nMIIC2jCCAcKgAwIBAgIRAPnXgpn+6biaAieROzh+VDYwDQYJKoZIhvcNAQELBQAw\nFDESMBAGA1UEAxMJZ29pbmdkYXJrMCAXDTI1MDgyMzA5MTI0N1oYDzIxMjUwODI0\nMDkxMjQ3WjAUMRIwEAYDVQQDEwlnb2luZ2RhcmswggEiMA0GCSqGSIb3DQEBAQUA\nA4IBDwAwggEKAoIBAQDRgrI1Ty5TlbY65wrgR0ErWAdUlu8R7+qk22e8hKg7mViK\neCLyhCFmpZp7s0LNTf6hbHDIL67Goz/UbYegIHWNGj7In4ZP5hjgRaOTTxoV7ZCm\ndIV1mAp5B4naU3fjyPG1Tc3sVUc0hIjRdp9H2OKTlQGp4WmrV+Z5Dve29tObTMld\nqMIkDwPG4jeAwUndAtpBdX1TAVKix8+Pqg1L/I/NeSOa2oVKlo1EtlDANIOKjhCl\nQk19ISuGqzIzLS0bu22L7tSUj7KOhPJx7Qm0jc/YQERiZr5nrSNwcNwIxNC5jSZZ\nDgj3WgMyZQVScXtmalNad79FvdV9ujIwRPzgbgdnAgMBAAGjJTAjMBMGA1UdJQQM\nMAoGCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwDQYJKoZIhvcNAQELBQADggEBAJNk\n7E8QpmF7KRvRO6rorysv82xT7Q+KSuFBdaxgu0V0LmHCpn3ZXZqJwAB1GLiKv1Th\n7cvhr+jsr5KdYHFbIEenOH0BsQnqODj5GJtRRXzvQLcL2awzb+NAN8n1wnwdC6cZ\nbwWFO3jg0DThGF80vEjNME3EGC3nfz8RIhH5zxSUjt3OX9s2xi2kttlgcw/HTP3C\np+mqLI6ZHlwsDM1+n+fBjJhPCWR/n+/OyDeyLbWkadgLjIU41hI7e9vH698ZHQNw\nToP/ckQD7X2FSpdBp/z+NQA63WzQZE30YEVxGXnonUu9Z1X55E2U6DkH2SFv44TL\n2qcGS3CP9LYspObP994=\n-----END CERTIFICATE-----\n","dns_names":null,"early_renewal_hours":0,"id":"332096763689078123659759323490975372342","ip_addresses":null,"is_ca_certificate":false,"key_algorithm":"RSA","private_key_pem":"-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA0YKyNU8uU5W2OucK4EdBK1gHVJbvEe/qpNtnvISoO5lYingi\n8oQhZqWae7NCzU3+oWxwyC+uxqM/1G2HoCB1jRo+yJ+GT+YY4EWjk08aFe2QpnSF\ndZgKeQeJ2lN348jxtU3N7FVHNISI0XafR9jik5UBqeFpq1fmeQ73tvbTm0zJXajC\nJA8DxuI3gMFJ3QLaQXV9UwFSosfPj6oNS/yPzXkjmtqFSpaNRLZQwDSDio4QpUJN\nfSErhqsyMy0tG7tti+7UlI+yjoTyce0JtI3P2EBEYma+Z60jcHDcCMTQuY0mWQ4I\n91oDMmUFUnF7ZmpTWne/Rb3VfboyMET84G4HZwIDAQABAoIBABFHhhAnLCsg+Asf\n9Bcu+UfXC5+6j77Qepnx2I/v9voXZHnjJ6q4eBLBtMiKrodFQqt980f8xXLvkuZK\ndFBMwl/gBW6W2EUttT6GBSmInB5T3hrEax9u6eo1KMr0ww4VNs4yZmSdXJvkacOw\n1Lk1wOuAHhzXO60HOQ8h9g9FMzWlzOngLWrjYB0SqtRxd9RoHEVzwIXh/KPlrvDn\nUeT06wgbLw5ps6z1ndGfsQUt0vnm3btwBfat0GXzd9a5+i3d7chgvvfrl3bWBScb\nk5C3jvg/HtTjOdfHUAs4eZxJLYROu+mgEwtrBge/a9YRdMcbetNnMANyAMneW9Kg\n8mn6xlUCgYEA5FAACadLKju/TWcrpcHcGNLwYnQu7p1tsaJc5mtxQGjqqNes2zah\nPzrgJvSljfX9W8xIvqfwy/UazBKxtD+s7yep+VinShaUPHFD0r07viyBmS/121CQ\nUt+ylHA7Xlnqq/Oik0GpaFIf0p8x2Tw4kpemFiRHT+aYxP1FY6pExbUCgYEA6ur8\nVx+jJWuGUPkajMEslKHR41L2C53+G6AzBoQF+Hls1jza6ylgMBoi9D4ecXwSG+67\nx12aMT5rGG+Z/FQPJE1mTSHI6O0DIJ/XjudSdnd17sI4voLAVzrkBPwH78Pxj/fn\nR6qOeaOBzUBHbouG68uGbAsJRypRLJ0busNgyisCgYB0vi92wDNgEhL82j9oT5ti\nmiHOxgflfVxE03fbXp8XR06OLVI2+VU0Rr+tS7WHxHvbv7aGvssTD263YOu69kBQ\nPaDdXiaUbL05ttTuYlK1KH7QPMTjcuuJZNckrcq9sQ6dBERiNdYRAMVC5qIPqYpn\n8uE7Oyu3wHrnZ3ZewwbNbQKBgBHMDaP4FJ8QH1PhG17Qf7Ue8Uy8i0FkDc0//L2g\noOcrI9CzOY6ZSt06Da3dSuckFCcjlfxLnhPe+7QVOGBCagdBCeAMEDF9S0Bce7Mq\n2Td6Y2fcNPpR44p4PrAV++/xM0rJ6C6owDR866eGGCNK1MTmXV+wElre3Nl5aZFj\nneCTAoGBAJWfDOmSsjpjt76zx7THcWRi56Y5kA1bsLIl6ZiKHne59cpleOY/D6WY\ns+DGTYw96sGGYkE+uessQfXwh4bKa8JRcM4VUZngHrqlWkqzu9Qk9BvjpoxXjFc6\n/IbEsCCeP/gQmk2I2X4mpdtoOPsDCPBVVkBkRcEzxE6xPALYg026\n-----END RSA PRIVATE KEY-----\n","ready_for_renewal":false,"set_authority_key_id":false,"set_subject_key_id":false,"subject":[{"common_name":"goingdark","country":null,"email_address":null,"locality":null,"organization":null,"organizational_unit":null,"postal_code":null,"province":null,"serial_number":null,"street_address":null}],"uris":null,"validity_end_time":"2125-08-24T11:12:47.051356396+02:00","validity_period_hours":876600,"validity_start_time":"2025-08-23T11:12:47.051356396+02:00"},"sensitive_attributes":[[{"type":"get_attr","value":"private_key_pem"}]],"dependencies":["module.kubernetes.tls_private_key.state"]}]}],"check_results":[{"object_kind":"var","config_addr":"module.kubernetes.var.cilium_hubble_relay_enabled","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cilium_hubble_relay_enabled","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.talos_kubelet_extra_mounts","status":"pass","objects":[{"object_addr":"module.kubernetes.var.talos_kubelet_extra_mounts","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_nginx_kind","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_nginx_kind","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_health_check_interval","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_health_check_interval","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.talos_machine_configuration_apply_mode","status":"pass","objects":[{"object_addr":"module.kubernetes.var.talos_machine_configuration_apply_mode","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.firewall_extra_rules","status":"pass","objects":[{"object_addr":"module.kubernetes.var.firewall_extra_rules","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.worker_nodepools","status":"pass","objects":[{"object_addr":"module.kubernetes.var.worker_nodepools","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cilium_encryption_type","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cilium_encryption_type","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cluster_name","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cluster_name","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_health_check_retries","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_health_check_retries","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cluster_autoscaler_nodepools","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cluster_autoscaler_nodepools","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.oidc_client_id","status":"pass","objects":[{"object_addr":"module.kubernetes.var.oidc_client_id","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.packer_arm64_builder","status":"pass","objects":[{"object_addr":"module.kubernetes.var.packer_arm64_builder","status":"pass"}]},{"object_kind":"resource","config_addr":"module.kubernetes.data.http.current_ipv6","status":"pass","objects":null},{"object_kind":"var","config_addr":"module.kubernetes.var.control_plane_nodepools","status":"pass","objects":[{"object_addr":"module.kubernetes.var.control_plane_nodepools","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cluster_rdns_ipv6","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cluster_rdns_ipv6","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.talos_extra_routes","status":"pass","objects":[{"object_addr":"module.kubernetes.var.talos_extra_routes","status":"pass"}]},{"object_kind":"resource","config_addr":"module.kubernetes.data.http.current_ipv4","status":"pass","objects":null},{"object_kind":"var","config_addr":"module.kubernetes.var.cilium_ipsec_key_id","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cilium_ipsec_key_id","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cilium_hubble_ui_enabled","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cilium_hubble_ui_enabled","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_rdns","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_rdns","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_nginx_replicas","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_nginx_replicas","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cluster_rdns_ipv4","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cluster_rdns_ipv4","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.hcloud_network_id","status":"pass","objects":[{"object_addr":"module.kubernetes.var.hcloud_network_id","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cilium_bpf_datapath_mode","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cilium_bpf_datapath_mode","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_nginx_service_external_traffic_policy","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_nginx_service_external_traffic_policy","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_nginx_enabled","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_nginx_enabled","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.oidc_issuer_url","status":"pass","objects":[{"object_addr":"module.kubernetes.var.oidc_issuer_url","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.rbac_roles","status":"pass","objects":[{"object_addr":"module.kubernetes.var.rbac_roles","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cluster_access","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cluster_access","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_type","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_type","status":"pass"}]},{"object_kind":"resource","config_addr":"module.kubernetes.data.http.kube_api_health","status":"unknown","objects":[{"object_addr":"module.kubernetes.data.http.kube_api_health[0]","status":"unknown"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.oidc_group_mappings","status":"pass","objects":[{"object_addr":"module.kubernetes.var.oidc_group_mappings","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_health_check_timeout","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_health_check_timeout","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.hcloud_csi_encryption_passphrase","status":"pass","objects":[{"object_addr":"module.kubernetes.var.hcloud_csi_encryption_passphrase","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.packer_amd64_builder","status":"pass","objects":[{"object_addr":"module.kubernetes.var.packer_amd64_builder","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_algorithm","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_algorithm","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cluster_domain","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cluster_domain","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cluster_rdns","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cluster_rdns","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_rdns_ipv4","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_rdns_ipv4","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.rbac_cluster_roles","status":"pass","objects":[{"object_addr":"module.kubernetes.var.rbac_cluster_roles","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_rdns_ipv6","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_rdns_ipv6","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cilium_routing_mode","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cilium_routing_mode","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.cilium_ipsec_key_size","status":"pass","objects":[{"object_addr":"module.kubernetes.var.cilium_ipsec_key_size","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.ingress_load_balancer_pools","status":"pass","objects":[{"object_addr":"module.kubernetes.var.ingress_load_balancer_pools","status":"pass"}]},{"object_kind":"var","config_addr":"module.kubernetes.var.hcloud_load_balancer_location","status":"pass","objects":[{"object_addr":"module.kubernetes.var.hcloud_load_balancer_location","status":"pass"}]}]}
