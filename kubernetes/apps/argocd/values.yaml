# https://github.com/argoproj/argo-helm/blob/main/charts/argo-cd/values.yaml
configs:
  cm:
    create: true
    application.resourceTrackingMethod: label
    admin.enabled: false
    url: https://argocd.goingdark.social
    kustomize.buildOptions: '--enable-helm'
    accounts.kubechecks: apiKey
    dex.config: |
      # Embedded Dex: generic OAuth2 connector to Mastodon
      connectors:
      - type: oauth
        id: mastodon
        name: Mastodon (goingdark.social)
        config:
          clientID: $dex.mastodon.clientId
          clientSecret: $dex.mastodon.clientSecret
          redirectURI: https://argocd.goingdark.social/api/dex/callback
          authorizationURL: https://goingdark.social/oauth/authorize
          tokenURL: https://goingdark.social/oauth/token
          # Use verify_credentials which works on all Mastodon ≥2.4 instances.
          # This endpoint returns user account info and role data.
          # See: /api/v1/accounts/verify_credentials
          userInfoURL: https://goingdark.social/api/v1/accounts/verify_credentials
          scopes:
            - read:accounts
          # Map Mastodon JSON fields to standard OIDC claims Dex will include.
          # userNameKey and preferredUsernameKey are safe. groupsKey is best-effort, see note below.
          claimMapping:
            userNameKey: acct
            preferredUsernameKey: username
            # If your Mastodon returns role as a *string* (older servers), this works:
            # groupsKey: role
            # If it returns a Role *object* (Mastodon ≥4.0), Dex cannot map nested objects to groups.
            # Leave groupsKey unset and use the RBAC default or explicit user mappings (see note).

  cmp:
    create: true
    plugins:
      kustomize-build-with-helm:
        generate:
          command: [sh, -c]
          args: [kustomize build --enable-helm]
  params:
    controller.diff.server.side: true
    server.insecure: true
  rbac:
    create: true
    policy.csv: |
      # Map SSO groups to roles.
      # If Dex was able to emit a 'groups' claim with 'Admin' or 'Moderator',
      # these two lines grant access as requested.
      g, Admin, role:admin
      g, Moderator, role:readonly
      g, ArgoCD Admins, role:admin
      g, ArgoCD Viewers, role:readonly
      g, ArgoCDAdmins, role:admin
      g, ArgoCDViewers, role:readonly
      g, argocd:admin, role:admin
      g, argocd:read_all, role:readonly
      p, role:kubechecks, applications, get,   */*, allow
      p, role:kubechecks, applications, list,  */*, allow
      p, role:kubechecks, applications, sync,  */*, allow
      p, role:kubechecks, applications, create, */*, allow
      p, role:kubechecks, applications, update, */*, allow
      p, role:kubechecks, projects, get,       *, allow
      p, role:kubechecks, projects, update,    *, allow
      p, role:kubechecks, clusters, get, *, allow
      g, kubechecks, role:kubechecks

crds:
  install: true
  # -- Keep CRDs on chart uninstall
  keep: false

controller:
  replicas: 1
  tolerations:
    - key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Exists
      effect: NoSchedule
  resources:
    requests:
      cpu: 50m
      memory: 256Mi
    limits:
      cpu: 300m
      memory: 800Mi

dex:
  enabled: true
  tolerations:
    - key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Exists
      effect: NoSchedule
    - key: "autoscaler-node"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"

  resources:
    requests:
      cpu: 10m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 128Mi

# Disable HA Redis, use single Redis
redis-ha:
  enabled: false

redis:
  enabled: true
  # keep it tiny
  auth:
    enabled: false
  master:
    resources:
      requests:
        cpu: 20m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 256Mi

server:
  autoscaling:
    enabled: false
  resources:
    requests:
      cpu: 30m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  tolerations:
    - key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Exists
      effect: NoSchedule
  extensions:
    enabled: true
  service:
    type: ClusterIP
    httpsPort: 443
    httpsTargetPort: 8443

# No HPA or strict PDB for repoServer
repoServer:
  autoscaling:
    enabled: false
  pdb:
    enabled: false
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  tolerations:
    - key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Exists
      effect: NoSchedule
  containerSecurityContext:
    readOnlyRootFilesystem: true
  volumes:
    - name: cmp-kustomize-build-with-helm
      configMap:
        name: argocd-cmp-cm
    - name: cmp-tmp
      emptyDir: {}
  extraContainers:
    - name: kustomize-build-with-helm
      command:
        - argocd-cmp-server
      image:
        '{{ default .Values.global.image.repository .Values.repoServer.image.repository }}:{{ default (include
        "argo-cd.defaultTag" .) .Values.repoServer.image.tag }}'
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi
      securityContext:
        runAsNonRoot: true
        runAsUser: 999
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        seccompProfile:
          type: RuntimeDefault
        capabilities:
          drop: [ALL]
      volumeMounts:
        - name: plugins
          mountPath: /home/argocd/cmp-server/plugins
        - name: cmp-kustomize-build-with-helm
          mountPath: /home/argocd/cmp-server/config/plugin.yaml
          subPath: kustomize-build-with-helm.yaml
        - mountPath: /tmp
          name: cmp-tmp

applicationSet:
  replicas: 1
  pdb:
    enabled: false
  resources:
    requests:
      cpu: 30m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 256Mi
  tolerations:
    - key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Exists
      effect: NoSchedule

notifications:
  enabled: false
  resources:
    requests:
      cpu: 100m
      memory: 64Mi
    limits:
      cpu: 1000m
      memory: 128Mi


redisSecretInit:
  # -- Enable Redis secret initialization. If disabled, secret must be provisioned by alternative methods
  enabled: true
  # -- Redis secret-init name
  name: redis-secret-init

  # -- Resource limits and requests for Redis secret-init Job
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi

  # -- Application controller container-level security context
  # @default -- See [values.yaml]
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault

  # -- Redis secret-init Job pod-level security context
  securityContext: {}

  serviceAccount:
    # -- Create a service account for the redis pod
    create: true
    # -- Service account name for redis pod
    name: ""
    # -- Annotations applied to created service account
    annotations: {}
    # -- Automount API credentials for the Service Account
    automountServiceAccountToken: true

  # -- Priority class for Redis secret-init Job
  # @default -- `""` (defaults to global.priorityClassName)
  priorityClassName: ""

  # -- Assign custom [affinity] rules to the Redis secret-init Job
  affinity: {}

  # -- Node selector to be added to the Redis secret-init Job
  # @default -- `{}` (defaults to global.nodeSelector)
  nodeSelector: {}

  # -- Tolerations to be added to the Redis secret-init Job
  # @default -- `[]` (defaults to global.tolerations)
  tolerations:
    - key: "autoscaler-node"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
