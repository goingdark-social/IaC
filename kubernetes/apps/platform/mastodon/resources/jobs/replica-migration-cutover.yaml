# Zalando to CNPG Migration Cutover Orchestration Job
#
# This job orchestrates the database cutover from Zalando PostgreSQL to CNPG.
# It performs the following steps:
# 1. Record current application replica counts for rollback
# 2. Scale down all Mastodon applications to 0 replicas
# 3. Wait for replication lag to be minimal (< 1 second)
# 4. Promote CNPG cluster to primary
# 5. Verify promotion completed successfully
# 6. Update application configuration to point to CNPG
# 7. Scale up applications to original replica counts
# 8. Perform basic health validation
#
# Safety features:
# - Stores replica counts in ConfigMap for rollback capability
# - Timeout enforcement on each critical step (5 minutes max)
# - Comprehensive error handling with automatic rollback on failure
# - Progress logging at each step for observability
# - Requires manual triggering (restartPolicy: Never)
#
# Prerequisites:
# - kubectl-cnpg plugin installed in job image
# - ServiceAccount with permissions to scale deployments and promote clusters
# - CNPG replica cluster running and catching up with Zalando primary
#
# Usage:
#   kubectl create -f replica-migration-cutover.yaml
#   kubectl logs -f job/replica-migration-cutover -n mastodon
#
# Rollback (if needed):
#   kubectl create configmap cutover-rollback --from-literal=rollback=true -n mastodon
#   # Manually scale deployments back to saved replica counts
#   # Revert DB_HOST configuration to Zalando endpoints
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cutover-orchestrator
  namespace: mastodon
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cutover-orchestrator-role
  namespace: mastodon
rules:
  # Deployments - scale and read
  - apiGroups: ["apps"]
    resources: ["deployments", "deployments/scale"]
    verbs: ["get", "list", "patch", "update"]
  # StatefulSets - read for validation
  - apiGroups: ["apps"]
    resources: ["statefulsets"]
    verbs: ["get", "list"]
  # ConfigMaps - create and update for state storage
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "create", "update", "patch"]
  # Pods - read for validation
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
  # CNPG Clusters - promote operation
  - apiGroups: ["postgresql.cnpg.io"]
    resources: ["clusters"]
    verbs: ["get", "patch", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cutover-orchestrator-binding
  namespace: mastodon
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cutover-orchestrator-role
subjects:
  - kind: ServiceAccount
    name: cutover-orchestrator
    namespace: mastodon
---
apiVersion: batch/v1
kind: Job
metadata:
  name: replica-migration-cutover
  namespace: mastodon
  labels:
    app: mastodon
    component: migration
    migration-phase: cutover
spec:
  backoffLimit: 0  # No automatic retries - manual intervention required on failure
  ttlSecondsAfterFinished: 86400  # Keep job for 24 hours for debugging
  template:
    metadata:
      labels:
        app: mastodon
        component: migration
        migration-phase: cutover
    spec:
      serviceAccountName: cutover-orchestrator
      restartPolicy: Never
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534  # nobody
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: cutover
          image: bitnami/kubectl:1.32.1
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
          env:
            - name: NAMESPACE
              value: "mastodon"
            - name: STEP_TIMEOUT
              value: "300"  # 5 minutes per step
            - name: REPLICATION_LAG_THRESHOLD
              value: "1"    # 1 second max lag
            - name: CNPG_CLUSTER_NAME
              value: "database"
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: state
              mountPath: /state
          command: ["/bin/bash"]
          args:
            - -c
            - |
              #!/bin/bash
              set -euo pipefail

              # Color codes for output
              RED='\033[0;31m'
              GREEN='\033[0;32m'
              YELLOW='\033[1;33m'
              BLUE='\033[0;34m'
              NC='\033[0m' # No Color

              # Logging functions
              log_info() {
                echo -e "${BLUE}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*"
              }

              log_success() {
                echo -e "${GREEN}[SUCCESS]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*"
              }

              log_warn() {
                echo -e "${YELLOW}[WARN]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*"
              }

              log_error() {
                echo -e "${RED}[ERROR]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $*"
              }

              # State files
              REPLICA_COUNTS_FILE="/state/replica-counts.json"
              CUTOVER_STATE_FILE="/state/cutover-state"

              # Deployment names
              DEPLOYMENTS=(
                "mastodon-web"
                "mastodon-sidekiq-default"
                "mastodon-sidekiq-federation"
                "mastodon-sidekiq-background"
                "mastodon-sidekiq-scheduler"
                "mastodon-streaming"
              )

              # Timeout wrapper function
              run_with_timeout() {
                local timeout=$1
                shift
                local cmd="$*"

                log_info "Running command with ${timeout}s timeout: $cmd"

                if timeout "$timeout" bash -c "$cmd"; then
                  return 0
                else
                  local exit_code=$?
                  if [ $exit_code -eq 124 ]; then
                    log_error "Command timed out after ${timeout}s"
                  else
                    log_error "Command failed with exit code $exit_code"
                  fi
                  return $exit_code
                fi
              }

              # Save current state to ConfigMap
              save_state_to_configmap() {
                local state_name="$1"
                local data_key="$2"
                local data_value="$3"

                log_info "Saving state to ConfigMap: $state_name"

                # Create or update ConfigMap
                kubectl create configmap "$state_name" \
                  --from-literal="$data_key=$data_value" \
                  --dry-run=client -o yaml | \
                  kubectl apply -f - -n "$NAMESPACE"
              }

              # Load state from ConfigMap
              load_state_from_configmap() {
                local state_name="$1"
                local data_key="$2"

                kubectl get configmap "$state_name" -n "$NAMESPACE" \
                  -o jsonpath="{.data.$data_key}" 2>/dev/null || echo ""
              }

              # Check if rollback is requested
              check_rollback_flag() {
                local rollback_flag=$(load_state_from_configmap "cutover-rollback" "rollback")
                if [ "$rollback_flag" = "true" ]; then
                  log_warn "Rollback flag detected! Initiating rollback procedure..."
                  return 0
                fi
                return 1
              }

              # Step 1: Record current replica counts
              step1_record_replica_counts() {
                log_info "=== STEP 1: Recording current replica counts ==="
                echo "step1_started" > "$CUTOVER_STATE_FILE"

                local replica_counts="{"
                for deployment in "${DEPLOYMENTS[@]}"; do
                  log_info "Getting replica count for $deployment"

                  local replicas=$(kubectl get deployment "$deployment" -n "$NAMESPACE" \
                    -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")

                  log_info "  $deployment: $replicas replicas"
                  replica_counts="${replica_counts}\"${deployment}\":${replicas},"
                done

                # Remove trailing comma and close JSON
                replica_counts="${replica_counts%,}}"

                # Save to file and ConfigMap
                echo "$replica_counts" > "$REPLICA_COUNTS_FILE"
                save_state_to_configmap "cutover-replica-counts" "counts" "$replica_counts"

                log_success "Replica counts recorded successfully"
                echo "step1_completed" > "$CUTOVER_STATE_FILE"
              }

              # Step 2: Scale down all applications
              step2_scale_down() {
                log_info "=== STEP 2: Scaling down all Mastodon applications ==="
                echo "step2_started" > "$CUTOVER_STATE_FILE"

                for deployment in "${DEPLOYMENTS[@]}"; do
                  log_info "Scaling down $deployment to 0 replicas"

                  if ! run_with_timeout "$STEP_TIMEOUT" \
                    "kubectl scale deployment $deployment --replicas=0 -n $NAMESPACE"; then
                    log_error "Failed to scale down $deployment"
                    return 1
                  fi
                done

                # Wait for all pods to terminate
                log_info "Waiting for all application pods to terminate..."
                local wait_start=$(date +%s)
                while true; do
                  local running_pods=0
                  for deployment in "${DEPLOYMENTS[@]}"; do
                    local pod_count=$(kubectl get pods -n "$NAMESPACE" \
                      -l "app=${deployment}" \
                      --field-selector=status.phase=Running \
                      --no-headers 2>/dev/null | wc -l)
                    running_pods=$((running_pods + pod_count))
                  done

                  if [ "$running_pods" -eq 0 ]; then
                    break
                  fi

                  local elapsed=$(($(date +%s) - wait_start))
                  if [ "$elapsed" -gt "$STEP_TIMEOUT" ]; then
                    log_error "Timeout waiting for pods to terminate (${running_pods} still running)"
                    return 1
                  fi

                  log_info "  Waiting for $running_pods pods to terminate... (${elapsed}s elapsed)"
                  sleep 5
                done

                log_success "All applications scaled down successfully"
                echo "step2_completed" > "$CUTOVER_STATE_FILE"
              }

              # Step 3: Wait for replication lag to be minimal
              step3_check_replication_lag() {
                log_info "=== STEP 3: Checking replication lag ==="
                echo "step3_started" > "$CUTOVER_STATE_FILE"

                # Get a pod from the CNPG replica cluster to check lag
                local cnpg_pod=$(kubectl get pods -n "$NAMESPACE" \
                  -l "cnpg.io/cluster=${CNPG_CLUSTER_NAME},role=replica" \
                  -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                if [ -z "$cnpg_pod" ]; then
                  log_error "No CNPG replica pod found"
                  return 1
                fi

                log_info "Using CNPG pod: $cnpg_pod"

                local wait_start=$(date +%s)
                while true; do
                  log_info "Checking replication lag..."

                  # Query replication lag in seconds
                  local lag_query="SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))::int AS lag_seconds;"
                  local lag_seconds=$(kubectl exec -n "$NAMESPACE" "$cnpg_pod" -c postgres -- \
                    psql -U postgres -d app -t -c "$lag_query" 2>/dev/null | tr -d ' ' || echo "999")

                  # Handle NULL result (when not in recovery or no replication yet)
                  if [ -z "$lag_seconds" ] || [ "$lag_seconds" = "NULL" ]; then
                    lag_seconds=0
                  fi

                  log_info "  Current replication lag: ${lag_seconds}s (threshold: ${REPLICATION_LAG_THRESHOLD}s)"

                  if [ "$lag_seconds" -le "$REPLICATION_LAG_THRESHOLD" ]; then
                    break
                  fi

                  local elapsed=$(($(date +%s) - wait_start))
                  if [ "$elapsed" -gt "$STEP_TIMEOUT" ]; then
                    log_error "Timeout waiting for replication lag to decrease (current: ${lag_seconds}s)"
                    return 1
                  fi

                  sleep 5
                done

                log_success "Replication lag is acceptable: ${lag_seconds}s"
                echo "step3_completed" > "$CUTOVER_STATE_FILE"
              }

              # Step 4: Promote CNPG cluster to primary
              step4_promote_cnpg() {
                log_info "=== STEP 4: Promoting CNPG cluster to primary ==="
                echo "step4_started" > "$CUTOVER_STATE_FILE"

                log_info "Promoting CNPG cluster: $CNPG_CLUSTER_NAME"

                # Use kubectl-cnpg plugin to promote
                # Note: This assumes kubectl-cnpg plugin is installed
                # Alternative: Use kubectl patch on the Cluster resource

                # Patch method (more reliable in container without plugin):
                log_info "Patching CNPG Cluster resource to trigger promotion..."

                if ! run_with_timeout "$STEP_TIMEOUT" \
                  "kubectl patch cluster ${CNPG_CLUSTER_NAME} -n ${NAMESPACE} --type=merge -p '{\"spec\":{\"replica\":{\"enabled\":false}}}'"; then
                  log_error "Failed to promote CNPG cluster"
                  return 1
                fi

                # Alternative annotation-based promotion:
                # kubectl annotate cluster ${CNPG_CLUSTER_NAME} -n ${NAMESPACE} \
                #   cnpg.io/promote=true

                log_success "CNPG cluster promotion initiated"
                echo "step4_completed" > "$CUTOVER_STATE_FILE"
              }

              # Step 5: Verify promotion completed
              step5_verify_promotion() {
                log_info "=== STEP 5: Verifying CNPG promotion ==="
                echo "step5_started" > "$CUTOVER_STATE_FILE"

                local wait_start=$(date +%s)
                while true; do
                  log_info "Checking if CNPG is now primary..."

                  # Get primary pod
                  local primary_pod=$(kubectl get pods -n "$NAMESPACE" \
                    -l "cnpg.io/cluster=${CNPG_CLUSTER_NAME},role=primary" \
                    -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                  if [ -n "$primary_pod" ]; then
                    log_info "Found primary pod: $primary_pod"

                    # Verify it's NOT in recovery mode
                    local recovery_check=$(kubectl exec -n "$NAMESPACE" "$primary_pod" -c postgres -- \
                      psql -U postgres -d app -t -c "SELECT pg_is_in_recovery();" 2>/dev/null | tr -d ' ' || echo "t")

                    log_info "  Recovery mode check: $recovery_check (should be 'f')"

                    if [ "$recovery_check" = "f" ]; then
                      log_success "CNPG cluster is now primary (not in recovery)"
                      break
                    fi
                  fi

                  local elapsed=$(($(date +%s) - wait_start))
                  if [ "$elapsed" -gt "$STEP_TIMEOUT" ]; then
                    log_error "Timeout waiting for CNPG promotion to complete"
                    return 1
                  fi

                  log_info "  Still waiting for promotion... (${elapsed}s elapsed)"
                  sleep 10
                done

                log_success "CNPG promotion verified successfully"
                echo "step5_completed" > "$CUTOVER_STATE_FILE"
              }

              # Step 6: Update application configuration (if needed)
              step6_update_config() {
                log_info "=== STEP 6: Verifying application configuration ==="
                echo "step6_started" > "$CUTOVER_STATE_FILE"

                # Check current DB_HOST in ConfigMap
                local current_db_host=$(kubectl get configmap mastodon-database -n "$NAMESPACE" \
                  -o jsonpath='{.data.DB_HOST}' 2>/dev/null || echo "")

                log_info "Current DB_HOST: $current_db_host"

                # Expected CNPG endpoints
                local expected_rw_host="database-pooler-rw"
                local expected_ro_host="database-pooler-ro"

                if [ "$current_db_host" = "$expected_rw_host" ]; then
                  log_success "DB_HOST already configured for CNPG: $current_db_host"
                else
                  log_warn "DB_HOST needs update: $current_db_host -> $expected_rw_host"
                  log_info "Configuration update should be done via GitOps (ArgoCD)"
                  log_info "Manual update not performed by this job"
                fi

                log_success "Configuration verification completed"
                echo "step6_completed" > "$CUTOVER_STATE_FILE"
              }

              # Step 7: Scale up applications
              step7_scale_up() {
                log_info "=== STEP 7: Scaling up applications to original counts ==="
                echo "step7_started" > "$CUTOVER_STATE_FILE"

                # Load saved replica counts
                local replica_counts=$(cat "$REPLICA_COUNTS_FILE")
                log_info "Loaded replica counts: $replica_counts"

                for deployment in "${DEPLOYMENTS[@]}"; do
                  # Extract replica count for this deployment from JSON
                  local target_replicas=$(echo "$replica_counts" | \
                    grep -oP "\"${deployment}\":\K[0-9]+" || echo "1")

                  log_info "Scaling up $deployment to $target_replicas replicas"

                  if ! run_with_timeout "$STEP_TIMEOUT" \
                    "kubectl scale deployment $deployment --replicas=$target_replicas -n $NAMESPACE"; then
                    log_error "Failed to scale up $deployment"
                    return 1
                  fi
                done

                log_success "All applications scaled up successfully"
                echo "step7_completed" > "$CUTOVER_STATE_FILE"
              }

              # Step 8: Perform basic validation
              step8_validate() {
                log_info "=== STEP 8: Performing basic health validation ==="
                echo "step8_started" > "$CUTOVER_STATE_FILE"

                # Wait for pods to become ready
                log_info "Waiting for pods to become ready..."
                sleep 30  # Give pods time to start

                local validation_failed=false

                for deployment in "${DEPLOYMENTS[@]}"; do
                  log_info "Checking readiness of $deployment"

                  local ready_replicas=$(kubectl get deployment "$deployment" -n "$NAMESPACE" \
                    -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
                  local desired_replicas=$(kubectl get deployment "$deployment" -n "$NAMESPACE" \
                    -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")

                  log_info "  $deployment: $ready_replicas/$desired_replicas ready"

                  if [ "$ready_replicas" != "$desired_replicas" ]; then
                    log_warn "  Not all replicas are ready yet for $deployment"
                    validation_failed=true
                  fi
                done

                # Check database connectivity from a web pod
                log_info "Testing database connectivity..."
                local web_pod=$(kubectl get pods -n "$NAMESPACE" \
                  -l "app=mastodon-web" \
                  --field-selector=status.phase=Running \
                  -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                if [ -n "$web_pod" ]; then
                  log_info "Using web pod for DB test: $web_pod"

                  # Test database connection
                  if kubectl exec -n "$NAMESPACE" "$web_pod" -c web -- \
                    bash -c 'RAILS_ENV=production bundle exec rails runner "puts ActiveRecord::Base.connection.execute(\"SELECT 1\").first"' \
                    2>/dev/null | grep -q "1"; then
                    log_success "Database connectivity verified"
                  else
                    log_warn "Database connectivity test failed (pod may still be starting)"
                    validation_failed=true
                  fi
                else
                  log_warn "No running web pod found for validation"
                  validation_failed=true
                fi

                if [ "$validation_failed" = "true" ]; then
                  log_warn "Some validation checks failed - manual verification recommended"
                  log_warn "This is not necessarily fatal - pods may still be starting up"
                else
                  log_success "All validation checks passed"
                fi

                echo "step8_completed" > "$CUTOVER_STATE_FILE"
              }

              # Rollback function
              rollback() {
                log_error "=== INITIATING ROLLBACK ==="

                local current_step=$(cat "$CUTOVER_STATE_FILE" 2>/dev/null || echo "unknown")
                log_info "Current step: $current_step"

                # If we've already scaled up, no need to rollback scale-down
                if [[ "$current_step" == *"step7"* ]] || [[ "$current_step" == *"step8"* ]]; then
                  log_warn "Already in scale-up phase - no rollback needed for scaling"
                  return 1
                fi

                # If promotion completed, we can't easily rollback
                if [[ "$current_step" == *"step4_completed"* ]] || [[ "$current_step" == *"step5"* ]]; then
                  log_error "CNPG promotion already completed - cannot automatically rollback"
                  log_error "Manual intervention required:"
                  log_error "  1. Restore replica counts using saved ConfigMap"
                  log_error "  2. Consider setting up replication from CNPG back to Zalando if needed"
                  return 1
                fi

                # Safe to rollback scale-down
                if [ -f "$REPLICA_COUNTS_FILE" ]; then
                  log_info "Restoring original replica counts..."

                  local replica_counts=$(cat "$REPLICA_COUNTS_FILE")
                  for deployment in "${DEPLOYMENTS[@]}"; do
                    local target_replicas=$(echo "$replica_counts" | \
                      grep -oP "\"${deployment}\":\K[0-9]+" || echo "1")

                    log_info "Restoring $deployment to $target_replicas replicas"
                    kubectl scale deployment "$deployment" --replicas="$target_replicas" -n "$NAMESPACE" || true
                  done

                  log_success "Rollback completed - applications restored to original scale"
                else
                  log_error "No replica counts file found - manual rollback required"
                  return 1
                fi
              }

              # Main execution
              main() {
                log_info "======================================"
                log_info "Zalando to CNPG Migration Cutover"
                log_info "======================================"
                log_info "Namespace: $NAMESPACE"
                log_info "CNPG Cluster: $CNPG_CLUSTER_NAME"
                log_info "Step Timeout: ${STEP_TIMEOUT}s"
                log_info "Replication Lag Threshold: ${REPLICATION_LAG_THRESHOLD}s"
                log_info "======================================"

                # Check for rollback flag
                if check_rollback_flag; then
                  rollback
                  exit $?
                fi

                # Execute cutover steps
                local failed=false

                step1_record_replica_counts || failed=true

                if [ "$failed" = "false" ]; then
                  step2_scale_down || failed=true
                fi

                if [ "$failed" = "false" ]; then
                  step3_check_replication_lag || failed=true
                fi

                if [ "$failed" = "false" ]; then
                  step4_promote_cnpg || failed=true
                fi

                if [ "$failed" = "false" ]; then
                  step5_verify_promotion || failed=true
                fi

                if [ "$failed" = "false" ]; then
                  step6_update_config || failed=true
                fi

                if [ "$failed" = "false" ]; then
                  step7_scale_up || failed=true
                fi

                if [ "$failed" = "false" ]; then
                  step8_validate || failed=true
                fi

                # Final status
                if [ "$failed" = "true" ]; then
                  log_error "======================================"
                  log_error "CUTOVER FAILED"
                  log_error "======================================"
                  log_error "Attempting automatic rollback..."
                  rollback
                  exit 1
                else
                  log_success "======================================"
                  log_success "CUTOVER COMPLETED SUCCESSFULLY"
                  log_success "======================================"
                  log_info "Next steps:"
                  log_info "  1. Monitor application health and performance"
                  log_info "  2. Verify database connectivity and queries"
                  log_info "  3. Check for any errors in application logs"
                  log_info "  4. Once stable, remove Zalando resources"
                  log_info "  5. Update backup configurations if needed"

                  # Save success state
                  save_state_to_configmap "cutover-status" "status" "completed"
                  save_state_to_configmap "cutover-status" "timestamp" "$(date -u +%Y-%m-%dT%H:%M:%SZ)"

                  exit 0
                fi
              }

              # Run main function
              main
      volumes:
        - name: tmp
          emptyDir: {}
        - name: state
          emptyDir: {}
